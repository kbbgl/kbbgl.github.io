"use strict";(self.webpackChunkkgkb=self.webpackChunkkgkb||[]).push([[16083],{40999:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>c});var s=t(74848),i=t(28453);const r={slug:"3-way-data-migration-between-support-systems",title:"3-Way Data Migration between Support\xa0Systems",description:"Fill me up!",authors:["kbbgl"],tags:["bash","data engineering","jq","migration","pandas","python"]},o=void 0,a={permalink:"/blog/3-way-data-migration-between-support-systems",source:"@site/blog/2021-04-25-3-way-data-migration-between-support-systems/index.md",title:"3-Way Data Migration between Support\xa0Systems",description:"Fill me up!",date:"2021-04-25T00:00:00.000Z",tags:[{inline:!0,label:"bash",permalink:"/blog/tags/bash"},{inline:!0,label:"data engineering",permalink:"/blog/tags/data-engineering"},{inline:!0,label:"jq",permalink:"/blog/tags/jq"},{inline:!0,label:"migration",permalink:"/blog/tags/migration"},{inline:!0,label:"pandas",permalink:"/blog/tags/pandas"},{inline:!0,label:"python",permalink:"/blog/tags/python"}],readingTime:11.27,hasTruncateMarker:!1,authors:[{name:"Kobbi Gal",title:"I like to pick things apart and see how they work inside",url:"https://github.com/kbbgl",imageURL:"https://avatars.githubusercontent.com/u/14372649",key:"kbbgl",page:null}],frontMatter:{slug:"3-way-data-migration-between-support-systems",title:"3-Way Data Migration between Support\xa0Systems",description:"Fill me up!",authors:["kbbgl"],tags:["bash","data engineering","jq","migration","pandas","python"]},unlisted:!1,prevItem:{title:"How To Trace/Read RabbitMQ\xa0Messages",permalink:"/blog/how-to-trace-read-rabbitmq\xa0messages"},nextItem:{title:"Angstrom CTF2021 | Exploiting Python Pickle in Flask Web\xa0App",permalink:"/blog/angstrom-ctf2021-exploiting-python-pickle-in-flask"}},l={authorsImageUrls:[void 0]},c=[{value:"Introduction",id:"introduction",level:2},{value:"Planning the Migration",id:"planning-the-migration",level:2},{value:"The User Uniqueness Problem",id:"the-user-uniqueness-problem",level:2},{value:"Step 1: Retrieving Users from Source Ticketing Platform",id:"step-1-retrieving-users-from-source-ticketing-platform",level:2},{value:"Step 2: Generating Unique Users List from Source Chat Platform",id:"step-2-generating-unique-users-list-from-source-chat-platform",level:2},{value:"Step 3: User List Merging based on Common Attribute",id:"step-3-user-list-merging-based-on-common-attribute",level:2},{value:"Creating Batch JSON Objects from CSV",id:"creating-batch-json-objects-from-csv",level:2}];function d(e){const n={a:"a",code:"code",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsxs)(n.p,{children:["The company I work for decided a few months ago that we\u2019ll be moving all customer tickets and assets from two separate systems (one for chat and one for old-school tickets) into a new, integrated system which provides both capabilites.\nMy task was to perform the migration between the systems.\nEven though I\u2019m not data engineer by any means, I accepted the challenge and thought it would teach me a lot about the planning and execution of such a complex project. It would also allow me to hone in my development/scripting skills and finally have some hands-on experience using a Python library I was always interested in working with, ",(0,s.jsx)(n.a,{href:"https://pandas.pydata.org/",children:"pandas"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"planning-the-migration",children:"Planning the Migration"}),"\n",(0,s.jsx)(n.p,{children:"The first step was to understand what format the source and destination platforms supplied and expected the data in, respectively.\nAfter reaching out to the destination platform engineers, they provided the following information:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Supply a TAR archive with JSON files named ",(0,s.jsx)(n.code,{children:"backup_tickets_{batch_number}.json"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The JSON files need to have the following structure:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n\xa0\xa0\xa0"data": {\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0"tickets": {\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0"data": [],\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0"comments": [],\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0"users": []\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0}\n\xa0\xa0\xa0}\n}\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Each JSON file should not include more than 100 objects within each nested array (",(0,s.jsx)(n.code,{children:"data"}),", ",(0,s.jsx)(n.code,{children:"comments"}),", ",(0,s.jsx)(n.code,{children:"users"}),", ",(0,s.jsx)(n.code,{children:"organizations"}),")."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The structure for each object had to be as follows:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"ticket"})," object blueprint:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n\xa0\xa0\xa0"created_at": "2021-01-01T00:00:00Z",\n\xa0\xa0\xa0"requester_id": 123456,\n\xa0\xa0\xa0"id": 654321\n}\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"comment"})," object blueprint:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n\xa0\xa0\xa0"created_at": "2021-02-02T02:02:02Z",\n\xa0\xa0\xa0"ticket_id": 211243,\n\xa0\xa0\xa0"id": 789012,\n\xa0\xa0\xa0"public": true,\n\xa0\xa0\xa0"html_body": "<p>dummy message</p>",\n\xa0\xa0\xa0"author_id": 123456\n}\n'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"user"})," object blueprint:"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n\xa0\xa0\xa0"name": "John Doe",\n\xa0\xa0\xa0"id": 00076,\n\xa0\xa0\xa0"email": "john.doe@somedomain.org"\n}\n'})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The source data came from two different places:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The chat-related data was stored in a database that I did not have access to. Therefore, I received the data in CSV format with the following headers:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"head -n1 chat_data.csv\n\nTICKET_CREATED_AT,TICKET_REQUESTER_ID,COMMENT_PART_ID,CONVERSATION_ID,COMMENT_PUBLIC,BODY,COMMENT_CREATED_AT,AUTHOR_ID,NAME,EMAIL\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The ticket-related data was available through the provider REST API. This data was already imported into the destination system when I was assigned this task. So I needed to focus my efforts on migrating the chat-related data. I thought it would make the process easier but it actually complicated it a bit. More details on that in the section below."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"the-user-uniqueness-problem",children:"The User Uniqueness Problem"}),"\n",(0,s.jsxs)(n.p,{children:["One crucial detail that was not mentioned in the destination platform documentation was that the ",(0,s.jsx)(n.strong,{children:"users imported need to be unique based on their email address"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"This complicated things because:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The users from the source ticketing platform already existed in the destination platform since they were previously imported."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"There were users that existed on both source platforms but had different id attribute values."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The requirement from the destination platform system was to use the user id attribute that was already imported into the system."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"After some brainstorming, I was able to think about how to solve this discrepency. I broke up the problem into the following steps:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Retrieve all users from the source ticketing platform."}),"\n",(0,s.jsx)(n.li,{children:"Retrieve all users from the source chat platform."}),"\n",(0,s.jsx)(n.li,{children:"Run a VLOOKUP to compare the id from both lists of users. If the email existed in both lists, take the id from the list of users generated in step 1. Otherwise, use the id from the list retrieved in step 2."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"step-1-retrieving-users-from-source-ticketing-platform",children:"Step 1: Retrieving Users from Source Ticketing Platform"}),"\n",(0,s.jsx)(n.p,{children:"Since I already knew the source ticketing platform could supply user data by accessing their REST API, I began by reading their documentation."}),"\n",(0,s.jsx)(n.p,{children:"I found out that:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["The endpoint to retrieve user data was:\n",(0,s.jsx)(n.code,{children:"GET /api/v1/users.json"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The endpoint returned a maximum of 100 users in JSON format."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["The endpoint offered a query parameter to specify paging through all users, i.e. ",(0,s.jsx)(n.code,{children:"GET /api/v1/users.json?page=100"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Basic authentication method was used to authenticate calls."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["The JSON response included many user attributes. I was only interested in 2 of them: ",(0,s.jsx)(n.code,{children:"id"})," and ",(0,s.jsx)(n.code,{children:"email"}),"."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Reviewing the total amount of users in that platform, I found that there were a total of 56.7k. So I needed to run 567 calls (56700/100) to retrieve all the users. Sounds like a perfect solution for a combination of ",(0,s.jsx)(n.code,{children:"curl"}),", ",(0,s.jsx)(n.code,{children:"jq"})," within ",(0,s.jsx)(n.code,{children:"bash"})," loops and redirection of output to appended file.\nThis is what I came up with:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'for i in {1..567}\ndo\n\xa0\xa0\xa0\xa0echo "Retrieving users page $i..."\n\xa0\xa0\xa0\xa0curl https://$SOURCE_URL.com/api/v1/users.json\\?page=$i -u my_user/my_password | jq \'.users[] | [.id, .email] | @csv\' >> ticket_platform_users.csv\n\xa0\xa0\xa0\xa0echo "Finished retrieving users from page $i"\ndone\n'})}),"\n",(0,s.jsx)(n.p,{children:"This loop generated a CSV file with the following data:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"head -n3 ticket_platform_users.csv\n\xa0\nid,email\n1,user1@somedomain1.org\n2,user2@somedomain2.org\n"})}),"\n",(0,s.jsx)(n.p,{children:"Step 1 is done!"}),"\n",(0,s.jsx)(n.h2,{id:"step-2-generating-unique-users-list-from-source-chat-platform",children:"Step 2: Generating Unique Users List from Source Chat Platform"}),"\n",(0,s.jsx)(n.p,{children:"As previously mentioned, the data from the chat platform was sent to me in CSV format with the following headers:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"  head -n1 chat_data.csv\n\n  TICKET_CREATED_AT,TICKET_REQUESTER_ID,COMMENT_PART_ID,CONVERSATION_ID,COMMENT_PUBLIC,BODY,COMMENT_CREATED_AT,AUTHOR_ID,NAME,EMAIL\n"})}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"chat_data.csv"})," file included all chat conversations which meant that every line in the CSV represented one message sent on the chatting platform. Since users (or ",(0,s.jsx)(n.code,{children:"AUTHOR_ID"})," in this case) can write multiple messages in different ",(0,s.jsx)(n.code,{children:"CONVERSATION_ID"}),"s, I needed a way to take only the unique ",(0,s.jsx)(n.code,{children:"AUTHOR_ID"}),"s from this CSV. This is where I was introduced to the power of the Python ",(0,s.jsx)(n.code,{children:"pandas"})," library.\nI was able to generate a list of unique users using the following script:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python\n"""\ngenerate_unique_users.py\n"""\nimport pandas as pd\n\xa0\n# Read only the relevant columns from the CSV file\nusers = pd.read_csv(\n\xa0\xa0\xa0\'chat_data.csv\',\n\xa0\xa0\xa0usecols=[\'NAME\', \'EMAIL\', \'AUTHOR_ID\']\n)\n\xa0\n# Rename the columns according to the `users` object blueprint\nusers_rename = users.rename({\n\xa0\xa0\xa0"AUTHOR_ID": "id",\n\xa0\xa0\xa0"NAME": "name",\n\xa0\xa0\xa0"EMAIL": "email"\n})\n\xa0\n# Modify the column data type according to `users` object blueprint. `int64` is the `numpy.dtype` representing a number\nusers_coltype = users_rename.astype({\n\xa0\xa0\xa0"id": "int64"\n})\n\xa0\n# Remove any rows with `null`\nremoved_na = users_coltype.dropna(how=\'any\')\n\xa0\n# Remove duplicate rows, use `email` column as key\nremoved_dups = removed_na.drop_duplicates(subset=["email"], keep="first")\n\xa0\n# Display the number of unique users\ncount = removed_dups.index\nprint("Found {} unique users".format(len(count)))\n\xa0\n# Generate new CSV with unique users\nremoved_dups.to_csv("chat_platform_users.csv", index=False)\n'})}),"\n",(0,s.jsxs)(n.p,{children:["I ran the script which generated ",(0,s.jsx)(n.code,{children:"chat_platform_users.csv"})," with 13k unique users (where the file ",(0,s.jsx)(n.code,{children:"chat_data.csv"})," had 2.5M rows)."]}),"\n",(0,s.jsx)(n.p,{children:"Step 2 done!"}),"\n",(0,s.jsx)(n.h2,{id:"step-3-user-list-merging-based-on-common-attribute",children:"Step 3: User List Merging based on Common Attribute"}),"\n",(0,s.jsx)(n.p,{children:"So now we had 2 CSV files:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"ticket_platform_users.csv"})," with 56k users."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"chat_platform_users.csv"})," with 13k users."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Since I am pretty proficient with Excel/Google Spreadsheet formulas, I decided to use the ",(0,s.jsx)(n.code,{children:"VLOOKUP"})," function to generate the required list.\nI imported both CSVs into two separate sheets named after their filenames, namely:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Sheet 1: ",(0,s.jsx)(n.code,{children:"ticket_platform_users"})," had 2 columns, ",(0,s.jsx)(n.code,{children:"email, id"})]}),"\n",(0,s.jsxs)(n.li,{children:["Sheet 2: ",(0,s.jsx)(n.code,{children:"chat_platform_users"}),", had 3 columns, ",(0,s.jsx)(n.code,{children:"name, email, id"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Sheet 3: ",(0,s.jsx)(n.code,{children:"merged"})," \u2013 This is where we\u2019re going to generate a merged list of users. It had 3 columns as well, ",(0,s.jsx)(n.code,{children:"name, email, id"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["The name and email rows were taken from Sheet 2, the ",(0,s.jsx)(n.code,{children:"id"})," had the following formula in it (in the 2nd row of the sheet):"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xlsx",children:"=IFNA(VLOOKUP(A2, ticket_platform_users!$A$2:$B$56748, 2, false), chat_platform_users!C2)\n"})}),"\n",(0,s.jsx)(n.p,{children:"The explanation of the formula:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"IFNA"})," function takes in 2 arguments:"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The first argument is the value which we want to check if it returns a N/A value."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The second argument determines what value will be returned in the cell if the value returned in the first argument is NA."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["We use this function because we know that some user emails (cells ",(0,s.jsx)(n.code,{children:"A2...AN"}),") will not be found in both sheets so we want to return the ",(0,s.jsx)(n.code,{children:"id"})," of the user from the ",(0,s.jsx)(n.code,{children:"chat_platform_users"})," in this case. Otherwise, if the ",(0,s.jsx)(n.code,{children:"VLOOKUP"})," function does return a successful match for email in both sheets, the ",(0,s.jsx)(n.code,{children:"IFNA"})," will evaluate the ",(0,s.jsx)(n.code,{children:"VLOOKUP"})," value instead."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"VLOOKUP"})," function takes 4 arguments:"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["The first argument is the value to search for. In the example above, cell ",(0,s.jsx)(n.code,{children:"A2"})," evaluates to a user email so we\u2019ll be searching for that email."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["The second argument is the range where we\u2019ll be looking for a value matching ",(0,s.jsx)(n.code,{children:"A2"}),". ",(0,s.jsx)(n.code,{children:"ticket_platform_users!$A$2:$B$56748"})," in plain words means: \u2018within the ticket_platform_users spreadsheet in the static range from cells ",(0,s.jsx)(n.code,{children:"A2"})," to ",(0,s.jsx)(n.code,{children:"B56748"}),"\u2018."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["The third argument identifies the column number to return if a successful match is found. We\u2019ve specified 2 in this case because we\u2019re interested in returning the value from 2nd column from the sheet ",(0,s.jsx)(n.code,{children:"ticket_platform_users"})," which is the id."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The fourth argument is optional and it indicates to the function whether the data is sorted. It is set to false because I imported the data unsorted."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["I exported the merged sheet to file ",(0,s.jsx)(n.code,{children:"merged_users.csv"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"Step 3 done!"}),"\n",(0,s.jsx)(n.h2,{id:"creating-batch-json-objects-from-csv",children:"Creating Batch JSON Objects from CSV"}),"\n",(0,s.jsxs)(n.p,{children:["At this point, I had put a lot of time and mental energy into manipulating the users data to fit the requirements but had not touched either the tickets or comments data yet. I still needed to somehow generate the JSON representation for all 3 data sets (",(0,s.jsx)(n.code,{children:"user"}),", ",(0,s.jsx)(n.code,{children:"comment"}),", ",(0,s.jsx)(n.code,{children:"ticket"}),") and somehow insert these data sets into their appropriate final JSON object arrays."]}),"\n",(0,s.jsxs)(n.p,{children:["Luckily, ",(0,s.jsx)(n.code,{children:"pandas"})," was there to lend a hand."]}),"\n",(0,s.jsxs)(n.p,{children:["I wrote 3 similar scripts, one for each data set.\nFor the ",(0,s.jsx)(n.code,{children:"user"})," CSV:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python\n\"\"\"\n# generate_user_json.py\n\"\"\"\n\nimport pandas as pd\n\xa0\nusers = pd.read_csv(\n\xa0\xa0\xa0\xa0'merged_users.csv'\n)\n\xa0\n# Modify column data types so `id` is a number\nusers_dt = users.astype(\n\xa0\xa0\xa0\xa0{\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0'id': 'int64'\n\xa0\xa0\xa0\xa0}\n)\n\xa0\n\xa0\n# Convert DataFrame to JSON object\n# One object per line\nout = users_dt.to_json(orient='records', lines=True)\n\xa0\nwith open('all_users_lines.json', 'w', encoding='utf-8') as f:\n\xa0\xa0\xa0\xa0f.write(out)\n\"\"\"\n"})}),"\n",(0,s.jsxs)(n.p,{children:["For ",(0,s.jsx)(n.code,{children:"comment"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python\n\"\"\"\ngenerate_comment_json.py\n\"\"\"\n\nimport pandas as pd\n\xa0\ncomments = pd.read_csv(\n\xa0\xa0\xa0\xa0'chat_data.csv',\n\xa0\xa0\xa0\xa0\xa0usecols=[\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0'COMMENT_PART_ID', \n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0'AUTHOR_ID', \n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0'BODY', \n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0'COMMENT_PUBLIC', \n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0'CONVERSATION_ID', \n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0'COMMENT_CREATED_AT'\n\xa0\xa0\xa0\xa0]\n)\n\xa0\n# Remove duplicates\nremoved_dups = comments.drop_duplicates(subset=[\"COMMENT_PART_ID\"], keep=\"first\")\n\xa0\n# Remove comments with null fields\nremoved_na = removed_dups.dropna(how='any')\n\xa0\n# Rename columns according to specifications\ncomments_object = removed_na.rename(columns=\n\xa0\xa0\xa0\xa0{\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\"COMMENT_PART_ID\": \"id\", \n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\"AUTHOR_ID\": \"author_id\", \n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\"BODY\": \"html_body\",\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\"COMMENT_PUBLIC\": \"public\",\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\"COMMENT_CREATED_AT\": \"created_at\",\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\"CONVERSATION_ID\": \"ticket_id\"\n\xa0\xa0\xa0\xa0}\n)\n\xa0\n# Modify column data types\n# According to numpy dtypes\n# https://numpy.org/doc/stable/user/basics.types.html\ncomments_final = comments_object.astype(\n\xa0\xa0\xa0\xa0{\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0'id': 'int64', \n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0'author_id': 'int64',\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0'html_body': 'str',\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0'public': 'bool',\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0'ticket_id': 'int64',\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0'created_at': 'str'\n\xa0\xa0\xa0\xa0}\n)\n\xa0\n# Convert DataFrame to JSON object\n# One object per line\nout = comments_final.to_json(orient='records', lines=True)\noutfile='all_comments_lines.json'\nwith open(outfile, 'w', encoding='utf-8') as f:\n\xa0\xa0\xa0\xa0f.write(out)\n"})}),"\n",(0,s.jsxs)(n.p,{children:["For ",(0,s.jsx)(n.code,{children:"ticket"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python\n\n\"\"\" \ngenerate_ticket_json.py\n\"\"\"\n\nimport pandas as pd\n\xa0\ntickets = pd.read_csv(\n\xa0\xa0\xa0\xa0'chat_data.csv', \n\xa0\xa0\xa0\xa0usecols=['TICKET_REQUESTER_ID', 'TICKET_CREATED_AT', 'CONVERSATION_ID']\n)\n\xa0\n# Remove ticket duplicates\nremoved_ticket_dups = tickets.drop_duplicates(subset=[\"CONVERSATION_ID\"], keep=\"first\")\n\xa0\n# Rename columns according to specifications\ntickets_object = removed_ticket_dups.rename(columns=\n\xa0\xa0\xa0\xa0{\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\"CONVERSATION_ID\": \"id\", \n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\"TICKET_CREATED_AT\": \"created_at\", \n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\"TICKET_REQUESTER_ID\": \"requester_id\"\n\xa0\xa0\xa0\xa0}\n)\n\xa0\n# Modify column data types\ntickets_final = tickets_object.astype(\n\xa0\xa0\xa0\xa0{\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0'id': 'int64', \n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0'requester_id': 'int64'\n\xa0\xa0\xa0\xa0}\n)\n\xa0\nout = tickets_final.to_json(orient='records', lines=True)\noutfile='all_tickets_lines.json'\nwith open(outfile, 'w', encoding='utf-8') as f:\n\xa0\xa0\xa0\xa0f.write(out)\n"})}),"\n",(0,s.jsxs)(n.p,{children:["I used the ",(0,s.jsx)(n.code,{children:"DataFrame::to_json(lines=True)"})," method, option because it generated a JSON object per line. Since I knew that the requirement was to have a maximum of 100 objects per array representing the comment, user or ticket in the final JSON asset, I found it easier to split the output files (",(0,s.jsx)(n.code,{children:"all_tickets_lines.json"}),", ",(0,s.jsx)(n.code,{children:"all_comments_lines.json"}),", ",(0,s.jsx)(n.code,{children:"all_users_lines.json"}),") into a batch of files where each one would have 100 JSON objects within them."]}),"\n",(0,s.jsx)(n.p,{children:"I developed the following script to produce just that:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python\n"""\nsplit_into_batch_files.py\n"""\n# Define iteration group\nassets=[\'tickets\', \'users\', \'comments\']\n\xa0\n\xa0\n"""\nInput JSON files\n"""\ntickets_file="../../{}/all_{}_lines.json".format(assets[0], assets[0])\nusers_file="../../{}/all_{}_lines.json".format(assets[1], assets[1])\ncomments_file="../../{}/all_{}_lines.json".format(assets[2], assets[2])\n\xa0\n\xa0\n### Split JSON files into smaller batches\nlines_per_file=100\n\xa0\nfor asset in assets:\n\xa0\xa0\xa0\xa0current_file=\'../../{}/all_{}_lines.json\'.format(asset, asset)\n\xa0\xa0\xa0\xa0print("Splitting file \'{}\' into files with {} lines...".format(current_file, str(lines_per_file)))\n\xa0\xa0\xa0\xa0batch_file = None\n\xa0\xa0\xa0\xa0with open(current_file) as bigfile:\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0for line_number, line in enumerate(bigfile):\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0if line_number % lines_per_file == 0:\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0if batch_file:\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0batch_file.close()\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0small_filename = \'./{}_split/{}.json\'.format(asset, line_number + lines_per_file)\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0batch_file = open(small_filename, "w")\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0batch_file.write(line)\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0if batch_file:\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0batch_file.close()\n\xa0\xa0\xa0\xa0print("Finished splitting file \'{}\'".format(current_file))\n'})}),"\n",(0,s.jsx)(n.p,{children:"After executing the script above, I had files with 100 JSON objects on each line in the file:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ls ./comments_split/*.json | wc -l\n\xa0\xa0\xa021004\n\xa0\nls ./tickets_split/*.json | wc -l\n\xa0\xa0\xa0\xa0\xa0925\n\xa0\nls ./users_split/*.json | wc -l\n\xa0\xa0\xa0\xa0\xa0138\n"})}),"\n",(0,s.jsxs)(n.p,{children:["The final step was to iterate over each folder, read the ",(0,s.jsx)(n.code,{children:"n"}),"th file within that folder and insert the 100 JSON objects into an output JSON file named ",(0,s.jsx)(n.code,{children:"backup_tickets_{batch_number}.json"}),".\nThis is the script I wrote to produce just that:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python\n\nimport os\nimport pandas as pd\nimport json\n\xa0\nassets=[\'tickets\', \'users\', \'comments\']\n\xa0\n"""\nInput JSON files\n"""\ntickets_dir="./{}_split".format(assets[0])\ncomments_dir="./{}_split".format(assets[2])\nusers_dir="./{}_split".format(assets[1])\n\xa0\n"""\nOutput JSON files\n"""\noutput_folder="../assets_to_send"\n\xa0\n# Every file in asset folder is called after the batch number * 100\n# So first file is called \'100.json\' per each folder (\'./tickets/100.json\', \'./users/100.json\', \'./comments/100.json\')\n# The last file is called \'./comments_split/2100400.json\nfor x in range(100, 2100400, 100):\n\xa0\n\xa0\xa0\xa0\xa0output_filename=os.path.join(output_folder, "backup_tickets_{}.json".format(x))\n\xa0\n\xa0\xa0\xa0\xa0print("Generating JSON for batch {}...".format(x))\n\xa0\n\xa0\xa0\xa0\xa0# Create root JSON object\n\xa0\xa0\xa0\xa0root={}\n\xa0\xa0\xa0\xa0root["data"] = {}\n\xa0\xa0\xa0\xa0root["data"]["tickets"] = {}\n\xa0\xa0\xa0\xa0root["data"]["tickets"]["data"] = []\n\xa0\xa0\xa0\xa0root["data"]["tickets"]["comments"] = []\n\xa0\xa0\xa0\xa0root["data"]["tickets"]["users"] = []\n\xa0\xa0\xa0\xa0root["data"]["tickets"]["organizations"] = []\n\xa0\n\xa0\xa0\xa0\xa0for asset in assets:\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0current_ticket_file = os.path.join(tickets_dir, "{}.json".format(x))\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0current_comments_file = os.path.join(comments_dir, "{}.json".format(x))\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0current_users_file = os.path.join(users_dir, "{}.json".format(x))\n\xa0\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0if os.path.exists(current_ticket_file):\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0tickets=pd.read_json(current_ticket_file, lines=True, convert_dates=False)\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0ticket_out = tickets.to_json(orient=\'records\')\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0ticket_out_dict = json.loads(ticket_out)\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0root["data"]["tickets"].update({\'data\': ticket_out_dict})            \n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0if os.path.exists(current_users_file):\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0users = pd.read_json(current_users_file, lines=True, convert_dates=False)\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0users_out = users.to_json(orient=\'records\')\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0users_out_dict = json.loads(users_out)\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0root["data"]["tickets"].update({\'users\': users_out_dict})\n\xa0\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0if os.path.exists(current_comments_file):\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0comments = pd.read_json(current_comments_file, lines=True, convert_dates=False)\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0comments_out = comments.to_json(orient=\'records\')\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0comments_out_dict = json.loads(comments_out)\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0root["data"]["tickets"].update({\'comments\': comments_out_dict})\n\xa0\n\xa0\xa0\xa0\xa0print("Writing ticket JSON to file \'{}\'...".format(output_filename))\n\xa0\xa0\xa0\xa0with open(output_filename, \'w\', encoding=\'utf-8\') as f:\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0json.dump(root, f, indent=3)\n'})}),"\n",(0,s.jsx)(n.p,{children:"I compressed the generated assets into an archive which contained 21k JSON files:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"tar tf backup_tickets.tar.gz | head\n./backup_tickets_1605900.json\n./backup_tickets_1670800.json\n./backup_tickets_1585100.json\n./backup_tickets_1217200.json\n./backup_tickets_1344500.json\n./backup_tickets_990600.json\n./backup_tickets_1331400.json\n./backup_tickets_1262300.json\n./backup_tickets_171600.json\n\xa0\ntar tf backup_tickets.tar.gz| wc -l\n\xa0\xa0\xa021004\n"})}),"\n",(0,s.jsx)(n.p,{children:"Migration complete!"})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>a});var s=t(96540);const i={},r=s.createContext(i);function o(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);