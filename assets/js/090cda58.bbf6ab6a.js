"use strict";(self.webpackChunkkgkb=self.webpackChunkkgkb||[]).push([[19727],{10021:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>d,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>l});var t=o(74848),i=o(28453);const r={slug:"prod-down-mongodb-corrupt-heketi-glusterfs-provisioning",title:"Fixing Production Down caused by MongoDB Corruption and Heketi/GlusterFS Failed\xa0Provisioning",description:"Fill me up!",authors:["kbbgl"],tags:["cluster","gluster","glusterfs","heketi","kubectl","kubernetes","mongo","mongodb","production","troubleshooting"]},s=void 0,a={permalink:"/blog/prod-down-mongodb-corrupt-heketi-glusterfs-provisioning",source:"@site/blog/2021-03-16-fixing-prod-down-mongodb-corrupt-heketi-glusterfs-failed-provisioning/index.md",title:"Fixing Production Down caused by MongoDB Corruption and Heketi/GlusterFS Failed\xa0Provisioning",description:"Fill me up!",date:"2021-03-16T00:00:00.000Z",tags:[{inline:!0,label:"cluster",permalink:"/blog/tags/cluster"},{inline:!0,label:"gluster",permalink:"/blog/tags/gluster"},{inline:!0,label:"glusterfs",permalink:"/blog/tags/glusterfs"},{inline:!0,label:"heketi",permalink:"/blog/tags/heketi"},{inline:!0,label:"kubectl",permalink:"/blog/tags/kubectl"},{inline:!0,label:"kubernetes",permalink:"/blog/tags/kubernetes"},{inline:!0,label:"mongo",permalink:"/blog/tags/mongo"},{inline:!0,label:"mongodb",permalink:"/blog/tags/mongodb"},{inline:!0,label:"production",permalink:"/blog/tags/production"},{inline:!0,label:"troubleshooting",permalink:"/blog/tags/troubleshooting"}],readingTime:10.61,hasTruncateMarker:!1,authors:[{name:"Kobbi Gal",title:"I like to pick things apart and see how they work inside",url:"https://github.com/kbbgl",imageURL:"https://avatars.githubusercontent.com/u/14372649",key:"kbbgl",page:null}],frontMatter:{slug:"prod-down-mongodb-corrupt-heketi-glusterfs-provisioning",title:"Fixing Production Down caused by MongoDB Corruption and Heketi/GlusterFS Failed\xa0Provisioning",description:"Fill me up!",authors:["kbbgl"],tags:["cluster","gluster","glusterfs","heketi","kubectl","kubernetes","mongo","mongodb","production","troubleshooting"]},unlisted:!1,prevItem:{title:"Installing Ubuntu 20.04 on 2013 MacBook\xa0Air",permalink:"/blog/installing-ubuntu-2004-on-2013-macbook-air"}},d={authorsImageUrls:[void 0]},l=[{value:"Introduction",id:"introduction",level:2},{value:"Salvaging MongoDB",id:"salvaging-mongodb",level:2},{value:"MongoDB Version Conflict",id:"mongodb-version-conflict",level:2},{value:"Fixing Unhealthy GlusterFS/Heketi",id:"fixing-unhealthy-glusterfsheketi",level:2},{value:"Restoring MongoDB and the Application",id:"restoring-mongodb-and-the-application",level:2}];function c(e){const n={a:"a",code:"code",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsxs)(n.p,{children:["Today I received an escalation from one of our largest and most strategic customers. Over the weekend, the customer had \u2018patched\u2019 their 3 Ubuntu 18.04 nodes running Kubernetes 1.17.  They were using ",(0,t.jsx)(n.a,{href:"https://www.gluster.org/",children:(0,t.jsx)(n.code,{children:"glusterfs"})})," as their shared storage class.\nI was trying to figure out what this \u2018patching\u2019 job entailed so we could assess which step of the maintenance to focus on but could not get all the necessary details from the customer support team except for the following order of operations:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["They ran ",(0,t.jsx)(n.code,{children:"kubectl drain"})," to prepare their 3 nodes for the patching. This ensured that all ",(0,t.jsx)(n.code,{children:"Pods"})," would get evicted, including all persistent storage and services."]}),"\n",(0,t.jsx)(n.li,{children:"They ran kernel updates to patch security vulnerabilities."}),"\n",(0,t.jsxs)(n.li,{children:["They upgraded Ubuntu packages using ",(0,t.jsx)(n.code,{children:"apt update&&apt upgrade"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["They restarted all their servers and ran ",(0,t.jsx)(n.code,{children:"kubectl uncordon"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"After they finished this maintenance job, their application was no longer loading.\nThey had not taken any snapshots of the system before running this \u2018patch\u2019 job so we had to figure out why the application was not available."}),"\n",(0,t.jsx)(n.h2,{id:"salvaging-mongodb",children:"Salvaging MongoDB"}),"\n",(0,t.jsxs)(n.p,{children:["I invited the customer to join me on a conference call so I could take a look and gather preliminary information about why the application is not running.\nWhat I noticed when running ",(0,t.jsx)(n.code,{children:"kubectl get pods"})," was that all the services were stuck on ",(0,t.jsx)(n.code,{children:"Init 0/1"}),", which indicated that the execution of the first ",(0,t.jsx)(n.code,{children:"initContainer"})," was terminating with non-zero return codes. All ",(0,t.jsx)(n.code,{children:"initContainer"}),"s were failing their connection to MongoDB as we can see from the spec of one of the ",(0,t.jsx)(n.code,{children:"Pods"})," which was stuck on ",(0,t.jsx)(n.code,{children:"Init"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"Init Containers:\n\xa0\xa0init-mongodb:\n\xa0\xa0\xa0\xa0Image:         busybox:1.30.1\n\xa0\xa0\xa0\xa0Command:\n\xa0\xa0\xa0\xa0\xa0\xa0sh\n\xa0\xa0\xa0\xa0\xa0\xa0-c\n\xa0\xa0\xa0\xa0\xa0\xa0until nc -zv mongodb-service.prod 27017; do echo waiting for mongodb; sleep 2; done;\n"})}),"\n",(0,t.jsxs)(n.p,{children:["It was pretty evident that the MongoDB service was the cause for the dependent application services inability to initialize because we saw that all 3 replica set member\u2019s ",(0,t.jsx)(n.code,{children:"Pod"}),"s were in `Init: CrashLoopBackOff`` state."]}),"\n",(0,t.jsxs)(n.p,{children:["We needed to understand which ",(0,t.jsx)(n.code,{children:"initContainer"})," of MongoDB was crashing. We were able to figure out why using ",(0,t.jsx)(n.code,{children:"kubectl describe pod mongodb-replicaset-0"})," and ",(0,t.jsx)(n.code,{children:"kubectl logs mongodb-replicaset-0 --all-containers"}),".\nWhen running ",(0,t.jsx)(n.code,{children:"kubectl describe pod mongodb-replicaset-0"})," we saw that the failing ",(0,t.jsx)(n.code,{children:"initContainer"})," was one called ",(0,t.jsx)(n.code,{children:"bootstrap"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"Init Containers:\nbootstrap:\n\xa0\xa0\xa0\xa0Image:         mongo:3.6.8\n\xa0\xa0\xa0\xa0Command:\n\xa0\xa0\xa0\xa0\xa0\xa0/work-dir/peer-finder\n\xa0\xa0\xa0\xa0Args:\n\xa0\xa0\xa0\xa0\xa0\xa0-on-start=/init/on-start.sh\n\xa0\xa0\xa0\xa0\xa0\xa0-service=mongodb-replicaset\n\xa0\xa0\xa0\xa0Environment:\n\xa0\xa0\xa0\xa0\xa0\xa0REPLICA_SET:    rs0\n\xa0\xa0\xa0\xa0\xa0\xa0TIMEOUT:        900\n"})}),"\n",(0,t.jsxs)(n.p,{children:["This container is a simple peer finder daemon that is useful with ",(0,t.jsx)(n.code,{children:"StatefulSet"})," and related use cases.\nAll it does is watch DNS for changes in the set of endpoints that are part of the governing service. It periodically looks up the SRV record of the DNS entry that corresponds to a Kubernetes Service which enumerates the set of peers for this the specified service. Not really helpful. So we needed to review the logs and find out what\u2019s causing this container to fail."]}),"\n",(0,t.jsxs)(n.p,{children:["When we ran ",(0,t.jsx)(n.code,{children:"kubectl logs mongod-replicaset-0 --all-containers"})," we noticed the following error:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-plaintext",children:"E STORAGE  WiredTiger error, file:WiredTiger.wt, connection: unable to read root page from file:WiredTiger.wt: WT_ERROR: non-specific WiredTiger error\nE STORAGE  WiredTiger error, file:WiredTiger.wt, connection: WiredTiger has failed to open its metadata\nE STORAGE  WiredTiger error, file:WiredTiger.wt, connection: This may be due to the database files being encrypted, being from an older version or due to corruption on disk\nE STORAGE  WiredTiger error, file:WiredTiger.wt, connection: You should confirm that you have opened the database with the correct options including all encryption and compression options\nE -        WT_ERROR: non-specific WiredTiger error src\\mongo\\db\\storage\\wiredtiger\\wiredtiger_kv_engine.cpp 397\nI STORAGE  WT_ERROR: non-specific WiredTiger error, terminating\n"})}),"\n",(0,t.jsx)(n.p,{children:"The log that immediately grabbed our attention was the following:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-plaintext",children:"E STORAGE  WiredTiger error, file:WiredTiger.wt, connection: This may be due to the database files being encrypted, being from an older version or due to corruption on disk\n"})}),"\n",(0,t.jsx)(n.p,{children:"Although we did not have any incriminating evidence, we were pretty sure that the server patching likely caused some data disk corruption because of an unclean shutdown of the MongoDB service.\nAt this point we were facing a few of problems:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["We could not create a mongodump since the MongoDB server was not running and the ",(0,t.jsx)(n.code,{children:"initContainers"})," were in a ",(0,t.jsx)(n.code,{children:"CrashLoopBackOff"}),"."]}),"\n",(0,t.jsx)(n.li,{children:"We did not have the necessary tools on the production environment since we could not install any additional sidecar containers or utilities."}),"\n",(0,t.jsxs)(n.li,{children:["As mentioned earlier, we did not have a snapshot of the server in a working state or a ",(0,t.jsx)(n.code,{children:"mongodump"})," to rely on so we needed to figure out a way to fix this corrupted state somehow."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Luckily, the environment did have a ",(0,t.jsx)(n.code,{children:"Deployment"})," called ",(0,t.jsx)(n.code,{children:"system-recovery"})," which allowed us access to the mounted storage points so we could access the MongoDB flat files. We could then take these flat files, compress them and transfer them to our lab environment to attempt recovery:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"> kubectl scale deployment system-recovery --replicas=1\ndeployment.apps/system-recovery scaled\n> kubectl cp system-recovery-4184bfa40da-sah4131:/mongodb0 /tmp/mongodb0\n> tar czvf /tmp/mongodb0.tar.gz /tmp/mongodb0\n"})}),"\n",(0,t.jsxs)(n.p,{children:["In our lab, we attempted to load MongoDB with the ",(0,t.jsx)(n.code,{children:"--repair"})," flag but saw that it was failing with the same error we saw earlier on the production environment:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"> mongod --version\n3.6.8\n> mongod --dbpath=/tmp/mongodb0 --repair\nE STORAGE  WiredTiger error, file:WiredTiger.wt, connection: This may be due to the database files being encrypted, being from an older version or due to corruption on disk\n"})}),"\n",(0,t.jsxs)(n.p,{children:["We found out, after further research, that ",(0,t.jsx)(n.a,{href:"https://jira.mongodb.org/browse/SERVER-19815",children:"MongoDB version 4 had a more robust repairing mechanism of corrupted WiredTiger schemas"}),". So we installed MongoDB 4 in our lab environment and reran the same repair:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"> mongod --version\n4.2.2\n> mongod --dbpath=/tmp/mongodb0 --repair\n"})}),"\n",(0,t.jsxs)(n.p,{children:["We received a message that the operation was successful!\nSo now that we had a repaired MongoDB, we wanted to create a ",(0,t.jsx)(n.code,{children:"mongodump"})," and then attempt to load the database in the lab environment to check whether the dependent Pods will successfully load which would mean that the application would load as well."]}),"\n",(0,t.jsx)(n.h2,{id:"mongodb-version-conflict",children:"MongoDB Version Conflict"}),"\n",(0,t.jsxs)(n.p,{children:["Earlier, When attempting to repair the MongoDB, we needed to run the MongoDB server with version 4+ because of the new repair mechanism offered. The actual data which we extracted from the production environment was from version 3.6.8. We didn\u2019t think this would be a problem until we actually attempted to load the database in order to generate a ",(0,t.jsx)(n.code,{children:"mongodump"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"> mongod --version\n3.6.8\n> mongod --dbpath=/tmp/mongodb0\n...\nFound an invalid featureCompatibilityVersion document (ERROR: BadValue: Invalid value for version, found 3.6, expected '4.2' or '4.0'. Contents of featureCompatibilityVersion document in admin.system.version: { _id: \"featureCompatibilityVersion\", version: \"3.6\" }. See http://dochub.mongodb.org/core/4.0-feature-compatibility.). If the current featureCompatibilityVersion is below 4.0, see the documentation on upgrading at \n"})}),"\n",(0,t.jsx)(n.p,{children:"We were now blocked because MongoDB server and the actual data were different versions. We could not run the command:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-javascript",children:'db.setFeatureCompatibility({_id: version"3.6"})\n'})}),"\n",(0,t.jsx)(n.p,{children:"as suggested by a few different StackOverflow articles and MongoDB official documentation since we would need to be able to run the database."}),"\n",(0,t.jsx)(n.p,{children:"This was really bad news since we did not know where this feature flag was actually stored, whether it was stored in clear text and if it was accessible. We decided to run a quick search within all the flat files in the MongoDB directory. It was a shot in the dark but we didn\u2019t really have any other options at this point."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'> grep -rnwi "compatibility" ./mongodb0/*\nWiredTiger.turtle:1:Compatibility version\n'})}),"\n",(0,t.jsx)(n.p,{children:"We managed to find something interesting within the WiredTiger.turtle file which was clear text. The whole file contents looked like this:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-plaintext",children:"Compatibility version\nmajor=4,minor=2\nWiredTiger version string\nWiredTiger 4.2.1: (September 15, 2019)\nWiredTiger version\nmajor=4,minor=2,patch=1\n"})}),"\n",(0,t.jsx)(n.p,{children:"Since we were in dire straits, we decided it was worth to modify the versions in this file and see if we could work around the MongoDB server validation. We modified it to the following:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-plaintext",children:"Compatibility version\nmajor=3,minor=6\nWiredTiger version string\nWiredTiger 3.6.8: (July 12, 2018)\nWiredTiger version\nmajor=3,minor=6,patch=8\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Miraculously, when running MongoDB daemon again, we were able to load the database and the application! We created a dump file using ",(0,t.jsx)(n.code,{children:"mongodump"})," and transferred it to the production environment."]}),"\n",(0,t.jsx)(n.h2,{id:"fixing-unhealthy-glusterfsheketi",children:"Fixing Unhealthy GlusterFS/Heketi"}),"\n",(0,t.jsxs)(n.p,{children:["We reconnected to the production environment, transferred the repaired and generated ",(0,t.jsx)(n.code,{children:"mongodump"})," and we needed to somehow restore it onto the shared storage. We saw three possible ways of performing this:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Modify the MongoDB ",(0,t.jsx)(n.code,{children:"StateFulSet"})," specification by changing the executed commands to something like an infinite sleep so that the ",(0,t.jsx)(n.code,{children:"bootstrap"})," ",(0,t.jsx)(n.code,{children:"initContainer"})," would not terminate. This would allow us to copy the ",(0,t.jsx)(n.code,{children:"mongodump"})," into the container and run ",(0,t.jsx)(n.code,{children:"mongorestore"})," to load the repaired databased and ensure it\u2019s replicated across all GlusterFS storage locations. This was the preferred option."]}),"\n",(0,t.jsx)(n.li,{children:"Replace all MongoDB flat files manually within each one of the GlusterFS shared storage locations. This would not be ideal since it would go against MongoDB replication best practices."}),"\n",(0,t.jsxs)(n.li,{children:["Removing all GlusterFS ",(0,t.jsx)(n.code,{children:"PersistentVolumeClaims"}),", recreate them from scratch and then bind them to the MongoDB replica set members."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["After reviewing all three options, we decided to go with the 3rd approach. It was decided as the best option after consulting with our DevOps because they mentioned that they have set up dynamic storage allocation upon removal of the GlusterFS ",(0,t.jsx)(n.code,{children:"PersistenVolumeClaims"})," and restart of the MongoDB ",(0,t.jsx)(n.code,{children:"StateFulSet"})," ",(0,t.jsx)(n.code,{children:"Pod"}),"s.\nThe first step would be to delete the ",(0,t.jsx)(n.code,{children:"PersistentVolumeClaims"}),". So we went to work."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"> kubectl delete pvc datadir-mongodb-replicaset-0\n> kubectl delete pvc datadir-mongodb-replicaset-1\n> kubectl delete pvc datadir-mongodb-replicaset-2\n"})}),"\n",(0,t.jsx)(n.p,{children:"But we noticed that these pvcs were stuck on Terminating. We even attempted to force delete them but they were still stuck:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"> kubectl delete pvc datadir-mongodb-replicaset-0 --force --grace-period 0\n"})}),"\n",(0,t.jsxs)(n.p,{children:["After some research, we found out that these PersistentVolumeClaim specs had ",(0,t.jsx)(n.a,{href:"https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#finalizers",children:(0,t.jsx)(n.code,{children:"finalizers"})}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"> kubectl describe pvc datadir-mongodb-replicaset-0\n\nName:          datadir-mongodb-replicaset-0\nNamespace:     prod\nStorageClass:  gluster\nStatus:        Bound\nVolume:        pvc-64340467-8109-4856-9a49-2fc36563e9ab\nLabels:        app=mongodb-replicaset\nAnnotations:   pv.kubernetes.io/bind-completed: yes\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0pv.kubernetes.io/bound-by-controller: yes\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/glusterfs\nFinalizers:    [kubernetes.io/pvc-protection]\nCapacity:      150Gi\nAccess Modes:  RWO\nVolumeMode:    Filesystem\nMounted By:    mongodb-replicaset-0\nEvents:        <none>\n"})}),"\n",(0,t.jsxs)(n.p,{children:["We edited the ",(0,t.jsx)(n.code,{children:"PVC"}),"s with:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl edit pvc datadir-mongodb-replicaset-0\n"})}),"\n",(0,t.jsxs)(n.p,{children:["removed the ",(0,t.jsx)(n.code,{children:"finalizer"})," section and reran the ",(0,t.jsx)(n.code,{children:"kubectl delete pvc"})," commands. This resulted in a successful deletion. We then deleted the MongoDB ",(0,t.jsx)(n.code,{children:"StateFulSet"})," to generate the ",(0,t.jsx)(n.code,{children:"PVC"}),"s using ",(0,t.jsx)(n.code,{children:"kubectl delete pod mongodb-replicaset-{0,1,2}"}),". But to our surprise, the ",(0,t.jsx)(n.code,{children:"PVC"})," and the ",(0,t.jsx)(n.code,{children:"Pod"}),"s were stuck ",(0,t.jsx)(n.code,{children:"Pending"}),". Something was preventing the dynamic ",(0,t.jsx)(n.code,{children:"PVC"})," allocation."]}),"\n",(0,t.jsxs)(n.p,{children:["When we ran ",(0,t.jsx)(n.code,{children:"kubectl describe pvc datadir-mongodb-replicaset-0"})," we saw that there was an ",(0,t.jsx)(n.code,{children:"Event"})," with ",(0,t.jsx)(n.code,{children:"ProvisioningFailed"})," with the following error message:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'> kubectl describe pvc datadir-mongodb-replicaset-0\nFailed to provision volume with StorageClass "gluster": failed to create volume.\n'})}),"\n",(0,t.jsxs)(n.p,{children:["The error message was far from informative so we needed to step back and review the state of ",(0,t.jsx)(n.code,{children:"heketi"})," and ",(0,t.jsx)(n.code,{children:"glusterfs"}),". We noticed that neither the ",(0,t.jsx)(n.code,{children:"glusterfs"})," or the ",(0,t.jsx)(n.code,{children:"heketi"})," ",(0,t.jsx)(n.code,{children:"Pod"}),"s had any logs from standard output and all of them, according to ",(0,t.jsx)(n.code,{children:"kubectl get pods -n storage"}),", were in healthy, ",(0,t.jsx)(n.code,{children:"Running"})," state."]}),"\n",(0,t.jsxs)(n.p,{children:["We decided it would be best to use the ",(0,t.jsx)(n.code,{children:"gluster"})," CLI and try to troubleshoot why the provisioning is failing. Luckily, the issue was pretty easy to find. We accessed the ",(0,t.jsx)(n.code,{children:"glusterfs"})," Pod and ran the following command to check the status of ",(0,t.jsx)(n.code,{children:"heketidbstorage"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"> kubectl exec glusterfs-k96d2 -it -- bash\n> [ec2-user@node1 /] gluster volume status heketidbstorage\nStatus of volume: heketidbstorage\nGluster process TCP Port RDMA Port Online Pid\n------------------------------------------------------------------------------\nBrick node1:/var/lib/heketi/mounts/vg\n_4a5d18544111232fc76cdc9872d340d6/brick_75c\nec7af846fbf2e770b9312c6bc56fe/brick 49152 0 N 198\nBrick node2:/var/lib/heketi/mounts/vg\n_3cf3e12449047bba9b1260d301914187/brick_3ea\n27507e65df9ec9a4c0841d27962f9/brick 49153 0 N 198\nBrick node3:/var/lib/heketi/mounts/vg\n_78c5285b77bde0e7ed344df72ef5c630/brick_d54\n3d1ea6e83036e7796a89b1594f4cc/brick 49152 0 N 186\nSelf-heal Daemon on localhost N/A N/A Y 177\nSelf-heal Daemon on node1 N/A N/A Y 183\nSelf-heal Daemon on node1 N/A N/A Y 189\n"})}),"\n",(0,t.jsxs)(n.p,{children:["We noticed that none of the bricks on ",(0,t.jsx)(n.code,{children:"node{1,2,3}"})," were online so we decided to restart it:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"gluster volume stop heketidbstorage \ngluster volume start heketidbstorage\n"})}),"\n",(0,t.jsxs)(n.p,{children:["This resulted in all bricks on all nodes getting back online, released the GlusterFS ",(0,t.jsx)(n.code,{children:"PVC"})," provisioning and allowed all MongoDB replicaset member ",(0,t.jsx)(n.code,{children:"Pod"}),"s to initialize successfully!"]}),"\n",(0,t.jsx)(n.h2,{id:"restoring-mongodb-and-the-application",children:"Restoring MongoDB and the Application"}),"\n",(0,t.jsxs)(n.p,{children:["At this point, 3 intense and excruciating hours have passed, we had gone through a set a complex problems and we were all hoping that the last phase would result in a recovery of the MongoDB and the application.\nWe had the GlusterFS ",(0,t.jsx)(n.code,{children:"PVC"})," provisioned, we had all 3 MongoDB replica set ",(0,t.jsx)(n.code,{children:"Pod"}),"s up. All that was left was to:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Copy the ",(0,t.jsx)(n.code,{children:"mongodump"})," from the host machine to the master MongoDB replica set:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Find out who is the master\n\n> kubectl exec mongodb-replicaset-0 -c mongodb-replicaset -- mongo --eval='db.isMaster().primary' --quiet\nmongodb-replicaset-0.mongodb-replicaset.svc.cluster.local:27017\n\n# Copy the mongodump into the master Pod\n\n> kubectl cp /tmp/mongodb0/mongodump mongodb-replicaset-0 -c mongodb-replicaset:/tmp/\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Run ",(0,t.jsx)(n.code,{children:"mongorestore"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Restore the database\n\n> kubectl exec mongodb-replicaset-0 -c mongodb-replicaset -- mongorestore -d prod --drop /tmp/mongodump/prod\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Clear cache by deleting all ",(0,t.jsx)(n.code,{children:"Pod"}),"s in the application namespace:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"> kubectl delete pod --all -n prod\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Voila! All Pods started successfully and the application was loading again!"})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},28453:(e,n,o)=>{o.d(n,{R:()=>s,x:()=>a});var t=o(96540);const i={},r=t.createContext(i);function s(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);